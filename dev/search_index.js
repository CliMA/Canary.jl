var documenterSearchIndex = {"docs": [

{
    "location": "#",
    "page": "Home",
    "title": "Home",
    "category": "page",
    "text": "DocTestSetup = :(using Canary)"
},

{
    "location": "#Canary.jl-1",
    "page": "Home",
    "title": "Canary.jl",
    "category": "section",
    "text": "An exploration of discontinuous Galerkin methods in Julia."
},

{
    "location": "#Installation-1",
    "page": "Home",
    "title": "Installation",
    "category": "section",
    "text": "To install, run the following commands in the Julia REPL:] add \"https://github.com/climate-machine/Canary.jl\"and then runusing Canaryto load the package.If you are using some of the MPI based functions at the REPL you will also need to load MPI with something likeusing MPI\nMPI.Init()\nMPI.finalize_atexit()If you have problems building MPI.jl try explicitly providing the MPI compiler wrappers through the CC and FC environment variables.  Something like the following at your Bourne shell prompt:export CC=mpicc\nexport FC=mpif90Then launch Julia rebuild MPI.jl with] build MPI"
},

{
    "location": "#Building-Documentation-1",
    "page": "Home",
    "title": "Building Documentation",
    "category": "section",
    "text": "You may build the documentation locally by running\njulia --color=yes --project=docs/ -e \'using Pkg; Pkg.instantiate()\'\n\njulia --color=yes --project=docs/ docs/make.jl\nwhere first invocation of julia only needs to be run the first time."
},

{
    "location": "manual/dg_intro/#",
    "page": "Introduction to DG",
    "title": "Introduction to DG",
    "category": "page",
    "text": ""
},

{
    "location": "manual/dg_intro/#Introduction-to-DG-1",
    "page": "Introduction to DG",
    "title": "Introduction to DG",
    "category": "section",
    "text": ""
},

{
    "location": "manual/mesh/#",
    "page": "Mesh",
    "title": "Mesh",
    "category": "page",
    "text": "DocTestSetup = :(using Canary)"
},

{
    "location": "manual/mesh/#Mesh-1",
    "page": "Mesh",
    "title": "Mesh",
    "category": "section",
    "text": ""
},

{
    "location": "manual/metric/#",
    "page": "Metric Terms",
    "title": "Metric Terms",
    "category": "page",
    "text": "DocTestSetup = :(using Canary)"
},

{
    "location": "manual/metric/#Metric-Terms-1",
    "page": "Metric Terms",
    "title": "Metric Terms",
    "category": "section",
    "text": ""
},

{
    "location": "manual/operators/#",
    "page": "Element Operators",
    "title": "Element Operators",
    "category": "page",
    "text": "DocTestSetup = :(using Canary)"
},

{
    "location": "manual/operators/#Element-Operators-1",
    "page": "Element Operators",
    "title": "Element Operators",
    "category": "section",
    "text": ""
},

{
    "location": "examples/generated/1d_kernels/LDG1d/#",
    "page": "1D Diffusion Equation Example",
    "title": "1D Diffusion Equation Example",
    "category": "page",
    "text": "EditURL = \"https://github.com/climate-machine/Canary.jl/blob/master/examples/1d_kernels/LDG1d.jl\""
},

{
    "location": "examples/generated/1d_kernels/LDG1d/#D-Diffusion-Equation-Example-1",
    "page": "1D Diffusion Equation Example",
    "title": "1D Diffusion Equation Example",
    "category": "section",
    "text": ""
},

{
    "location": "examples/generated/1d_kernels/LDG1d/#Introduction-1",
    "page": "1D Diffusion Equation Example",
    "title": "Introduction",
    "category": "section",
    "text": "This example shows how to construct a 2nd derivative with DG using LDG."
},

{
    "location": "examples/generated/1d_kernels/LDG1d/#Continuous-Governing-Equations-1",
    "page": "1D Diffusion Equation Example",
    "title": "Continuous Governing Equations",
    "category": "section",
    "text": "We discretize the operator:fracpartial^2 qpartial x^2    (1)in the following two-step process. First we discretizeQ = fracpartial qpartial x    (2)followed byfracpartial Qpartial x =  fracpartial^2 qpartial x^2    (3)"
},

{
    "location": "examples/generated/1d_kernels/LDG1d/#Local-Discontinous-Galerkin-(LDG)-Method-1",
    "page": "1D Diffusion Equation Example",
    "title": "Local Discontinous Galerkin (LDG) Method",
    "category": "section",
    "text": "Discretizing Eq. (2) we getint_Omega_e psi Q^(e)_N dOmega_e = int_Omega_e psi fracpartial q^(e)_Npartial x dOmega_e   (4)where q^(e)_N=sum_i=1^(N+1)^dim psi_i(mathbfx) q_i and  Q^(e)_N=sum_i=1^(N+1)^dim psi_i(mathbfx) Q_i are the finite dimensional expansion with basis functions psi(x). Integrating Eq. (4) by parts yieldsint_Omega_e psi Q^(e)_N dOmega_e = left psi q^(*e)_N right_Gamma_e - int_Omega_e fracpartial psipartial x q^(e)_N dOmega_e   (5)where the first term on the right denotes the flux integral term (computed in \"function fluxQ\") and the second term on the right denotes the volume integral term (computed in \"function volumeQ\").  The superscript (*e) in the flux integral term denotes the numerical flux. Here we use the average flux. In matrix form, Eq. (5) becomesM^(e)_ij Q^(e)_j = F_ij q^(*e)_j - widetildeD^(e) q^(e)_j   (6)Next, integrating Eq. (3) by parts gives a similar form to Eq. (6) as followsM^(e)_ij fracpartial^2 q^(e)_jpartial x^2 = F_ij Q^(*e)_j - widetildeD^(e) Q^(e)_j   (7)Since we use the average flux for both q and Q, we can reuse the same functions to construct F and widetildeD in both Eqs.\\ (6) and (7).  However, this will not be the case in multiple dimensions since Eq. (6) represents a gradient operator while Eq. (7) represents a divergence operator."
},

{
    "location": "examples/generated/1d_kernels/LDG1d/#Commented-Program-1",
    "page": "1D Diffusion Equation Example",
    "title": "Commented Program",
    "category": "section",
    "text": "include(joinpath(@__DIR__,\"vtk.jl\"))\nusing MPI\nusing Canary\nusing Printf: @sprintf\nconst HAVE_CUDA = try\n    using CUDAnative\n    using CUDAdrv\n    true\ncatch\n    false\nend\nif HAVE_CUDA\n    macro hascuda(ex)\n        return :($(esc(ex)))\n    end\nelse\n    macro hascuda(ex)\n        return :()\n    end\nend{{{ reshape for CuArray@hascuda function Base.reshape(A::CuArray, dims::NTuple{N, Int}) where {N}\n    @assert prod(dims) == prod(size(A))\n    CuArray{eltype(A), length(dims)}(dims, A.buf)\nend}}}{{{ constants note the order of the fields below is also assumed in the code.const _nstate = 2\nconst _U, _h = 1:_nstate\nconst stateid = (U = _U, h = _h)\n\nconst _nvgeo = 4\nconst _ξx, _MJ, _MJI, _x = 1:_nvgeo\n\nconst _nsgeo = 3\nconst _nx, _sMJ, _vMJI = 1:_nsgeo}}}{{{ cflfunction cfl(::Val{dim}, ::Val{N}, vgeo, Q, mpicomm) where {dim, N}\n    DFloat = eltype(Q)\n    Np = (N+1)^dim\n    (~, ~, nelem) = size(Q)\n    dt = [floatmax(DFloat)]\n    Courant = - [floatmax(DFloat)]\n\n    #Compute DT\n    @inbounds for e = 1:nelem, n = 1:Np\n        U = Q[n, _U, e]\n        ξx = vgeo[n, _ξx, e]\n        dx=1.0/(2*ξx)\n        wave_speed = abs(U)\n        loc_dt = 0.25*dx/wave_speed/N\n        dt[1] = min(dt[1], loc_dt)\n    end\n    dt_min=MPI.Allreduce(dt[1], MPI.MIN, mpicomm)\n\n    #Compute Courant\n    @inbounds for e = 1:nelem, n = 1:Np\n        U = Q[n, _U, e]\n        ξx = vgeo[n, _ξx, e]\n        dx=1.0/(2*ξx)\n        wave_speed = abs(U)\n        loc_Courant = wave_speed*dt[1]/dx*N\n        Courant[1] = max(Courant[1], loc_Courant)\n    end\n    Courant_max=MPI.Allreduce(Courant[1], MPI.MAX, mpicomm)\n\n    (dt_min, Courant_max)\nend}}}{{{ compute geometryfunction computegeometry(::Val{dim}, mesh, D, ξ, ω, meshwarp, vmapM) where dimCompute metric terms    Nq = size(D, 1)\n    DFloat = eltype(D)\n\n    (nface, nelem) = size(mesh.elemtoelem)\n\n    crd = creategrid(Val(dim), mesh.elemtocoord, ξ)\n\n    vgeo = zeros(DFloat, Nq^dim, _nvgeo, nelem)\n    sgeo = zeros(DFloat, _nsgeo, Nq^(dim-1), nface, nelem)\n\n    (ξx, MJ, MJI, x) = ntuple(j->(@view vgeo[:, j, :]), _nvgeo)\n    J = similar(x)\n    (nx, sMJ, vMJI) = ntuple(j->(@view sgeo[ j, :, :, :]), _nsgeo)\n    sJ = similar(sMJ)\n\n    X = ntuple(j->(@view vgeo[:, _x+j-1, :]), dim)\n    creategrid!(X..., mesh.elemtocoord, ξ)\n\n    @inbounds for j = 1:length(x)(x[j]) = meshwarp(x[j],)    endCompute the metric terms    computemetric!(x, J, ξx, sJ, nx, D)\n\n    M = kron(1, ntuple(j->ω, dim)...)\n    MJ .= M .* J\n    MJI .= 1 ./ MJ\n    vMJI .= MJI[vmapM]\n\n    sM = dim > 1 ? kron(1, ntuple(j->ω, dim-1)...) : one(DFloat)\n    sMJ .= sM .* sJ\n\n    (vgeo, sgeo)\nend}}}{{{ CPU Kernels {{{ Volume RHS for 1Dfunction volumerhs!(::Val{1}, ::Val{N}, rhs::Array, Q, vgeo, D, elems) where N\n    DFloat = eltype(Q)\n    Nq = N + 1\n    nelem = size(Q)[end]\n\n    Q = reshape(Q, Nq, _nstate, nelem)\n    rhs = reshape(rhs, Nq, _nstate, nelem)\n    vgeo = reshape(vgeo, Nq, _nvgeo, nelem)\n\n    #Allocate Arrays\n    s_F = Array{DFloat}(undef, Nq, _nstate)\n\n    @inbounds for e in elems\n        for i = 1:Nq\n            MJ, ξx = vgeo[i,_MJ,e], vgeo[i,_ξx,e]\n            U = Q[i, _U, e]\n\n            #Get primitive variables\n            U=U\n\n            #Compute fluxes\n            fluxU_x = 0.5*U^2\n            s_F[i, _U] = MJ * (ξx * fluxU_x)\n        endloop of ξ-grid lines        for s = 1:_nstate, i = 1:Nq, k = 1:Nq\n            rhs[i, s, e] += D[k, i] * s_F[k, s]\n        end\n    end\nend}}}Flux RHS for 1Dfunction fluxrhs!(::Val{1}, ::Val{N}, rhs::Array,  Q, sgeo, elems, vmapM, vmapP, elemtobndy) where N\n    DFloat = eltype(Q)\n    Np = (N+1)\n    Nfp = 1\n    nface = 2\n\n    @inbounds for e in elems\n        for f = 1:nface\n            for n = 1:Nfp\n                nxM, sMJ = sgeo[_nx, n, f, e], sgeo[_sMJ, n, f, e]\n                idM, idP = vmapM[n, f, e], vmapP[n, f, e]\n\n                eM, eP = e, ((idP - 1) ÷ Np) + 1\n                vidM, vidP = ((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1\n\n                UM = Q[vidM, _U, eM]\n                bc = elemtobndy[f, e]\n                UP = zero(eltype(Q))\n                if bc == 0\n                    UP = Q[vidP, _U, eP]\n                elseif bc == 1\n                    UnM = nxM * UM\n                    UP = UM - 2 * UnM * nxM\n                else\n                    error(\"Invalid boundary conditions $bc on face $f of element $e\")\n                end\n\n                #Left Fluxes\n                fluxUM_x = 0.5*UM^2\n\n                #Right Fluxes\n                fluxUP_x = 0.5*UP^2\n\n                #Compute wave speed\n                λM=abs(nxM * UM)\n                λP=abs(nxM * UP)\n                λ = max( λM, λP )\n\n                #Compute Numerical/Rusanov Flux\n                fluxUS = (nxM * (fluxUM_x + fluxUP_x) - λ * (UP - UM)) / 2\n\n                #Update RHS\n                rhs[vidM, _U, eM] -= sMJ * fluxUS\n            end\n        end\n    end\nend}}}{{{ Volume Qfunction volumeQ!(::Val{1}, ::Val{N}, rhs::Array, Q, vgeo, D, elems) where N\n    DFloat = eltype(Q)\n    Nq = N + 1\n    nelem = size(Q)[end]\n\n    Q = reshape(Q, Nq, _nstate, nelem)\n    rhs = reshape(rhs, Nq, _nstate, nelem)\n    vgeo = reshape(vgeo, Nq, _nvgeo, nelem)\n\n    #Initialize RHS vector\n    fill!( rhs, zero(rhs[1]))\n\n    #Allocate Arrays\n    s_F = Array{DFloat}(undef, Nq, _nstate)\n\n    @inbounds for e in elems\n        for i = 1:Nq\n            MJ, ξx = vgeo[i,_MJ,e], vgeo[i,_ξx,e]\n            U = Q[i, _U, e]\n\n            #Get primitive variables\n            U=U\n\n            #Compute fluxes\n            fluxU = U\n            s_F[i, _U] = MJ * (ξx * fluxU)\n        endloop of ξ-grid lines        for s = 1:_nstate, i = 1:Nq, k = 1:Nq\n            rhs[i, s, e] -= D[k, i] * s_F[k, s]\n        end\n    end\nend}}}Flux Qfunction fluxQ!(::Val{1}, ::Val{N}, rhs::Array,  Q, sgeo, elems, vmapM, vmapP, elemtobndy) where N\n    DFloat = eltype(Q)\n    Np = (N+1)\n    Nfp = 1\n    nface = 2\n\n    @inbounds for e in elems\n        for f = 1:nface\n            for n = 1:Nfp\n                nxM, sMJ = sgeo[_nx, n, f, e], sgeo[_sMJ, n, f, e]\n                idM, idP = vmapM[n, f, e], vmapP[n, f, e]\n\n                eM, eP = e, ((idP - 1) ÷ Np) + 1\n                vidM, vidP = ((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1\n\n                UM = Q[vidM, _U, eM]\n                bc = elemtobndy[f, e]\n                UP = zero(eltype(Q))\n                if bc == 0\n                    UP = Q[vidP, _U, eP]\n                elseif bc == 1\n                    UnM = nxM * UM\n                    UP = UM - 2 * UnM * nxM\n                else\n                    error(\"Invalid boundary conditions $bc on face $f of element $e\")\n                end\n\n                #Left Fluxes\n                fluxUM = UM\n\n                #Right Fluxes\n                fluxUP = UP\n\n                #Compute Numerical/Rusanov Flux\n                fluxUS = 0.5*(fluxUM + fluxUP)\n\n                #Update RHS\n                rhs[vidM, _U, eM] += sMJ * nxM * fluxUS\n            end\n        end\n    end\nend}}}{{{ Update solutionfunction updatesolution!(::Val{dim}, ::Val{N}, rhs, rhs_gradQ, Q, vgeo, elems, rka, rkb, dt, visc) where {dim, N}\n\n    DFloat = eltype(Q)\n    Nq=(N+1)^dim\n    (~, ~, nelem) = size(Q)\n\n    @inbounds for e = elems, s = 1:_nstate, i = 1:Nq\n        rhs[i, s, e] += visc*rhs_gradQ[i,s,e]\n        Q[i, s, e] += rkb * dt * rhs[i, s, e] * vgeo[i, _MJI, e]\n        rhs[i, s, e] *= rka\n    end\n\nend}}}{{{ Update grad Q solutionfunction update_gradQ!(::Val{dim}, ::Val{N}, Q, rhs, vgeo, elems) where {dim, N}\n\n    DFloat = eltype(Q)\n    Nq=(N+1)^dim\n    (~, ~, nelem) = size(Q)\n\n    @inbounds for e = elems, s = 1:_nstate, i = 1:Nq\n        Q[i, s, e] = rhs[i, s, e] * vgeo[i, _MJI, e] / π\n    end\n\nend}}}{{{ GPU kernels {{{ Volume RHS for 1D@hascuda function knl_volumerhs!(::Val{1}, ::Val{N}, rhs, Q, vgeo, D, nelem) where N\n    DFloat = eltype(D)\n    Nq = N + 1\n\n    #Point Thread to DOF and Block to element\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    #Allocate Arrays\n    s_D = @cuStaticSharedMem(eltype(D), (Nq, Nq))\n    s_F = @cuStaticSharedMem(eltype(Q), (Nq, _nstate))\n\n    rhsU = rhsV = rhsh = zero(eltype(rhs))\n    if i <= Nq && j == 1 && k == 1 && e <= nelemLoad derivative into shared memory        if k == 1\n            s_D[i, j] = D[i, j]\n        endLoad values needed into registers        MJ = vgeo[i, j, _MJ, e]\n        ξx = vgeo[i, j, _ξx, e]\n        U = Q[i, _U, e]\n        rhsU = rhs[i, _U, e]\n\n        #Get primitive variables and fluxes\n        U=U\n        fluxU_x = 0.5*U^2\n        s_F[i, _U] = MJ * (ξx * fluxU_x)\n    end\n\n    sync_threads()\n\n    @inbounds if i <= Nq && j == 1 && k == 1 && e <= nelem\n        for n = 1:Nq\n            #ξ-grid lines\n            Dni = s_D[n, i]\n            rhsU += Dni * s_F[n, _U]\n        end\n        rhs[i, _U, e] = rhsU\n    end\n    nothing\nend}}}{{{ Face RHS (all dimensions)@hascuda function knl_fluxrhs!(::Val{dim}, ::Val{N}, rhs, Q, sgeo, nelem, vmapM, vmapP, elemtobndy) where {dim, N}\n    DFloat = eltype(Q)\n    Np = (N+1)^dim\n    nface = 2*dim\n\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    Nq = N+1\n    half = convert(eltype(Q), 0.5)\n\n    @inbounds if i <= Nq && j == 1 && k == 1 && e <= nelem\n        n = i + (j-1) * Nq\n        for lf = 1:2:nface\n            for f = lf:lf+1\n                nxM, sMJ = sgeo[_nx, n, f, e], sgeo[_sMJ, n, f, e]\n                (idM, idP) = (vmapM[n, f, e], vmapP[n, f, e])\n\n                (eM, eP) = (e, ((idP - 1) ÷ Np) + 1)\n                (vidM, vidP) = (((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1)\n\n                UM = Q[vidM, _U, eM]\n\n                bc = elemtobndy[f, e]\n                UP = zero(eltype(Q))\n                if bc == 0\n                    UP = Q[vidP, _U, eP]\n                elseif bc == 1\n                    UnM = nxM * UM\n                    UP = UM - 2 * UnM * nxM         else\n             error(\"Invalid boundary conditions $bc on face $f of element $e\")                end\n\n                #Left Fluxes\n                fluxUM_x = 0.5*UM^2\n\n                #Right Fluxes\n                fluxUP_x = 0.5*UP^2\n\n                #Compute wave speed\n                λM=abs(nxM * uM)\n                λP=abs(nxM * uP)\n                λ = max( λM, λP )\n\n                #Compute Numerical Flux and Update\n                fluxUS = (nxM * (fluxUM_x + fluxUP_x) - λ * (UP - UM)) / 2\n\n                #Update RHS\n                rhs[vidM, _U, eM] -= sMJ * fluxUS\n            end\n            sync_threads()\n        end\n    end\n    nothing\nend}}}{{{ Update solution (for all dimensions)@hascuda function knl_updatesolution!(::Val{dim}, ::Val{N}, rhs, Q, vgeo, nelem, rka, rkb, dt) where {dim, N}\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    Nq = N+1\n    @inbounds if i <= Nq && j <= Nq && k <= Nq && e <= nelem\n        n = i + (j-1) * Nq + (k-1) * Nq * Nq\n        MJI = vgeo[n, _MJI, e]\n        for s = 1:_nstate\n            Q[n, s, e] += rkb * dt * rhs[n, s, e] * MJI\n            rhs[n, s, e] *= rka\n        end\n    end\n    nothing\nend}}}{{{ Fill sendQ on device with Q (for all dimensions)@hascuda function knl_fillsendQ!(::Val{dim}, ::Val{N}, sendQ, Q, sendelems) where {N, dim}\n    Nq = N + 1\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    @inbounds if i <= Nq && j <= Nq && k <= Nq && e <= length(sendelems)\n        n = i + (j-1) * Nq + (k-1) * Nq * Nq\n        re = sendelems[e]\n        for s = 1:_nstate\n            sendQ[n, s, e] = Q[n, s, re]\n        end\n    end\n    nothing\nend}}}{{{ Fill Q on device with recvQ (for all dimensions)@hascuda function knl_transferrecvQ!(::Val{dim}, ::Val{N}, Q, recvQ, nelem, nrealelem) where {N, dim}\n    Nq = N + 1\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    @inbounds if i <= Nq && j <= Nq && k <= Nq && e <= nelem\n        n = i + (j-1) * Nq + (k-1) * Nq * Nq\n        for s = 1:_nstate\n            Q[n, s, nrealelem + e] = recvQ[n, s, e]\n        end\n    end\n    nothing\nend}}}{{{ MPI Buffer handlingfunction fillsendQ!(::Val{dim}, ::Val{N}, sendQ, d_sendQ::Array, Q,\n                    sendelems) where {dim, N}\n    sendQ[:, :, :] .= Q[:, :, sendelems]\nend\n\n@hascuda function fillsendQ!(::Val{dim}, ::Val{N}, sendQ, d_sendQ::CuArray,\n                             d_QL, d_sendelems) where {dim, N}\n    nsendelem = length(d_sendelems)\n    if nsendelem > 0\n        @cuda(threads=ntuple(j->N+1, dim), blocks=nsendelem,\n              knl_fillsendQ!(Val(dim), Val(N), d_sendQ, d_QL, d_sendelems))\n        sendQ .= d_sendQ\n    end\nend\n\n@hascuda function transferrecvQ!(::Val{dim}, ::Val{N}, d_recvQ::CuArray, recvQ,\n                                 d_QL, nrealelem) where {dim, N}\n    nrecvelem = size(recvQ)[end]\n    if nrecvelem > 0\n        d_recvQ .= recvQ\n        @cuda(threads=ntuple(j->N+1, dim), blocks=nrecvelem,\n              knl_transferrecvQ!(Val(dim), Val(N), d_QL, d_recvQ, nrecvelem,\n                                 nrealelem))\n    end\nend\n\nfunction transferrecvQ!(::Val{dim}, ::Val{N}, d_recvQ::Array, recvQ, Q,\n                        nrealelem) where {dim, N}\n    Q[:, :, nrealelem+1:end] .= recvQ[:, :, :]\nend}}}{{{ GPU kernel wrappers@hascuda function volumerhs!(::Val{dim}, ::Val{N}, d_rhsC::CuArray, d_QC, d_vgeoC, d_D, elems) where {dim, N}\n    nelem = length(elems)\n    @cuda(threads=ntuple(j->N+1, dim), blocks=nelem,\n          knl_volumerhs!(Val(dim), Val(N), d_rhsC, d_QC, d_vgeoC, d_D, nelem))\nend\n\n@hascuda function fluxrhs!(::Val{dim}, ::Val{N}, d_rhsL::CuArray, d_QL, d_sgeo, elems, d_vmapM, d_vmapP, d_elemtobndy) where {dim, N}\n    nelem = length(elems)\n    @cuda(threads=(ntuple(j->N+1, dim-1)..., 1), blocks=nelem,\n          knl_fluxrhs!(Val(dim), Val(N), d_rhsL, d_QL, d_sgeo, nelem, d_vmapM, d_vmapP, d_elemtobndy))\nend\n\n@hascuda function updatesolution!(::Val{dim}, ::Val{N}, d_rhsL::CuArray, d_QL, d_vgeoL, elems, rka, rkb, dt) where {dim, N}\n    nelem = length(elems)\n    @cuda(threads=ntuple(j->N+1, dim), blocks=nelem,\n          knl_updatesolution!(Val(dim), Val(N), d_rhsL, d_QL, d_vgeoL, nelem, rka, rkb, dt))\nend}}}{{{ L2 Error (for all dimensions)function L2errorsquared(::Val{dim}, ::Val{N}, Q, vgeo, elems, Qex, t) where {dim, N}\n    DFloat = eltype(Q)\n    Np = (N+1)^dim\n    (~, nstate, nelem) = size(Q)\n\n    err = zero(DFloat)\n\n    @inbounds for e = elems, i = 1:Np\n        X = ntuple(j -> vgeo[i, _x-1+j, e] - Q[i, _U-1+j, e]*t, Val(dim))\n        diff = Q[i, _U, e] - Qex[i, _U, e]\n        err += vgeo[i, _MJ, e] * diff^2\n    end\n\n    err\nend}}}{{{ L2 Energy (for all dimensions)function L2energysquared(::Val{dim}, ::Val{N}, Q, vgeo, elems) where {dim, N}\n    DFloat = eltype(Q)\n    Np = (N+1)^dim\n    (~, nstate, nelem) = size(Q)\n\n    energy = zero(DFloat)\n\n    @inbounds for e = elems, q = 1:nstate, i = 1:Np\n        energy += vgeo[i, _MJ, e] * Q[i, q, e]^2\n    end\n\n    energy\nend}}}{{{ Send Datafunction senddata(::Val{dim}, ::Val{N}, mesh, sendreq, recvreq, sendQ,\n                  recvQ, d_sendelems, d_sendQ, d_recvQ, d_QL, mpicomm;\n                  ArrType=ArrType) where {dim, N}\n    DFloat = eltype(d_QL)\n    mpirank = MPI.Comm_rank(mpicomm)Create send and recv request array    nnabr = length(mesh.nabrtorank)\n    d_sendelems = ArrType(mesh.sendelems)\n    nrealelem = length(mesh.realelems)post MPI receives    for n = 1:nnabr\n        recvreq[n] = MPI.Irecv!((@view recvQ[:, :, mesh.nabrtorecv[n]]),\n                                mesh.nabrtorank[n], 777, mpicomm)\n    endwait on (prior) MPI sends    MPI.Waitall!(sendreq)pack data from d_QL into send buffer    fillsendQ!(Val(dim), Val(N), sendQ, d_sendQ, d_QL, d_sendelems)post MPI sends    for n = 1:nnabr\n        sendreq[n] = MPI.Isend((@view sendQ[:, :, mesh.nabrtosend[n]]),\n                               mesh.nabrtorank[n], 777, mpicomm)\n    end\nend}}}{{{ Receive Datafunction receivedata!(::Val{dim}, ::Val{N}, mesh, recvreq,\n                      recvQ, d_recvQ, d_QL) where {dim, N}\n    DFloat = eltype(d_QL)\n    nrealelem = length(mesh.realelems)wait on MPI receives    MPI.Waitall!(recvreq)copy data to state vector d_QL    transferrecvQ!(Val(dim), Val(N), d_recvQ, recvQ, d_QL, nrealelem)\n\nend}}}{{{ RK loopfunction lowstorageRK(::Val{dim}, ::Val{N}, mesh, vgeo, sgeo, Q, rhs, D,\n                      dt, nsteps, tout, vmapM, vmapP, mpicomm, visc;\n                      ArrType=ArrType, plotstep=0) where {dim, N}\n    DFloat = eltype(Q)\n    mpirank = MPI.Comm_rank(mpicomm)Fourth-order, low-storage, Runge–Kutta scheme of Carpenter and Kennedy (1994) ((5,4) 2N-Storage RK scheme.Ref: @TECHREPORT{CarpenterKennedy1994,   author = {M.~H. Carpenter and C.~A. Kennedy},   title = {Fourth-order {2N-storage} {Runge-Kutta} schemes},   institution = {National Aeronautics and Space Administration},   year = {1994},   number = {NASA TM-109112},   address = {Langley Research Center, Hampton, VA}, }    RKA = (DFloat(0),\n           DFloat(-567301805773)  / DFloat(1357537059087),\n           DFloat(-2404267990393) / DFloat(2016746695238),\n           DFloat(-3550918686646) / DFloat(2091501179385),\n           DFloat(-1275806237668) / DFloat(842570457699 ))\n\n    RKB = (DFloat(1432997174477) / DFloat(9575080441755 ),\n           DFloat(5161836677717) / DFloat(13612068292357),\n           DFloat(1720146321549) / DFloat(2090206949498 ),\n           DFloat(3134564353537) / DFloat(4481467310338 ),\n           DFloat(2277821191437) / DFloat(14882151754819))\n\n    RKC = (DFloat(0),\n           DFloat(1432997174477) / DFloat(9575080441755),\n           DFloat(2526269341429) / DFloat(6820363962896),\n           DFloat(2006345519317) / DFloat(3224310063776),\n           DFloat(2802321613138) / DFloat(2924317926251))Create send and recv request array    nnabr = length(mesh.nabrtorank)\n    sendreq = fill(MPI.REQUEST_NULL, nnabr)\n    recvreq = fill(MPI.REQUEST_NULL, nnabr)Create send and recv buffer    sendQ = zeros(DFloat, (N+1)^dim, size(Q,2), length(mesh.sendelems))\n    recvQ = zeros(DFloat, (N+1)^dim, size(Q,2), length(mesh.ghostelems))\n\n    nrealelem = length(mesh.realelems)\n    nsendelem = length(mesh.sendelems)\n    nrecvelem = length(mesh.ghostelems)\n    nelem = length(mesh.elems)\n\n    d_QL, d_rhsL = ArrType(Q), ArrType(rhs)\n    d_vgeoL, d_sgeo = ArrType(vgeo), ArrType(sgeo)\n    d_vmapM, d_vmapP = ArrType(vmapM), ArrType(vmapP)\n    d_sendelems, d_elemtobndy = ArrType(mesh.sendelems), ArrType(mesh.elemtobndy)\n    d_sendQ, d_recvQ = ArrType(sendQ), ArrType(recvQ)\n    d_D = ArrType(D)\n    d_gradQL = ArrType(Q)\n    d_rhs_gradQL = ArrType(rhs)\n\n    Qshape    = (fill(N+1, dim)..., size(Q, 2), size(Q, 3))\n    vgeoshape = (fill(N+1, dim)..., _nvgeo, size(Q, 3))\n\n    d_QC = reshape(d_QL, Qshape)\n    d_rhsC = reshape(d_rhsL, Qshape...)\n    d_vgeoC = reshape(d_vgeoL, vgeoshape)\n    d_gradQC = reshape(d_gradQL, Qshape)\n    d_rhs_gradQC = reshape(d_rhs_gradQL, Qshape)Send Data    senddata(Val(dim), Val(N), mesh, sendreq, recvreq, sendQ,\n             recvQ, d_sendelems, d_sendQ, d_recvQ, d_QL, mpicomm;\n             ArrType=ArrType)Receive Data    receivedata!(Val(dim), Val(N), mesh, recvreq, recvQ, d_recvQ, d_QL)volume Q computation    volumeQ!(Val(dim), Val(N), d_rhs_gradQC, d_QC, d_vgeoC, d_D, mesh.realelems)face Q computation    fluxQ!(Val(dim), Val(N), d_rhs_gradQL, d_QL, d_sgeo, mesh.realelems, d_vmapM, d_vmapP, d_elemtobndy)Construct grad Q    update_gradQ!(Val(dim), Val(N), d_gradQL, d_rhs_gradQL, d_vgeoL, mesh.realelems)Send Data    senddata(Val(dim), Val(N), mesh, sendreq, recvreq, sendQ,\n             recvQ, d_sendelems, d_sendQ, d_recvQ, d_gradQL,\n             mpicomm;ArrType=ArrType)volume grad Q computation    volumeQ!(Val(dim), Val(N), d_rhs_gradQC, d_gradQC, d_vgeoC, d_D, mesh.realelems)Receive Data    receivedata!(Val(dim), Val(N), mesh, recvreq, recvQ, d_recvQ, d_gradQL)face grad Q computation    fluxQ!(Val(dim), Val(N), d_rhs_gradQL, d_gradQL, d_sgeo, mesh.realelems, d_vmapM, d_vmapP, d_elemtobndy)Construct grad Q    update_gradQ!(Val(dim), Val(N), d_QL, d_rhs_gradQL, d_vgeoL, mesh.realelems)\n    Q .= d_QL\n    rhs .= d_rhsL\nend}}}{{{ LDG driverfunction LDG(::Val{dim}, ::Val{N}, mpicomm, ic, mesh, tend, visc;\n             meshwarp=(x...)->identity(x), tout = 60, ArrType=Array,\n             plotstep=0) where {dim, N}\n    DFloat = typeof(tend)\n\n    mpirank = MPI.Comm_rank(mpicomm)\n    mpisize = MPI.Comm_size(mpicomm)Partion the mesh using a Hilbert curve based partitioning    mpirank == 0 && println(\"[CPU] partitioning mesh...\")\n    mesh = partition(mpicomm, mesh...)Connect the mesh in parallel    mpirank == 0 && println(\"[CPU] connecting mesh...\")\n    mesh = connectmesh(mpicomm, mesh...)Get the vmaps    mpirank == 0 && println(\"[CPU] computing mappings...\")\n    (vmapM, vmapP) = mappings(N, mesh.elemtoelem, mesh.elemtoface,\n                              mesh.elemtoordr)Create 1-D operators    (ξ, ω) = lglpoints(DFloat, N)\n    D = spectralderivative(ξ)Compute the geometry    mpirank == 0 && println(\"[CPU] computing metrics...\")\n    (vgeo, sgeo) = computegeometry(Val(dim), mesh, D, ξ, ω, meshwarp, vmapM)\n    (nface, nelem) = size(mesh.elemtoelem)Storage for the solution, rhs, and error    mpirank == 0 && println(\"[CPU] creating fields (CPU)...\")\n    Q = zeros(DFloat, (N+1)^dim, _nstate, nelem)\n    Qexact = zeros(DFloat, (N+1)^dim, _nstate, nelem)\n    rhs = zeros(DFloat, (N+1)^dim, _nstate, nelem)setup the initial condition    mpirank == 0 && println(\"[CPU] computing initial conditions (CPU)...\")\n    @inbounds for e = 1:nelem, i = 1:(N+1)^dim\n        x = vgeo[i, _x, e]\n        (U, Uexact) = ic(x)\n        Q[i, _U, e] = U\n        Qexact[i, _U, e] = Uexact\n    endCompute time step    mpirank == 0 && println(\"[CPU] computing dt (CPU)...\")\n    (base_dt,Courant) = cfl(Val(dim), Val(N), vgeo, Q, mpicomm)\n    base_dt=0.001  #FXG debug\n    mpirank == 0 && @show (base_dt,Courant)\n\n    nsteps = ceil(Int64, tend / base_dt)\n    dt = tend / nsteps\n    mpirank == 0 && @show (dt, nsteps, dt * nsteps, tend)Do time stepping    stats = zeros(DFloat, 3)\n    mpirank == 0 && println(\"[CPU] computing initial energy...\")\n    stats[1] = L2energysquared(Val(dim), Val(N), Q, vgeo, mesh.realelems)plot initial condition    mkpath(\"viz\")\n    X = ntuple(j->reshape((@view vgeo[:, _x+j-1, :]), ntuple(j->N+1,dim)...,\n                          nelem), dim)\n    U = reshape((@view Q[:, _U, :]), ntuple(j->(N+1),dim)..., nelem)\n    writemesh(@sprintf(\"viz/LDG%dD_%s_rank_%04d_step_%05d\",\n                       dim, ArrType, mpirank, 0), X...;\n              fields=((\"h\", U),(\"U\",U)),realelems=mesh.realelems)\n\n    #Call Time-stepping Routine\n    mpirank == 0 && println(\"[DEV] starting time stepper...\")\n    lowstorageRK(Val(dim), Val(N), mesh, vgeo, sgeo, Q, rhs, D, dt, nsteps, tout,\n                 vmapM, vmapP, mpicomm, visc;\n                 ArrType=ArrType, plotstep=plotstep)plot final solution    X = ntuple(j->reshape((@view vgeo[:, _x+j-1, :]), ntuple(j->N+1,dim)...,\n                          nelem), dim)\n    U = reshape((@view Q[:, _U, :]), ntuple(j->(N+1),dim)..., nelem)\n    writemesh(@sprintf(\"viz/LDG%dD_%s_rank_%04d_step_%05d\",\n                       dim, ArrType, mpirank, nsteps), X...;\n              fields=((\"h\", U),(\"U\",U)),realelems=mesh.realelems)\n\n    mpirank == 0 && println(\"[CPU] computing final energy...\")\n    stats[2] = L2energysquared(Val(dim), Val(N), Q, vgeo, mesh.realelems)\n    stats[3] = L2errorsquared(Val(dim), Val(N), Q, vgeo, mesh.realelems, Qexact,\n                              tend)\n\n    stats = sqrt.(MPI.allreduce(stats, MPI.SUM, mpicomm))\n\n    if  mpirank == 0\n        @show eng0 = stats[1]\n        @show engf = stats[2]\n        @show Δeng = engf - eng0\n        @show err = stats[3]\n    end\nend}}}{{{ mainfunction main()\n    DFloat = Float64\n\n    MPI.Initialized() || MPI.Init()\n    MPI.finalize_atexit()\n\n    mpicomm = MPI.COMM_WORLD\n    mpirank = MPI.Comm_rank(mpicomm)\n    mpisize = MPI.Comm_size(mpicomm)FIXME: query via hostname    @hascuda device!(mpirank % length(devices()))\n\n    #Input Parameters\n    N=4\n    Ne=10\n    visc=0.01\n    iplot=10\n    time_final=DFloat(1.0)\n    hardware=\"cpu\"\n    @show (N,Ne,visc,iplot,time_final,hardware)\n\n    #Initial Conditions\n    function ic(x...)\n        U = sin( π*x[1] )\n        Uexact = - sin( π*x[1] ) #2nd derivative\n        (U, Uexact)\n    end\n    periodic = (true, )\n\n    mesh = brickmesh((range(DFloat(0); length=Ne+1, stop=2),), periodic; part=mpirank+1, numparts=mpisize)\n\n    if hardware == \"cpu\"\n        mpirank == 0 && println(\"Running (CPU)...\")\n        LDG(Val(1), Val(N), mpicomm, ic, mesh, time_final, visc;\n            ArrType=Array, tout = 10, plotstep = iplot)\n        mpirank == 0 && println()\n    elseif hardware == \"gpu\"\n        @hascuda begin\n            mpirank == 0 && println(\"Running (GPU)...\")\n            LDG(Val(1), Val(N), mpicomm, ic, mesh, time_final, visc;\n                ArrType=CuArray, tout = 10, plotstep = iplot)\n            mpirank == 0 && println()\n        end\n    end\n    nothing\nend}}}main()This page was generated using Literate.jl."
},

{
    "location": "examples/generated/1d_kernels/burger1d/#",
    "page": "1D Burgers Equation",
    "title": "1D Burgers Equation",
    "category": "page",
    "text": "EditURL = \"https://github.com/climate-machine/Canary.jl/blob/master/examples/1d_kernels/burger1d.jl\""
},

{
    "location": "examples/generated/1d_kernels/burger1d/#D-Burgers-Equation-1",
    "page": "1D Burgers Equation",
    "title": "1D Burgers Equation",
    "category": "section",
    "text": ""
},

{
    "location": "examples/generated/1d_kernels/burger1d/#Introduction-1",
    "page": "1D Burgers Equation",
    "title": "Introduction",
    "category": "section",
    "text": "This example shows how to solve the 1D Burgers Equation using vanilla DG with LDG."
},

{
    "location": "examples/generated/1d_kernels/burger1d/#Continuous-Governing-Equations-1",
    "page": "1D Burgers Equation",
    "title": "Continuous Governing Equations",
    "category": "section",
    "text": "We solve the equation: fracpartial qpartial t + fracpartial partial x left( frac12 q^2 right) = fracpartial partial x left( nu  fracpartial q partial x right)   (1)"
},

{
    "location": "examples/generated/1d_kernels/burger1d/#Discontinous-Galerkin-Method-1",
    "page": "1D Burgers Equation",
    "title": "Discontinous Galerkin Method",
    "category": "section",
    "text": "To solve Eq. (1) we use the discontinuous Galerkin method with Lagrange polynomials based on Lobatto points. Multiplying Eq. (1) by a test function psi and integrating within each element Omega_e such that Omega = bigcup_e=1^N_e Omega_e we getint_Omega_e psi fracpartial q^(e)_Npartial t dOmega_e + int_Omega_e psi  fracpartial f^(e)_Npartial x  dOmega_e =  int_Omega_e psi   fracpartial partial x left( nu  fracpartial q^(e)_N partial x right) dOmega_e   (2)where q^(e)_N=sum_i=1^(N+1)^dim psi_i(mathbfx) q_i(t) is the finite dimensional expansion with basis functions psi(mathbfx), where f=left( frac12 q^2 right) Integrating Eq. (2) by parts yieldsint_Omega_e psi fracpartial q^(e)_Npartial t dOmega_e +\nint_Gamma_e psi mathbfn cdot f^(*e)_N dGamma_e - int_Omega_e nabla psi cdot f^(e)_N dOmega_e\n= int_Gamma_e psi mathbfn cdot mathbfQ^(*e)_N dGamma_e - int_Omega_e nabla psi cdot mathbfQ^(e)_N dOmega_e   (3)where the second term on the left denotes the flux integral term (computed in \"function fluxrhs\") and the third term on the left denotes the volume integral term (computed in \"function volumerhs\").  The superscript (*e) in the flux integral term denotes the numerical flux. Here we use the Rusanov flux. Note that mathbfQ is constructed from the LDG method, which we now describe."
},

{
    "location": "examples/generated/1d_kernels/burger1d/#Local-Discontinous-Galerkin-(LDG)-Method-1",
    "page": "1D Burgers Equation",
    "title": "Local Discontinous Galerkin (LDG) Method",
    "category": "section",
    "text": "The LDG method allows us to approximate Q as followsint_Omega_e Psi cdot mathbfQ^(e)_N dOmega_e = int_Omega_e Psi cdot nabla q^(e)_N dOmega_e   (4)where q^(e)_N=sum_i=1^(N+1)^dim psi_i(mathbfx) q_i and  mathbfQ^(e)_N=sum_i=1^(N+1)^dim psi_i(mathbfx) mathbfQ_i are the finite dimensional expansion with basis functions psi(mathbfx) and Psi is the block diagonal tensor with blocks comprised of psi as followsPsi = left(beginarraycc\npsi  0 \n0 psi\nendarray\nright)Integrating Eq. (4) by parts yieldsint_Omega_e Psi cdot mathbfQ^(e)_N dOmega_e = int_Gamma_e left( mathbfn cdot Psi right) q^(*e)_N dGamma_e - int_Omega_e left( nabla cdot Psi right) q^(e)_N dOmega_e   (5)where the first term on the right denotes the flux integral term (computed in \"function fluxQ\") and the second term on the rightdenotes the volume integral term (computed in \"function volumeQ\").  The superscript (*e) in the flux integral term denotes the numerical flux. Here we use the average flux. In matrix form, Eq. (5) becomesM^(e)_ij Q^(e)_j = mathbfF_ij q^(*e)_j - widetildemathbfD^(e) q^(e)_j   (6)Next, integrating Eq. (3) by parts gives a similar form to Eq. (6) as followsM^(e)_ij fracpartial^2 q^(e)_jpartial x^2 = mathbfF_ij Q^(*e)_j - widetildemathbfD^(e) Q^(e)_j   (7)Since we use the average flux for both q and Q, we can reuse the same functions to construct mathbfF and widetildemathbfD."
},

{
    "location": "examples/generated/1d_kernels/burger1d/#Commented-Program-1",
    "page": "1D Burgers Equation",
    "title": "Commented Program",
    "category": "section",
    "text": "include(joinpath(@__DIR__,\"vtk.jl\"))\nusing MPI\nusing Canary\nusing Printf: @sprintf\nconst HAVE_CUDA = try\n    using CUDAnative\n    using CUDAdrv\n    true\ncatch\n    false\nend\nif HAVE_CUDA\n    macro hascuda(ex)\n        return :($(esc(ex)))\n    end\nelse\n    macro hascuda(ex)\n        return :()\n    end\nend{{{ reshape for CuArray@hascuda function Base.reshape(A::CuArray, dims::NTuple{N, Int}) where {N}\n    @assert prod(dims) == prod(size(A))\n    CuArray{eltype(A), length(dims)}(dims, A.buf)\nend}}}{{{ constants note the order of the fields below is also assumed in the code.const _nstate = 2\nconst _U, _h = 1:_nstate\nconst stateid = (U = _U, h = _h)\n\nconst _nvgeo = 4\nconst _ξx, _MJ, _MJI, _x = 1:_nvgeo\n\nconst _nsgeo = 3\nconst _nx, _sMJ, _vMJI = 1:_nsgeo}}}{{{ courantfunction courantnumber(::Val{dim}, ::Val{N}, vgeo, Q, mpicomm) where {dim, N}\n    DFloat = eltype(Q)\n    Np = (N+1)^dim\n    (~, ~, nelem) = size(Q)\n    dt = [floatmax(DFloat)]\n    Courant = - [floatmax(DFloat)]\n\n    #Compute DT\n    @inbounds for e = 1:nelem, n = 1:Np\n        U = Q[n, _U, e]\n        ξx = vgeo[n, _ξx, e]\n        dx=1.0/(2*ξx)\n        wave_speed = abs(U)\n        loc_dt = 0.5*dx/wave_speed/N\n        dt[1] = min(dt[1], loc_dt)\n    end\n    dt_min=MPI.Allreduce(dt[1], MPI.MIN, mpicomm)\n\n    #Compute Courant\n    @inbounds for e = 1:nelem, n = 1:Np\n        U = Q[n, _U, e]\n        ξx = vgeo[n, _ξx, e]\n        dx=1.0/(2*ξx)\n        wave_speed = abs(U)\n        loc_Courant = wave_speed*dt[1]/dx*N\n        Courant[1] = max(Courant[1], loc_Courant)\n    end\n    Courant_max=MPI.Allreduce(Courant[1], MPI.MAX, mpicomm)\n\n    (dt_min, Courant_max)\nend}}}{{{ compute geometryfunction computegeometry(::Val{dim}, mesh, D, ξ, ω, meshwarp, vmapM) where dimCompute metric terms    Nq = size(D, 1)\n    DFloat = eltype(D)\n\n    (nface, nelem) = size(mesh.elemtoelem)\n\n    crd = creategrid(Val(dim), mesh.elemtocoord, ξ)\n\n    vgeo = zeros(DFloat, Nq^dim, _nvgeo, nelem)\n    sgeo = zeros(DFloat, _nsgeo, Nq^(dim-1), nface, nelem)\n\n    (ξx, MJ, MJI, x) = ntuple(j->(@view vgeo[:, j, :]), _nvgeo)\n    J = similar(x)\n    (nx, sMJ, vMJI) = ntuple(j->(@view sgeo[ j, :, :, :]), _nsgeo)\n    sJ = similar(sMJ)\n\n    X = ntuple(j->(@view vgeo[:, _x+j-1, :]), dim)\n    creategrid!(X..., mesh.elemtocoord, ξ)\n\n    @inbounds for j = 1:length(x)(x[j]) = meshwarp(x[j],)    endCompute the metric terms    computemetric!(x, J, ξx, sJ, nx, D)\n\n    M = kron(1, ntuple(j->ω, dim)...)\n    MJ .= M .* J\n    MJI .= 1 ./ MJ\n    vMJI .= MJI[vmapM]\n\n    sM = dim > 1 ? kron(1, ntuple(j->ω, dim-1)...) : one(DFloat)\n    sMJ .= sM .* sJ\n\n    (vgeo, sgeo)\nend}}}{{{ CPU Kernels {{{ Volume RHS for 1Dfunction volumerhs!(::Val{1}, ::Val{N}, rhs::Array, Q, vgeo, D, elems) where N\n    DFloat = eltype(Q)\n    Nq = N + 1\n    nelem = size(Q)[end]\n\n    Q = reshape(Q, Nq, _nstate, nelem)\n    rhs = reshape(rhs, Nq, _nstate, nelem)\n    vgeo = reshape(vgeo, Nq, _nvgeo, nelem)\n\n    #Allocate Arrays\n    s_F = Array{DFloat}(undef, Nq, _nstate)\n\n    @inbounds for e in elems\n        for i = 1:Nq\n            MJ, ξx = vgeo[i,_MJ,e], vgeo[i,_ξx,e]\n            U = Q[i, _U, e]\n\n            #Get primitive variables\n            U=U\n\n            #Compute fluxes\n            fluxU_x = 0.5*U^2\n            s_F[i, _U] = MJ * (ξx * fluxU_x)\n        endloop of ξ-grid lines        for s = 1:_nstate, i = 1:Nq, k = 1:Nq\n            rhs[i, s, e] += D[k, i] * s_F[k, s]\n        end\n    end\nend}}}Flux RHS for 1Dfunction fluxrhs!(::Val{1}, ::Val{N}, rhs::Array,  Q, sgeo, elems, vmapM, vmapP, elemtobndy) where N\n    DFloat = eltype(Q)\n    Np = (N+1)\n    Nfp = 1\n    nface = 2\n\n    @inbounds for e in elems\n        for f = 1:nface\n            for n = 1:Nfp\n                nxM, sMJ = sgeo[_nx, n, f, e], sgeo[_sMJ, n, f, e]\n                idM, idP = vmapM[n, f, e], vmapP[n, f, e]\n\n                eM, eP = e, ((idP - 1) ÷ Np) + 1\n                vidM, vidP = ((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1\n\n                UM = Q[vidM, _U, eM]\n                bc = elemtobndy[f, e]\n                UP = zero(eltype(Q))\n                if bc == 0\n                    UP = Q[vidP, _U, eP]\n                elseif bc == 1\n                    UnM = nxM * UM\n                    UP = UM - 2 * UnM * nxM\n                else\n                    error(\"Invalid boundary conditions $bc on face $f of element $e\")\n                end\n\n                #Left Fluxes\n                fluxUM_x = 0.5*UM^2\n\n                #Right Fluxes\n                fluxUP_x = 0.5*UP^2\n\n                #Compute wave speed\n                λM=abs(nxM * UM)\n                λP=abs(nxM * UP)\n                λ = max( λM, λP )\n\n                #Compute Numerical/Rusanov Flux\n                fluxUS = (nxM * (fluxUM_x + fluxUP_x) - λ * (UP - UM)) / 2\n\n                #Update RHS\n                rhs[vidM, _U, eM] -= sMJ * fluxUS\n            end\n        end\n    end\nend}}}{{{ Volume Qfunction volumeQ!(::Val{1}, ::Val{N}, rhs::Array, Q, vgeo, D, elems) where N\n    DFloat = eltype(Q)\n    Nq = N + 1\n    nelem = size(Q)[end]\n\n    Q = reshape(Q, Nq, _nstate, nelem)\n    rhs = reshape(rhs, Nq, _nstate, nelem)\n    vgeo = reshape(vgeo, Nq, _nvgeo, nelem)\n\n    #Initialize RHS vector\n    fill!( rhs, zero(rhs[1]))\n\n    #Allocate Arrays\n    s_F = Array{DFloat}(undef, Nq, _nstate)\n\n    @inbounds for e in elems\n        for i = 1:Nq\n            MJ, ξx = vgeo[i,_MJ,e], vgeo[i,_ξx,e]\n            U = Q[i, _U, e]\n\n            #Get primitive variables\n            U=U\n\n            #Compute fluxes\n            fluxU = U\n            s_F[i, _U] = MJ * (ξx * fluxU)\n        endloop of ξ-grid lines        for s = 1:_nstate, i = 1:Nq, k = 1:Nq\n            rhs[i, s, e] -= D[k, i] * s_F[k, s]\n        end\n    end\nend}}}Flux Qfunction fluxQ!(::Val{1}, ::Val{N}, rhs::Array,  Q, sgeo, elems, vmapM, vmapP, elemtobndy) where N\n    DFloat = eltype(Q)\n    Np = (N+1)\n    Nfp = 1\n    nface = 2\n\n    @inbounds for e in elems\n        for f = 1:nface\n            for n = 1:Nfp\n                nxM, sMJ = sgeo[_nx, n, f, e], sgeo[_sMJ, n, f, e]\n                idM, idP = vmapM[n, f, e], vmapP[n, f, e]\n\n                eM, eP = e, ((idP - 1) ÷ Np) + 1\n                vidM, vidP = ((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1\n\n                UM = Q[vidM, _U, eM]\n                bc = elemtobndy[f, e]\n                UP = zero(eltype(Q))\n                if bc == 0\n                    UP = Q[vidP, _U, eP]\n                elseif bc == 1\n                    UnM = nxM * UM\n                    UP = UM - 2 * UnM * nxM\n                else\n                    error(\"Invalid boundary conditions $bc on face $f of element $e\")\n                end\n\n                #Left Fluxes\n                fluxUM = UM\n\n                #Right Fluxes\n                fluxUP = UP\n\n                #Compute Numerical/Rusanov Flux\n                fluxUS = 0.5*(fluxUM + fluxUP)\n\n                #Update RHS\n                rhs[vidM, _U, eM] += sMJ * nxM * fluxUS\n            end\n        end\n    end\nend}}}{{{ Update solutionfunction updatesolution!(::Val{dim}, ::Val{N}, rhs, rhs_gradQ, Q, vgeo, elems, rka, rkb, dt, visc) where {dim, N}\n\n    DFloat = eltype(Q)\n    Nq=(N+1)^dim\n    (~, ~, nelem) = size(Q)\n\n    @inbounds for e = elems, s = 1:_nstate, i = 1:Nq\n        rhs[i, s, e] += visc*rhs_gradQ[i,s,e]\n        Q[i, s, e] += rkb * dt * rhs[i, s, e] * vgeo[i, _MJI, e]\n        rhs[i, s, e] *= rka\n    end\n\nend}}}{{{ Update grad Q solutionfunction update_gradQ!(::Val{dim}, ::Val{N}, Q, rhs, vgeo, elems) where {dim, N}\n\n    DFloat = eltype(Q)\n    Nq=(N+1)^dim\n    (~, ~, nelem) = size(Q)\n\n    @inbounds for e = elems, s = 1:_nstate, i = 1:Nq\n        Q[i, s, e] = rhs[i, s, e] * vgeo[i, _MJI, e]\n    end\n\nend}}}{{{ GPU kernels {{{ Volume RHS for 1D@hascuda function knl_volumerhs!(::Val{1}, ::Val{N}, rhs, Q, vgeo, D, nelem) where N\n    DFloat = eltype(D)\n    Nq = N + 1\n\n    #Point Thread to DOF and Block to element\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    #Allocate Arrays\n    s_D = @cuStaticSharedMem(eltype(D), (Nq, Nq))\n    s_F = @cuStaticSharedMem(eltype(Q), (Nq, _nstate))\n\n    rhsU = rhsV = rhsh = zero(eltype(rhs))\n    if i <= Nq && j == 1 && k == 1 && e <= nelemLoad derivative into shared memory        if k == 1\n            s_D[i, j] = D[i, j]\n        endLoad values needed into registers        MJ = vgeo[i, j, _MJ, e]\n        ξx = vgeo[i, j, _ξx, e]\n        U = Q[i, _U, e]\n        rhsU = rhs[i, _U, e]\n\n        #Get primitive variables and fluxes\n        U=U\n        fluxU_x = 0.5*U^2\n        s_F[i, _U] = MJ * (ξx * fluxU_x)\n    end\n\n    sync_threads()\n\n    @inbounds if i <= Nq && j == 1 && k == 1 && e <= nelem\n        for n = 1:Nq\n            #ξ-grid lines\n            Dni = s_D[n, i]\n            rhsU += Dni * s_F[n, _U]\n        end\n        rhs[i, _U, e] = rhsU\n    end\n    nothing\nend}}}{{{ Face RHS (all dimensions)@hascuda function knl_fluxrhs!(::Val{dim}, ::Val{N}, rhs, Q, sgeo, nelem, vmapM, vmapP, elemtobndy) where {dim, N}\n    DFloat = eltype(Q)\n    Np = (N+1)^dim\n    nface = 2*dim\n\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    Nq = N+1\n    half = convert(eltype(Q), 0.5)\n\n    @inbounds if i <= Nq && j == 1 && k == 1 && e <= nelem\n        n = i + (j-1) * Nq\n        for lf = 1:2:nface\n            for f = lf:lf+1\n                nxM, sMJ = sgeo[_nx, n, f, e], sgeo[_sMJ, n, f, e]\n                (idM, idP) = (vmapM[n, f, e], vmapP[n, f, e])\n\n                (eM, eP) = (e, ((idP - 1) ÷ Np) + 1)\n                (vidM, vidP) = (((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1)\n\n                UM = Q[vidM, _U, eM]\n\n                bc = elemtobndy[f, e]\n                UP = zero(eltype(Q))\n                if bc == 0\n                    UP = Q[vidP, _U, eP]\n                elseif bc == 1\n                    UnM = nxM * UM\n                    UP = UM - 2 * UnM * nxM         else\n             error(\"Invalid boundary conditions $bc on face $f of element $e\")                end\n\n                #Left Fluxes\n                fluxUM_x = 0.5*UM^2\n\n                #Right Fluxes\n                fluxUP_x = 0.5*UP^2\n\n                #Compute wave speed\n                λM=abs(nxM * uM)\n                λP=abs(nxM * uP)\n                λ = max( λM, λP )\n\n                #Compute Numerical Flux and Update\n                fluxUS = (nxM * (fluxUM_x + fluxUP_x) - λ * (UP - UM)) / 2\n\n                #Update RHS\n                rhs[vidM, _U, eM] -= sMJ * fluxUS\n            end\n            sync_threads()\n        end\n    end\n    nothing\nend}}}{{{ Update solution (for all dimensions)@hascuda function knl_updatesolution!(::Val{dim}, ::Val{N}, rhs, Q, vgeo, nelem, rka, rkb, dt) where {dim, N}\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    Nq = N+1\n    @inbounds if i <= Nq && j <= Nq && k <= Nq && e <= nelem\n        n = i + (j-1) * Nq + (k-1) * Nq * Nq\n        MJI = vgeo[n, _MJI, e]\n        for s = 1:_nstate\n            Q[n, s, e] += rkb * dt * rhs[n, s, e] * MJI\n            rhs[n, s, e] *= rka\n        end\n    end\n    nothing\nend}}}{{{ Fill sendQ on device with Q (for all dimensions)@hascuda function knl_fillsendQ!(::Val{dim}, ::Val{N}, sendQ, Q, sendelems) where {N, dim}\n    Nq = N + 1\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    @inbounds if i <= Nq && j <= Nq && k <= Nq && e <= length(sendelems)\n        n = i + (j-1) * Nq + (k-1) * Nq * Nq\n        re = sendelems[e]\n        for s = 1:_nstate\n            sendQ[n, s, e] = Q[n, s, re]\n        end\n    end\n    nothing\nend}}}{{{ Fill Q on device with recvQ (for all dimensions)@hascuda function knl_transferrecvQ!(::Val{dim}, ::Val{N}, Q, recvQ, nelem, nrealelem) where {N, dim}\n    Nq = N + 1\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    @inbounds if i <= Nq && j <= Nq && k <= Nq && e <= nelem\n        n = i + (j-1) * Nq + (k-1) * Nq * Nq\n        for s = 1:_nstate\n            Q[n, s, nrealelem + e] = recvQ[n, s, e]\n        end\n    end\n    nothing\nend}}}{{{ MPI Buffer handlingfunction fillsendQ!(::Val{dim}, ::Val{N}, sendQ, d_sendQ::Array, Q,\n                    sendelems) where {dim, N}\n    sendQ[:, :, :] .= Q[:, :, sendelems]\nend\n\n@hascuda function fillsendQ!(::Val{dim}, ::Val{N}, sendQ, d_sendQ::CuArray,\n                             d_QL, d_sendelems) where {dim, N}\n    nsendelem = length(d_sendelems)\n    if nsendelem > 0\n        @cuda(threads=ntuple(j->N+1, dim), blocks=nsendelem,\n              knl_fillsendQ!(Val(dim), Val(N), d_sendQ, d_QL, d_sendelems))\n        sendQ .= d_sendQ\n    end\nend\n\n@hascuda function transferrecvQ!(::Val{dim}, ::Val{N}, d_recvQ::CuArray, recvQ,\n                                 d_QL, nrealelem) where {dim, N}\n    nrecvelem = size(recvQ)[end]\n    if nrecvelem > 0\n        d_recvQ .= recvQ\n        @cuda(threads=ntuple(j->N+1, dim), blocks=nrecvelem,\n              knl_transferrecvQ!(Val(dim), Val(N), d_QL, d_recvQ, nrecvelem,\n                                 nrealelem))\n    end\nend\n\nfunction transferrecvQ!(::Val{dim}, ::Val{N}, d_recvQ::Array, recvQ, Q,\n                        nrealelem) where {dim, N}\n    Q[:, :, nrealelem+1:end] .= recvQ[:, :, :]\nend}}}{{{ GPU kernel wrappers@hascuda function volumerhs!(::Val{dim}, ::Val{N}, d_rhsC::CuArray, d_QC, d_vgeoC, d_D, elems) where {dim, N}\n    nelem = length(elems)\n    @cuda(threads=ntuple(j->N+1, dim), blocks=nelem,\n          knl_volumerhs!(Val(dim), Val(N), d_rhsC, d_QC, d_vgeoC, d_D, nelem))\nend\n\n@hascuda function fluxrhs!(::Val{dim}, ::Val{N}, d_rhsL::CuArray, d_QL, d_sgeo, elems, d_vmapM, d_vmapP, d_elemtobndy) where {dim, N}\n    nelem = length(elems)\n    @cuda(threads=(ntuple(j->N+1, dim-1)..., 1), blocks=nelem,\n          knl_fluxrhs!(Val(dim), Val(N), d_rhsL, d_QL, d_sgeo, nelem, d_vmapM, d_vmapP, d_elemtobndy))\nend\n\n@hascuda function updatesolution!(::Val{dim}, ::Val{N}, d_rhsL::CuArray, d_QL, d_vgeoL, elems, rka, rkb, dt) where {dim, N}\n    nelem = length(elems)\n    @cuda(threads=ntuple(j->N+1, dim), blocks=nelem,\n          knl_updatesolution!(Val(dim), Val(N), d_rhsL, d_QL, d_vgeoL, nelem, rka, rkb, dt))\nend}}}{{{ L2 Error (for all dimensions)function L2errorsquared(::Val{dim}, ::Val{N}, Q, vgeo, elems, Qex, t) where {dim, N}\n    DFloat = eltype(Q)\n    Np = (N+1)^dim\n    (~, nstate, nelem) = size(Q)\n\n    err = zero(DFloat)\n\n    @inbounds for e = elems, i = 1:Np\n        X = ntuple(j -> vgeo[i, _x-1+j, e] - Q[i, _U-1+j, e]*t, Val(dim))\n        diff = Q[i, _U, e] - Qex[i, _U, e]\n        err += vgeo[i, _MJ, e] * diff^2\n    end\n\n    err\nend}}}{{{ L2 Energy (for all dimensions)function L2energysquared(::Val{dim}, ::Val{N}, Q, vgeo, elems) where {dim, N}\n    DFloat = eltype(Q)\n    Np = (N+1)^dim\n    (~, nstate, nelem) = size(Q)\n\n    energy = zero(DFloat)\n\n    @inbounds for e = elems, i = 1:Np\n        energy += vgeo[i, _MJ, e] * Q[i, _U, e]^2\n    end\n\n    energy\nend}}}{{{ Send Datafunction senddata(::Val{dim}, ::Val{N}, mesh, sendreq, recvreq, sendQ,\n                  recvQ, d_sendelems, d_sendQ, d_recvQ, d_QL, mpicomm;\n                  ArrType=ArrType) where {dim, N}\n    DFloat = eltype(d_QL)\n    mpirank = MPI.Comm_rank(mpicomm)Create send and recv request array    nnabr = length(mesh.nabrtorank)\n    d_sendelems = ArrType(mesh.sendelems)\n    nrealelem = length(mesh.realelems)post MPI receives    for n = 1:nnabr\n        recvreq[n] = MPI.Irecv!((@view recvQ[:, :, mesh.nabrtorecv[n]]),\n                                mesh.nabrtorank[n], 777, mpicomm)\n    endwait on (prior) MPI sends    MPI.Waitall!(sendreq)pack data from d_QL into send buffer    fillsendQ!(Val(dim), Val(N), sendQ, d_sendQ, d_QL, d_sendelems)post MPI sends    for n = 1:nnabr\n        sendreq[n] = MPI.Isend((@view sendQ[:, :, mesh.nabrtosend[n]]),\n                               mesh.nabrtorank[n], 777, mpicomm)\n    end\nend}}}{{{ Receive Datafunction receivedata!(::Val{dim}, ::Val{N}, mesh, recvreq,\n                      recvQ, d_recvQ, d_QL) where {dim, N}\n    DFloat = eltype(d_QL)\n    nrealelem = length(mesh.realelems)wait on MPI receives    MPI.Waitall!(recvreq)copy data to state vector d_QL    transferrecvQ!(Val(dim), Val(N), d_recvQ, recvQ, d_QL, nrealelem)\n\nend}}}{{{ RK loopfunction lowstorageRK(::Val{dim}, ::Val{N}, mesh, vgeo, sgeo, Q, rhs, D,\n                      dt, nsteps, tout, vmapM, vmapP, mpicomm, visc;\n                      ArrType=ArrType, plotstep=0) where {dim, N}\n    DFloat = eltype(Q)\n    mpirank = MPI.Comm_rank(mpicomm)Fourth-order, low-storage, Runge–Kutta scheme of Carpenter and Kennedy (1994) ((5,4) 2N-Storage RK scheme.Ref: @TECHREPORT{CarpenterKennedy1994,   author = {M.~H. Carpenter and C.~A. Kennedy},   title = {Fourth-order {2N-storage} {Runge-Kutta} schemes},   institution = {National Aeronautics and Space Administration},   year = {1994},   number = {NASA TM-109112},   address = {Langley Research Center, Hampton, VA}, }    RKA = (DFloat(0),\n           DFloat(-567301805773)  / DFloat(1357537059087),\n           DFloat(-2404267990393) / DFloat(2016746695238),\n           DFloat(-3550918686646) / DFloat(2091501179385),\n           DFloat(-1275806237668) / DFloat(842570457699 ))\n\n    RKB = (DFloat(1432997174477) / DFloat(9575080441755 ),\n           DFloat(5161836677717) / DFloat(13612068292357),\n           DFloat(1720146321549) / DFloat(2090206949498 ),\n           DFloat(3134564353537) / DFloat(4481467310338 ),\n           DFloat(2277821191437) / DFloat(14882151754819))\n\n    RKC = (DFloat(0),\n           DFloat(1432997174477) / DFloat(9575080441755),\n           DFloat(2526269341429) / DFloat(6820363962896),\n           DFloat(2006345519317) / DFloat(3224310063776),\n           DFloat(2802321613138) / DFloat(2924317926251))Create send and recv request array    nnabr = length(mesh.nabrtorank)\n    sendreq = fill(MPI.REQUEST_NULL, nnabr)\n    recvreq = fill(MPI.REQUEST_NULL, nnabr)Create send and recv buffer    sendQ = zeros(DFloat, (N+1)^dim, size(Q,2), length(mesh.sendelems))\n    recvQ = zeros(DFloat, (N+1)^dim, size(Q,2), length(mesh.ghostelems))\n\n    nrealelem = length(mesh.realelems)\n    nsendelem = length(mesh.sendelems)\n    nrecvelem = length(mesh.ghostelems)\n    nelem = length(mesh.elems)\n\n    #Create Device Arrays\n    d_QL, d_rhsL = ArrType(Q), ArrType(rhs)\n    d_vgeoL, d_sgeo = ArrType(vgeo), ArrType(sgeo)\n    d_vmapM, d_vmapP = ArrType(vmapM), ArrType(vmapP)\n    d_sendelems, d_elemtobndy = ArrType(mesh.sendelems), ArrType(mesh.elemtobndy)\n    d_sendQ, d_recvQ = ArrType(sendQ), ArrType(recvQ)\n    d_D = ArrType(D)\n    d_gradQL = ArrType(Q)\n    d_rhs_gradQL = ArrType(rhs)\n\n    Qshape    = (fill(N+1, dim)..., size(Q, 2), size(Q, 3))\n    vgeoshape = (fill(N+1, dim)..., _nvgeo, size(Q, 3))\n\n    #Reshape Device Arrays\n    d_QC = reshape(d_QL, Qshape)\n    d_rhsC = reshape(d_rhsL, Qshape...)\n    d_vgeoC = reshape(d_vgeoL, vgeoshape)\n    d_gradQC = reshape(d_gradQL, Qshape)\n    d_rhs_gradQC = reshape(d_rhs_gradQL, Qshape)\n\n    start_time = t1 = time_ns()\n    for step = 1:nsteps\n        for s = 1:length(RKA)Send Data            senddata(Val(dim), Val(N), mesh, sendreq, recvreq, sendQ,\n                     recvQ, d_sendelems, d_sendQ, d_recvQ, d_QL, mpicomm;\n                     ArrType=ArrType)volume RHS computation            volumerhs!(Val(dim), Val(N), d_rhsC, d_QC, d_vgeoC, d_D, mesh.realelems)Receive Data            receivedata!(Val(dim), Val(N), mesh, recvreq, recvQ, d_recvQ, d_QL)face RHS computation            fluxrhs!(Val(dim), Val(N), d_rhsL, d_QL, d_sgeo, mesh.realelems, d_vmapM, d_vmapP, d_elemtobndy)            if (visc > 0)volume Q computation                volumeQ!(Val(dim), Val(N), d_rhs_gradQC, d_QC, d_vgeoC, d_D, mesh.realelems)face Q computation                fluxQ!(Val(dim), Val(N), d_rhs_gradQL, d_QL, d_sgeo, mesh.realelems, d_vmapM, d_vmapP, d_elemtobndy)Construct grad Q                update_gradQ!(Val(dim), Val(N), d_gradQL, d_rhs_gradQL, d_vgeoL, mesh.realelems)Send Data                senddata(Val(dim), Val(N), mesh, sendreq, recvreq, sendQ,\n                         recvQ, d_sendelems, d_sendQ, d_recvQ, d_gradQL,\n                         mpicomm;ArrType=ArrType)volume grad Q computation                volumeQ!(Val(dim), Val(N), d_rhs_gradQC, d_gradQC, d_vgeoC, d_D, mesh.realelems)Receive Data                receivedata!(Val(dim), Val(N), mesh, recvreq, recvQ, d_recvQ, d_gradQL)face grad Q computation                fluxQ!(Val(dim), Val(N), d_rhs_gradQL, d_gradQL, d_sgeo, mesh.realelems, d_vmapM, d_vmapP, d_elemtobndy)\n            endupdate solution and scale RHS            updatesolution!(Val(dim), Val(N), d_rhsL, d_rhs_gradQL, d_QL, d_vgeoL, mesh.realelems,\n                            RKA[s%length(RKA)+1], RKB[s], dt, visc)\n        end\n\n        if step == 1\n            @hascuda synchronize()\n            start_time = time_ns()\n        end\n        if mpirank == 0 && (time_ns() - t1)*1e-9 > tout\n            @hascuda synchronize()\n            t1 = time_ns()\n            avg_stage_time = (time_ns() - start_time) * 1e-9 /\n            ((step-1) * length(RKA))\n            @show (step, nsteps, avg_stage_time)\n        end\n        if plotstep > 0 && step % plotstep == 0\n            Q .= d_QL\n            X = ntuple(j->reshape((@view vgeo[:, _x+j-1, :]), ntuple(j->N+1,dim)...,\n                                  nelem), dim)\n            U = reshape((@view Q[:, _U, :]), ntuple(j->(N+1),dim)..., nelem)\n            writemesh(@sprintf(\"viz/burger%dD_%s_rank_%04d_step_%05d\",\n                               dim, ArrType, mpirank, step), X...;\n                      fields=((\"h\", U),(\"U\",U)),realelems=mesh.realelems)\n        end\n    end\n\nQ .= d_QL\nrhs .= d_rhsL\nend}}}{{{ BURGER driverfunction burger(::Val{dim}, ::Val{N}, mpicomm, ic, mesh, tend, visc;\n                meshwarp=(x...)->identity(x), tout = 60, ArrType=Array,\n                plotstep=0) where {dim, N}\n    DFloat = typeof(tend)\n\n    mpirank = MPI.Comm_rank(mpicomm)\n    mpisize = MPI.Comm_size(mpicomm)Partion the mesh using a Hilbert curve based partitioning    mpirank == 0 && println(\"[CPU] partitioning mesh...\")\n    mesh = partition(mpicomm, mesh...)Connect the mesh in parallel    mpirank == 0 && println(\"[CPU] connecting mesh...\")\n    mesh = connectmesh(mpicomm, mesh...)Get the vmaps    mpirank == 0 && println(\"[CPU] computing mappings...\")\n    (vmapM, vmapP) = mappings(N, mesh.elemtoelem, mesh.elemtoface,\n                              mesh.elemtoordr)Create 1-D operators    (ξ, ω) = lglpoints(DFloat, N)\n    D = spectralderivative(ξ)Compute the geometry    mpirank == 0 && println(\"[CPU] computing metrics...\")\n    (vgeo, sgeo) = computegeometry(Val(dim), mesh, D, ξ, ω, meshwarp, vmapM)\n    (nface, nelem) = size(mesh.elemtoelem)Storage for the solution, rhs, and error    mpirank == 0 && println(\"[CPU] creating fields (CPU)...\")\n    Q = zeros(DFloat, (N+1)^dim, _nstate, nelem)\n    Qexact = zeros(DFloat, (N+1)^dim, _nstate, nelem)\n    rhs = zeros(DFloat, (N+1)^dim, _nstate, nelem)setup the initial condition    mpirank == 0 && println(\"[CPU] computing initial conditions (CPU)...\")\n    @inbounds for e = 1:nelem, i = 1:(N+1)^dim\n        x = vgeo[i, _x, e]\n        U = ic(x)\n        Q[i, _U, e] = U\n        Qexact[i, _U, e] = U\n    endCompute time step    mpirank == 0 && println(\"[CPU] computing dt (CPU)...\")\n    (base_dt,Courant) = courantnumber(Val(dim), Val(N), vgeo, Q, mpicomm)\n    mpirank == 0 && @show (base_dt,Courant)\n\n    nsteps = ceil(Int64, tend / base_dt)\n    dt = tend / nsteps\n    mpirank == 0 && @show (dt, nsteps, dt * nsteps, tend)Do time stepping    stats = zeros(DFloat, 3)\n    mpirank == 0 && println(\"[CPU] computing initial energy...\")\n    stats[1] = L2energysquared(Val(dim), Val(N), Q, vgeo, mesh.realelems)plot initial condition    mkpath(\"viz\")\n    X = ntuple(j->reshape((@view vgeo[:, _x+j-1, :]), ntuple(j->N+1,dim)...,\n                          nelem), dim)\n    U = reshape((@view Q[:, _U, :]), ntuple(j->(N+1),dim)..., nelem)\n    writemesh(@sprintf(\"viz/burger%dD_%s_rank_%04d_step_%05d\",\n                       dim, ArrType, mpirank, 0), X...;\n              fields=((\"h\", U),(\"U\",U)),realelems=mesh.realelems)\n\n    #Call Time-stepping Routine\n    mpirank == 0 && println(\"[DEV] starting time stepper...\")\n    lowstorageRK(Val(dim), Val(N), mesh, vgeo, sgeo, Q, rhs, D, dt, nsteps, tout,\n                 vmapM, vmapP, mpicomm, visc;\n                 ArrType=ArrType, plotstep=plotstep)plot final solution    X = ntuple(j->reshape((@view vgeo[:, _x+j-1, :]), ntuple(j->N+1,dim)...,\n                          nelem), dim)\n    U = reshape((@view Q[:, _U, :]), ntuple(j->(N+1),dim)..., nelem)\n    writemesh(@sprintf(\"viz/burger%dD_%s_rank_%04d_step_%05d\",\n                       dim, ArrType, mpirank, nsteps), X...;\n              fields=((\"h\", U),(\"U\",U)),realelems=mesh.realelems)\n\n    mpirank == 0 && println(\"[CPU] computing final energy...\")\n    stats[2] = L2energysquared(Val(dim), Val(N), Q, vgeo, mesh.realelems)\n    stats[3] = L2errorsquared(Val(dim), Val(N), Q, vgeo, mesh.realelems, Qexact,\n                              tend)\n\n    stats = sqrt.(MPI.allreduce(stats, MPI.SUM, mpicomm))\n\n    if  mpirank == 0\n        @show eng0 = stats[1]\n        @show engf = stats[2]\n        @show Δeng = engf - eng0\n        @show err = stats[3]\n    end\nend}}}{{{ mainfunction main()\n    DFloat = Float64\n\n    MPI.Initialized() || MPI.Init()\n    MPI.finalize_atexit()\n\n    mpicomm = MPI.COMM_WORLD\n    mpirank = MPI.Comm_rank(mpicomm)\n    mpisize = MPI.Comm_size(mpicomm)FIXME: query via hostname    @hascuda device!(mpirank % length(devices()))\n\n    #Input Parameters\n    N=8\n    Ne=10\n    visc=0.01\n    iplot=10\n    time_final=DFloat(1.0)\n    hardware=\"cpu\"\n    @show (N,Ne,visc,iplot,time_final,hardware)\n\n    #Initial Conditions\n    function ic(x...)\n        U = sin( π*x[1] ) + 0.01\n    end\n    periodic = (true, )\n\n    mesh = brickmesh((range(DFloat(0); length=Ne+1, stop=2),), periodic; part=mpirank+1, numparts=mpisize)\n\n    if hardware == \"cpu\"\n        mpirank == 0 && println(\"Running (CPU)...\")\n        burger(Val(1), Val(N), mpicomm, ic, mesh, time_final, visc;\n               ArrType=Array, tout = 10, plotstep = iplot)\n        mpirank == 0 && println()\n    elseif hardware == \"gpu\"\n        @hascuda begin\n            mpirank == 0 && println(\"Running (GPU)...\")\n            burger(Val(1), Val(N), mpicomm, ic, mesh, time_final, visc;\n                   ArrType=CuArray, tout = 10, plotstep = iplot)\n            mpirank == 0 && println()\n        end\n    end\n    nothing\nend}}}main()This page was generated using Literate.jl."
},

{
    "location": "examples/generated/1d_kernels/swe1d/#",
    "page": "1D Shallow Water Equations",
    "title": "1D Shallow Water Equations",
    "category": "page",
    "text": "EditURL = \"https://github.com/climate-machine/Canary.jl/blob/master/examples/1d_kernels/swe1d.jl\""
},

{
    "location": "examples/generated/1d_kernels/swe1d/#D-Shallow-Water-Equations-1",
    "page": "1D Shallow Water Equations",
    "title": "1D Shallow Water Equations",
    "category": "section",
    "text": ""
},

{
    "location": "examples/generated/1d_kernels/swe1d/#Introduction-1",
    "page": "1D Shallow Water Equations",
    "title": "Introduction",
    "category": "section",
    "text": "This example shows how to solve the 1D shallow water equations using vanilla DG."
},

{
    "location": "examples/generated/1d_kernels/swe1d/#Continuous-Governing-Equations-1",
    "page": "1D Shallow Water Equations",
    "title": "Continuous Governing Equations",
    "category": "section",
    "text": "We solve the following equation:fracpartial h_spartial t + nabla cdot mathbfU = 0   (11)fracpartial mathbfUpartial t + nabla cdot left( fracmathbfU otimes mathbfUh + g (h^2 - h^2_b) mathbfI_2 right) + h_s nabla h_b = 0   (12)where mathbfu=(u) and mathbfU=h mathbfu, with h=h_s(mathbfxt) + h_b(mathbfx) being the total water column with h_s and h_b being the height of the water surface and depth of the bathymetry (which we assume to be constant for simplicity), respectively, measured from a zero mean sea-level.  We employ periodic boundary conditions."
},

{
    "location": "examples/generated/1d_kernels/swe1d/#Discontinous-Galerkin-Method-1",
    "page": "1D Shallow Water Equations",
    "title": "Discontinous Galerkin Method",
    "category": "section",
    "text": "To solve Eq. (1) we use the discontinuous Galerkin method with basis functions comprised of Lagrange polynomials based on Lobatto points. Multiplying Eq. (1) by a test function psi and integrating within each element Omega_e such that Omega = bigcup_e=1^N_e Omega_e we getint_Omega_e psi fracpartial mathbfq^(e)_Npartial t dOmega_e + int_Omega_e psi nabla cdot mathbff^(e)_N dOmega_e =  int_Omega_e psi Sleft( q^(e)_N right) dOmega_e   (2)where mathbfq^(e)_N=sum_i=1^(N+1)^dim psi_i(mathbfx) mathbfq_i(t) is the finite dimensional expansion with basis functions psi(mathbfx), where mathbfq=left( h mathbfU^T right)^T andmathbff=left( mathbfU fracmathbfU otimes mathbfUh + g (h^2 - h^2_b) mathbfI_2 right)Integrating Eq. (2) by parts yieldsint_Omega_e psi fracpartial mathbfq^(e)_Npartial t dOmega_e + int_Gamma_e psi mathbfn cdot mathbff^(*e)_N dGamma_e - int_Omega_e nabla psi cdot mathbff^(e)_N dOmega_e = int_Omega_e psi Sleft( q^(e)_N right) dOmega_e   (3)where the second term on the left denotes the flux integral term (computed in \"function fluxrhs\") and the third term denotes the volume integral term (computed in \"function volumerhs\").  The superscript (*e) in the flux integral term denotes the numerical flux. Here we use the Rusanov flux."
},

{
    "location": "examples/generated/1d_kernels/swe1d/#Commented-Program-1",
    "page": "1D Shallow Water Equations",
    "title": "Commented Program",
    "category": "section",
    "text": "include(joinpath(@__DIR__,\"vtk.jl\"))\nusing MPI\nusing Canary\nusing Printf: @sprintf\nconst HAVE_CUDA = try\n    using CUDAnative\n    using CUDAdrv\n    true\ncatch\n    false\nend\nif HAVE_CUDA\n    macro hascuda(ex)\n        return :($(esc(ex)))\n    end\nelse\n    macro hascuda(ex)\n        return :()\n    end\nend{{{ reshape for CuArray@hascuda function Base.reshape(A::CuArray, dims::NTuple{N, Int}) where {N}\n    @assert prod(dims) == prod(size(A))\n    CuArray{eltype(A), length(dims)}(dims, A.buf)\nend}}}{{{ constants note the order of the fields below is also assumed in the code.const _nstate = 3\nconst _U, _h, _b = 1:_nstate\nconst stateid = (U = _U, h = _h, b = _b)\n\nconst _nvgeo = 4\nconst _ξx, _MJ, _MJI, _x = 1:_nvgeo\n\nconst _nsgeo = 3\nconst _nx, _sMJ, _vMJI = 1:_nsgeo}}}{{{ courantfunction courantnumber(::Val{dim}, ::Val{N}, vgeo, Q, mpicomm, gravity, δnl, advection) where {dim, N}\n    DFloat = eltype(Q)\n    Np = (N+1)^dim\n    (~, ~, nelem) = size(Q)\n    dt = [floatmax(DFloat)]\n    Courant = - [floatmax(DFloat)]\n    δ_wave=1\n    if advection\n        δ_wave=0\n    end\n\n    #Compute DT\n    @inbounds for e = 1:nelem, n = 1:Np\n        h, b, U = Q[n, _h, e],  Q[n, _b, e], Q[n, _U, e]\n        ξx = vgeo[n, _ξx, e]\n        H = h+b\n        u=U/H\n        dx=1.0/(2*ξx)\n        wave_speed = ( abs(u) + δ_wave*sqrt(gravity*H)*δnl + gravity*b*(1-δnl) )\n        loc_dt = 0.5*dx/wave_speed/N\n        dt[1] = min(dt[1], loc_dt)\n    end\n    dt_min=MPI.Allreduce(dt[1], MPI.MIN, mpicomm)\n\n    #Compute Courant\n    @inbounds for e = 1:nelem, n = 1:Np\n        h, b, U = Q[n, _h, e],  Q[n, _b, e], Q[n, _U, e]\n        ξx = vgeo[n, _ξx, e]\n        H = h+b\n        u=U/H\n        dx=1.0/(2*ξx)\n        wave_speed = ( abs(u) + δ_wave*sqrt(gravity*H)*δnl + gravity*b*(1-δnl) )\n        loc_Courant = wave_speed*dt_min/dx*N\n        Courant[1] = max(Courant[1], loc_Courant)\n    end\n    Courant_max=MPI.Allreduce(Courant[1], MPI.MAX, mpicomm)\n\n    (dt_min, Courant_max)\nend}}}{{{ compute geometryfunction computegeometry(::Val{dim}, mesh, D, ξ, ω, meshwarp, vmapM) where dimCompute metric terms    Nq = size(D, 1)\n    DFloat = eltype(D)\n\n    (nface, nelem) = size(mesh.elemtoelem)\n\n    crd = creategrid(Val(dim), mesh.elemtocoord, ξ)\n\n    vgeo = zeros(DFloat, Nq^dim, _nvgeo, nelem)\n    sgeo = zeros(DFloat, _nsgeo, Nq^(dim-1), nface, nelem)\n\n    (ξx, MJ, MJI, x) = ntuple(j->(@view vgeo[:, j, :]), _nvgeo)\n    J = similar(x)\n    (nx, sMJ, vMJI) = ntuple(j->(@view sgeo[ j, :, :, :]), _nsgeo)\n    sJ = similar(sMJ)\n\n    X = ntuple(j->(@view vgeo[:, _x+j-1, :]), dim)\n    creategrid!(X..., mesh.elemtocoord, ξ)\n\n    @inbounds for j = 1:length(x)(x[j]) = meshwarp(x[j],)    endCompute the metric terms    computemetric!(x, J, ξx, sJ, nx, D)\n\n    M = kron(1, ntuple(j->ω, dim)...)\n    MJ .= M .* J\n    MJI .= 1 ./ MJ\n    vMJI .= MJI[vmapM]\n\n    sM = dim > 1 ? kron(1, ntuple(j->ω, dim-1)...) : one(DFloat)\n    sMJ .= sM .* sJ\n\n    (vgeo, sgeo)\nend}}}{{{ CPU Kernels {{{ Volume RHS for 1Dfunction volumerhs!(::Val{1}, ::Val{N}, rhs::Array, Q, vgeo, D, elems, gravity, δnl) where N\n    DFloat = eltype(Q)\n    Nq = N + 1\n    nelem = size(Q)[end]\n\n    Q = reshape(Q, Nq, _nstate, nelem)\n    rhs = reshape(rhs, Nq, _nstate, nelem)\n    vgeo = reshape(vgeo, Nq, _nvgeo, nelem)\n\n    #Allocate Arrays\n    s_F = Array{DFloat}(undef, Nq, _nstate-1)\n\n    @inbounds for e in elems\n        for i = 1:Nq\n            MJ, ξx = vgeo[i,_MJ,e], vgeo[i,_ξx,e]\n            h, b, U = Q[i, _h, e], Q[i, _b, e], Q[i, _U, e]\n\n            #Get primitive variables\n            H=h + b\n            u=U/H\n\n            #Compute fluxes\n            fluxh_x = U\n            fluxU_x = (H * u * u + 0.5 * gravity * h^2) * δnl + gravity * h * b\n\n            s_F[i, _h] = MJ * (ξx * fluxh_x)\n            s_F[i, _U] = MJ * (ξx * fluxU_x)\n        endloop of ξ-grid lines        for s = 1:_nstate-1, i = 1:Nq, k = 1:Nq\n            rhs[i, s, e] += D[k, i] * s_F[k, s]\n        end\n    end\nend}}}Flux RHS for 1Dfunction fluxrhs!(::Val{1}, ::Val{N}, rhs::Array,  Q, sgeo, elems, vmapM, vmapP, elemtobndy, gravity, δnl) where N\n    DFloat = eltype(Q)\n    Np = (N+1)\n    Nfp = 1\n    nface = 2\n\n    @inbounds for e in elems\n        for f = 1:nface\n            for n = 1:Nfp\n                nxM, sMJ = sgeo[_nx, n, f, e], sgeo[_sMJ, n, f, e]\n                idM, idP = vmapM[n, f, e], vmapP[n, f, e]\n\n                eM, eP = e, ((idP - 1) ÷ Np) + 1\n                vidM, vidP = ((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1\n\n                hM = Q[vidM, _h, eM]\n                bM = Q[vidM, _b, eM]\n                UM = Q[vidM, _U, eM]\n\n                bc = elemtobndy[f, e]\n                hP = bP = UP = VP = zero(eltype(Q))\n                if bc == 0\n                    hP = Q[vidP, _h, eP]\n                    bP = Q[vidP, _b, eM]\n                    UP = Q[vidP, _U, eP]\n                elseif bc == 1\n                    UnM = nxM * UM\n                    UP = UM - 2 * UnM * nxM\n                    hP = hM\n                    bP = bM\n                else\n                    error(\"Invalid boundary conditions $bc on face $f of element $e\")\n                end\n                HM=hM + bM\n                uM = UM / HM\n                HP=hP + bP\n                uP = UP / HP\n\n                #Left Fluxes\n                fluxhM_x = UM\n                fluxUM_x = (HM * uM * uM + 0.5 * gravity * hM^2) * δnl + gravity * hM * bM\n\n                #Right Fluxes\n                fluxhP_x = UP\n                fluxUP_x = (HP * uP * uP + 0.5 * gravity * hP^2) * δnl + gravity * hP * bP\n\n                #Compute wave speed\n                λM=( abs(nxM * uM) + sqrt(gravity*HM) ) * δnl + ( sqrt(gravity*bM) ) * (1.0-δnl)\n                λP=( abs(nxM * uP) + sqrt(gravity*HP) ) * δnl + ( sqrt(gravity*bP) ) * (1.0-δnl)\n                λ = max( λM, λP )\n\n                #Compute Numerical/Rusanov Flux\n                fluxhS = (nxM * (fluxhM_x + fluxhP_x) - λ * (hP - hM)) / 2\n                fluxUS = (nxM * (fluxUM_x + fluxUP_x) - λ * (UP - UM)) / 2\n\n                #Update RHS\n                rhs[vidM, _h, eM] -= sMJ * fluxhS\n                rhs[vidM, _U, eM] -= sMJ * fluxUS\n            end\n        end\n    end\nend}}}{{{ Volume Qfunction volumeQ!(::Val{1}, ::Val{N}, rhs::Array, Q, vgeo, D, elems) where N\n    DFloat = eltype(Q)\n    Nq = N + 1\n    nelem = size(Q)[end]\n\n    Q = reshape(Q, Nq, _nstate, nelem)\n    rhs = reshape(rhs, Nq, _nstate, nelem)\n    vgeo = reshape(vgeo, Nq, _nvgeo, nelem)\n\n    #Initialize RHS vector\n    fill!( rhs, zero(rhs[1]))\n\n    #Allocate Arrays\n    s_F = Array{DFloat}(undef, Nq, _nstate-1)\n\n    @inbounds for e in elems\n        for i = 1:Nq\n            MJ, ξx = vgeo[i,_MJ,e], vgeo[i,_ξx,e]\n            h, b, U = Q[i, _h, e], Q[i, _b, e], Q[i, _U, e]\n\n            #Get primitive variables\n            H=h + b\n            u=U/H\n\n            #Compute fluxes\n            fluxh = h\n            fluxU = U\n            s_F[i, _h] = MJ * (ξx * fluxh)\n            s_F[i, _U] = MJ * (ξx * fluxU)\n        endloop of ξ-grid lines        for s = 1:_nstate-1, i = 1:Nq, k = 1:Nq\n            rhs[i, s, e] -= D[k, i] * s_F[k, s]\n        end\n    end\nend}}}Flux Qfunction fluxQ!(::Val{1}, ::Val{N}, rhs::Array,  Q, sgeo, elems, vmapM, vmapP, elemtobndy) where N\n    DFloat = eltype(Q)\n    Np = (N+1)\n    Nfp = 1\n    nface = 2\n\n    @inbounds for e in elems\n        for f = 1:nface\n            for n = 1:Nfp\n                nxM, sMJ = sgeo[_nx, n, f, e], sgeo[_sMJ, n, f, e]\n                idM, idP = vmapM[n, f, e], vmapP[n, f, e]\n\n                eM, eP = e, ((idP - 1) ÷ Np) + 1\n                vidM, vidP = ((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1\n\n                hM = Q[vidM, _h, eM]\n                bM = Q[vidM, _b, eM]\n                UM = Q[vidM, _U, eM]\n\n                bc = elemtobndy[f, e]\n                hP = bP = UP = VP = zero(eltype(Q))\n                if bc == 0\n                    hP = Q[vidP, _h, eP]\n                    bP = Q[vidP, _b, eM]\n                    UP = Q[vidP, _U, eP]\n                elseif bc == 1\n                    UnM = nxM * UM\n                    UP = UM - 2 * UnM * nxM\n                    hP = hM\n                    bP = bM\n                else\n                    error(\"Invalid boundary conditions $bc on face $f of element $e\")\n                end\n                HM=hM + bM\n                uM = UM / HM\n                HP=hP + bP\n                uP = UP / HP\n\n                #Left Fluxes\n                fluxhM = hM\n                fluxUM = UM\n\n                #Right Fluxes\n                fluxhP = hP\n                fluxUP = UP\n\n                #Compute Numerical/Rusanov Flux\n                fluxhS = 0.5*(fluxhM + fluxhP)\n                fluxUS = 0.5*(fluxUM + fluxUP)\n\n                #Update RHS\n                rhs[vidM, _h, eM] += sMJ * nxM * fluxhS\n                rhs[vidM, _U, eM] += sMJ * nxM * fluxUS\n            end\n        end\n    end\nend}}}{{{ Update grad Q solutionfunction update_gradQ!(::Val{dim}, ::Val{N}, Q, rhs, vgeo, elems) where {dim, N}\n\n    DFloat = eltype(Q)\n    Nq=(N+1)^dim\n    (~, ~, nelem) = size(Q)\n\n    @inbounds for e = elems, s = 1:_nstate-1, i = 1:Nq\n        Q[i, s, e] = rhs[i, s, e] * vgeo[i, _MJI, e]\n    end\n\nend}}}{{{ Update solutionfunction updatesolution!(::Val{dim}, ::Val{N}, rhs, Q, rhs_gradQ, vgeo, elems, rka, rkb, dt, advection, visc) where {dim, N}\n\n    DFloat = eltype(Q)\n    Nq=(N+1)^dim\n    (~, ~, nelem) = size(Q)\n\n    #Store Velocity\n    if (advection)\n\n        #Allocate local arrays\n        u=Array{DFloat,2}(undef,Nq,nelem)\n\n        @inbounds for e = elems, i = 1:Nq\n            u[i,e] = Q[i,_U,e] / ( Q[i,_h,e] + Q[i,_b,e] )\n        end\n    end\n\n    @inbounds for e = elems, s = 1:_nstate-1, i = 1:Nq\n        rhs[i, s, e] += visc*rhs_gradQ[i,s,e]\n        Q[i, s, e] += rkb * dt * rhs[i, s, e] * vgeo[i, _MJI, e]\n        rhs[i, s, e] *= rka\n    end\n\n    #Reset Velocity\n    if (advection)\n        @inbounds for e = elems, i = 1:Nq\n            Q[i,_U,e] = ( Q[i,_h,e] + Q[i,_b,e] ) * u[i,e]\n        end\n    end\nend}}}{{{ GPU kernels {{{ Volume RHS for 1D@hascuda function knl_volumerhs!(::Val{1}, ::Val{N}, rhs, Q, vgeo, D, nelem, gravity, δnl) where N\n    DFloat = eltype(D)\n    Nq = N + 1\n\n    #Point Thread to DOF and Block to element\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    #Allocate Arrays\n    s_D = @cuStaticSharedMem(eltype(D), (Nq, Nq))\n    s_F = @cuStaticSharedMem(eltype(Q), (Nq, _nstate))\n\n    rhsU = rhsV = rhsh = zero(eltype(rhs))\n    if i <= Nq && j == 1 && k == 1 && e <= nelemLoad derivative into shared memory        if k == 1\n            s_D[i, j] = D[i, j]\n        endLoad values needed into registers        MJ = vgeo[i, j, _MJ, e]\n        ξx = vgeo[i, j, _ξx, e]\n        h, b, U = Q[i, _h, e], Q[i, _b, e], Q[i, _U, e]\n        rhsh, rhsU = rhs[i, _h, e], rhs[i, _U, e]\n\n        #Get primitive variables and fluxes\n        H=h + b\n        u=U/H\n\n        fluxh_x = U\n        fluxU_x = (H * u * u + 0.5 * gravity * h^2) * δnl + gravity * h * b\n\n        s_F[i, _h] = MJ * (ξx * fluxh_x)\n        s_F[i, _U] = MJ * (ξx * fluxU_x)\n    end\n\n    sync_threads()\n\n    @inbounds if i <= Nq && j == 1 && k == 1 && e <= nelem\n        for n = 1:Nq\n            #ξ-grid lines\n            Dni = s_D[n, i]\n            rhsh += Dni * s_F[n, _h]\n            rhsU += Dni * s_F[n, _U]\n        end\n        rhs[i, _U, e] = rhsU\n        rhs[i, _h, e] = rhsh\n    end\n    nothing\nend}}}{{{ Face RHS (all dimensions)@hascuda function knl_fluxrhs!(::Val{dim}, ::Val{N}, rhs, Q, sgeo, nelem, vmapM, vmapP, elemtobndy, gravity, δnl) where {dim, N}\n  DFloat = eltype(Q)\n    Np = (N+1)^dim\n    nface = 2*dim\n\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    Nq = N+1\n    half = convert(eltype(Q), 0.5)\n\n    @inbounds if i <= Nq && j == 1 && k == 1 && e <= nelem\n        n = i + (j-1) * Nq\n        for lf = 1:2:nface\n            for f = lf:lf+1\n                nxM, sMJ = sgeo[_nx, n, f, e], sgeo[_sMJ, n, f, e]\n                (idM, idP) = (vmapM[n, f, e], vmapP[n, f, e])\n\n                (eM, eP) = (e, ((idP - 1) ÷ Np) + 1)\n                (vidM, vidP) = (((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1)\n\n                hM = Q[vidM, _h, eM]\n                bM = Q[vidM, _b, eM]\n                UM = Q[vidM, _U, eM]\n\n                bc = elemtobndy[f, e]\n                hP = bP = UP = VP = zero(eltype(Q))\n                if bc == 0\n                    hP = Q[vidP, _h, eP]\n                    bP = Q[vidP, _b, eM]\n                    UP = Q[vidP, _U, eP]\n                elseif bc == 1\n                    UnM = nxM * UM\n                    UP = UM - 2 * UnM * nxM\n                    hP = hM\n                    bP = bM         else\n             error(\"Invalid boundary conditions $bc on face $f of element $e\")                end\n                HM=hM + bM\n                uM = UM / HM\n                HP=hP + bP\n                uP = UP / HP\n\n                #Left Fluxes\n                fluxhM_x = UM\n                fluxUM_x = (HM * uM * uM + 0.5 * gravity * hM * hM) * δnl + gravity * hM * bM\n\n                #Right Fluxes\n                fluxhP_x = UP\n                fluxUP_x = (HP * uP * uP + 0.5 * gravity * hP * hP) * δnl + gravity * hP * bP\n\n                #Compute wave speed\n                λM=( abs(nxM * uM) + CUDAnative.sqrt(gravity*HM) ) * δnl + ( CUDAnative.sqrt(gravity*bM) ) * (1.0-δnl)\n                λP=( abs(nxM * uP) + CUDAnative.sqrt(gravity*HP) ) * δnl + ( CUDAnative.sqrt(gravity*bP) ) * (1.0-δnl)\n              λ = max( λM, λP )\n\n                #Compute Numerical Flux and Update\n                fluxhS = (nxM * (fluxhM_x + fluxhP_x) - λ * (hP - hM)) / 2\n                fluxUS = (nxM * (fluxUM_x + fluxUP_x) - λ * (UP - UM)) / 2\n\n                #Update RHS\n                rhs[vidM, _h, eM] -= sMJ * fluxhS\n                rhs[vidM, _U, eM] -= sMJ * fluxUS\n            end\n            sync_threads()\n        end\n    end\n    nothing\nend}}}{{{ Update solution (for all dimensions)@hascuda function knl_updatesolution!(::Val{dim}, ::Val{N}, rhs, Q, vgeo, nelem, rka, rkb, dt, advection) where {dim, N}\n  (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    Nq = N+1\n    @inbounds if i <= Nq && j <= Nq && k <= Nq && e <= nelem\n        n = i + (j-1) * Nq + (k-1) * Nq * Nq\n        MJI = vgeo[n, _MJI, e]\n        for s = 1:_nstate-1\n            Q[n, s, e] += rkb * dt * rhs[n, s, e] * MJI\n            rhs[n, s, e] *= rka\n        end\n    end\n    nothing\nend}}}{{{ Fill sendQ on device with Q (for all dimensions)@hascuda function knl_fillsendQ!(::Val{dim}, ::Val{N}, sendQ, Q, sendelems) where {N, dim}\n    Nq = N + 1\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    @inbounds if i <= Nq && j <= Nq && k <= Nq && e <= length(sendelems)\n        n = i + (j-1) * Nq + (k-1) * Nq * Nq\n        re = sendelems[e]\n        for s = 1:_nstate\n            sendQ[n, s, e] = Q[n, s, re]\n        end\n    end\n    nothing\nend}}}{{{ Fill Q on device with recvQ (for all dimensions)@hascuda function knl_transferrecvQ!(::Val{dim}, ::Val{N}, Q, recvQ, nelem, nrealelem) where {N, dim}\n    Nq = N + 1\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    @inbounds if i <= Nq && j <= Nq && k <= Nq && e <= nelem\n        n = i + (j-1) * Nq + (k-1) * Nq * Nq\n        for s = 1:_nstate\n            Q[n, s, nrealelem + e] = recvQ[n, s, e]\n        end\n    end\n    nothing\nend}}}{{{ MPI Buffer handlingfunction fillsendQ!(::Val{dim}, ::Val{N}, sendQ, d_sendQ::Array, Q,\n                    sendelems) where {dim, N}\n    sendQ[:, :, :] .= Q[:, :, sendelems]\nend\n\n@hascuda function fillsendQ!(::Val{dim}, ::Val{N}, sendQ, d_sendQ::CuArray,\n                             d_QL, d_sendelems) where {dim, N}\n    nsendelem = length(d_sendelems)\n    if nsendelem > 0\n        @cuda(threads=ntuple(j->N+1, dim), blocks=nsendelem,\n              knl_fillsendQ!(Val(dim), Val(N), d_sendQ, d_QL, d_sendelems))\n        sendQ .= d_sendQ\n    end\nend\n\n@hascuda function transferrecvQ!(::Val{dim}, ::Val{N}, d_recvQ::CuArray, recvQ,\n                                 d_QL, nrealelem) where {dim, N}\n    nrecvelem = size(recvQ)[end]\n    if nrecvelem > 0\n        d_recvQ .= recvQ\n        @cuda(threads=ntuple(j->N+1, dim), blocks=nrecvelem,\n              knl_transferrecvQ!(Val(dim), Val(N), d_QL, d_recvQ, nrecvelem,\n                                 nrealelem))\n    end\nend\n\nfunction transferrecvQ!(::Val{dim}, ::Val{N}, d_recvQ::Array, recvQ, Q,\n                        nrealelem) where {dim, N}\n    Q[:, :, nrealelem+1:end] .= recvQ[:, :, :]\nend}}}{{{ GPU kernel wrappers@hascuda function volumerhs!(::Val{dim}, ::Val{N}, d_rhsC::CuArray, d_QC, d_vgeoC, d_D, elems, gravity, δnl) where {dim, N}\n    nelem = length(elems)\n    @cuda(threads=ntuple(j->N+1, dim), blocks=nelem,\n          knl_volumerhs!(Val(dim), Val(N), d_rhsC, d_QC, d_vgeoC, d_D, nelem, gravity, δnl))\nend\n\n@hascuda function fluxrhs!(::Val{dim}, ::Val{N}, d_rhsL::CuArray, d_QL, d_sgeo, elems, d_vmapM, d_vmapP, d_elemtobndy, gravity, δnl) where {dim, N}\n    nelem = length(elems)\n    @cuda(threads=(ntuple(j->N+1, dim-1)..., 1), blocks=nelem,\n          knl_fluxrhs!(Val(dim), Val(N), d_rhsL, d_QL, d_sgeo, nelem, d_vmapM, d_vmapP, d_elemtobndy, gravity, δnl))\nend\n\n@hascuda function updatesolution!(::Val{dim}, ::Val{N}, d_rhsL::CuArray, d_QL, d_vgeoL, elems, rka, rkb, dt, advection) where {dim, N}\n    nelem = length(elems)\n    @cuda(threads=ntuple(j->N+1, dim), blocks=nelem,\n          knl_updatesolution!(Val(dim), Val(N), d_rhsL, d_QL, d_vgeoL, nelem, rka, rkb, dt, advection))\nend}}}{{{ L2 Error (for all dimensions)function L2errorsquared(::Val{dim}, ::Val{N}, Q, vgeo, elems, Qex, t) where {dim, N}\n    DFloat = eltype(Q)\n    Np = (N+1)^dim\n    (~, nstate, nelem) = size(Q)\n\n    err = zero(DFloat)\n\n    @inbounds for e = elems, i = 1:Np\n        X = ntuple(j -> vgeo[i, _x-1+j, e] - Q[i, _U-1+j, e]*t, Val(dim))\n        diff = Q[i, _h, e] - Qex[i, _h, e]\n        err += vgeo[i, _MJ, e] * diff^2\n    end\n\n    err\nend}}}{{{ L2 Energy (for all dimensions)function L2energysquared(::Val{dim}, ::Val{N}, Q, vgeo, elems) where {dim, N}\n    DFloat = eltype(Q)\n    Np = (N+1)^dim\n    (~, nstate, nelem) = size(Q)\n\n    energy = zero(DFloat)\n\n    @inbounds for e = elems, q = 1:nstate, i = 1:Np\n        energy += vgeo[i, _MJ, e] * Q[i, q, e]^2\n    end\n\n    energy\nend}}}{{{ Send Datafunction senddata(::Val{dim}, ::Val{N}, mesh, sendreq, recvreq, sendQ,\n                  recvQ, d_sendelems, d_sendQ, d_recvQ, d_QL, mpicomm;\n                  ArrType=ArrType) where {dim, N}\n    DFloat = eltype(d_QL)\n    mpirank = MPI.Comm_rank(mpicomm)Create send and recv request array    nnabr = length(mesh.nabrtorank)\n    d_sendelems = ArrType(mesh.sendelems)\n    nrealelem = length(mesh.realelems)post MPI receives    for n = 1:nnabr\n        recvreq[n] = MPI.Irecv!((@view recvQ[:, :, mesh.nabrtorecv[n]]),\n                                mesh.nabrtorank[n], 777, mpicomm)\n    endwait on (prior) MPI sends    MPI.Waitall!(sendreq)pack data from d_QL into send buffer    fillsendQ!(Val(dim), Val(N), sendQ, d_sendQ, d_QL, d_sendelems)post MPI sends    for n = 1:nnabr\n        sendreq[n] = MPI.Isend((@view sendQ[:, :, mesh.nabrtosend[n]]),\n                               mesh.nabrtorank[n], 777, mpicomm)\n    end\nend}}}{{{ Receive Datafunction receivedata!(::Val{dim}, ::Val{N}, mesh, recvreq,\n                      recvQ, d_recvQ, d_QL) where {dim, N}\n    DFloat = eltype(d_QL)\n    nrealelem = length(mesh.realelems)wait on MPI receives    MPI.Waitall!(recvreq)copy data to state vector d_QL    transferrecvQ!(Val(dim), Val(N), d_recvQ, recvQ, d_QL, nrealelem)\n\nend}}}{{{ RK loopfunction lowstorageRK(::Val{dim}, ::Val{N}, mesh, vgeo, sgeo, Q, rhs, D,\n                      dt, nsteps, tout, vmapM, vmapP, mpicomm,\n                      gravity, δnl, advection, visc;\n                      ArrType=ArrType, plotstep=0) where {dim, N}\n    DFloat = eltype(Q)\n    mpirank = MPI.Comm_rank(mpicomm)Fourth-order, low-storage, Runge–Kutta scheme of Carpenter and Kennedy (1994) ((5,4) 2N-Storage RK scheme.Ref: @TECHREPORT{CarpenterKennedy1994,   author = {M.~H. Carpenter and C.~A. Kennedy},   title = {Fourth-order {2N-storage} {Runge-Kutta} schemes},   institution = {National Aeronautics and Space Administration},   year = {1994},   number = {NASA TM-109112},   address = {Langley Research Center, Hampton, VA}, }    RKA = (DFloat(0),\n           DFloat(-567301805773)  / DFloat(1357537059087),\n           DFloat(-2404267990393) / DFloat(2016746695238),\n           DFloat(-3550918686646) / DFloat(2091501179385),\n           DFloat(-1275806237668) / DFloat(842570457699 ))\n\n    RKB = (DFloat(1432997174477) / DFloat(9575080441755 ),\n           DFloat(5161836677717) / DFloat(13612068292357),\n           DFloat(1720146321549) / DFloat(2090206949498 ),\n           DFloat(3134564353537) / DFloat(4481467310338 ),\n           DFloat(2277821191437) / DFloat(14882151754819))\n\n    RKC = (DFloat(0),\n           DFloat(1432997174477) / DFloat(9575080441755),\n           DFloat(2526269341429) / DFloat(6820363962896),\n           DFloat(2006345519317) / DFloat(3224310063776),\n           DFloat(2802321613138) / DFloat(2924317926251))Create send and recv request array    nnabr = length(mesh.nabrtorank)\n    sendreq = fill(MPI.REQUEST_NULL, nnabr)\n    recvreq = fill(MPI.REQUEST_NULL, nnabr)Create send and recv buffer    sendQ = zeros(DFloat, (N+1)^dim, size(Q,2), length(mesh.sendelems))\n    recvQ = zeros(DFloat, (N+1)^dim, size(Q,2), length(mesh.ghostelems))\n\n    nrealelem = length(mesh.realelems)\n    nsendelem = length(mesh.sendelems)\n    nrecvelem = length(mesh.ghostelems)\n    nelem = length(mesh.elems)\n\n    #Create Device Arrays\n    d_QL, d_rhsL = ArrType(Q), ArrType(rhs)\n    d_vgeoL, d_sgeo = ArrType(vgeo), ArrType(sgeo)\n    d_vmapM, d_vmapP = ArrType(vmapM), ArrType(vmapP)\n    d_sendelems, d_elemtobndy = ArrType(mesh.sendelems), ArrType(mesh.elemtobndy)\n    d_sendQ, d_recvQ = ArrType(sendQ), ArrType(recvQ)\n    d_D = ArrType(D)\n    d_gradQL = ArrType(Q)\n    d_rhs_gradQL = ArrType(rhs)\n\n    #Template Reshape Arrays\n    Qshape    = (fill(N+1, dim)..., size(Q, 2), size(Q, 3))\n    vgeoshape = (fill(N+1, dim)..., _nvgeo, size(Q, 3))\n\n    #Reshape Device Arrays\n    d_QC = reshape(d_QL, Qshape)\n    d_rhsC = reshape(d_rhsL, Qshape...)\n    d_vgeoC = reshape(d_vgeoL, vgeoshape)\n    d_gradQC = reshape(d_gradQL, Qshape)\n    d_rhs_gradQC = reshape(d_rhs_gradQL, Qshape)\n\n    start_time = t1 = time_ns()\n    for step = 1:nsteps\n        for s = 1:length(RKA)Send Data            senddata(Val(dim), Val(N), mesh, sendreq, recvreq, sendQ,\n                     recvQ, d_sendelems, d_sendQ, d_recvQ, d_QL, mpicomm;\n                     ArrType=ArrType)volume RHS computation            volumerhs!(Val(dim), Val(N), d_rhsC, d_QC, d_vgeoC, d_D, mesh.realelems, gravity, δnl)Receive Data            receivedata!(Val(dim), Val(N), mesh, recvreq, recvQ, d_recvQ, d_QL)face RHS computation            fluxrhs!(Val(dim), Val(N), d_rhsL, d_QL, d_sgeo, mesh.realelems, d_vmapM, d_vmapP, d_elemtobndy, gravity, δnl)            if (visc > 0)volume Q computation                volumeQ!(Val(dim), Val(N), d_rhs_gradQC, d_QC, d_vgeoC, d_D, mesh.realelems)face Q computation                fluxQ!(Val(dim), Val(N), d_rhs_gradQL, d_QL, d_sgeo, mesh.realelems, d_vmapM, d_vmapP, d_elemtobndy)Construct grad Q                update_gradQ!(Val(dim), Val(N), d_gradQL, d_rhs_gradQL, d_vgeoL, mesh.realelems)Send Data                senddata(Val(dim), Val(N), mesh, sendreq, recvreq, sendQ,\n                         recvQ, d_sendelems, d_sendQ, d_recvQ, d_gradQL,\n                         mpicomm;ArrType=ArrType)volume grad Q computation                volumeQ!(Val(dim), Val(N), d_rhs_gradQC, d_gradQC, d_vgeoC, d_D, mesh.realelems)Receive Data                receivedata!(Val(dim), Val(N), mesh, recvreq, recvQ, d_recvQ, d_gradQL)face grad Q computation                fluxQ!(Val(dim), Val(N), d_rhs_gradQL, d_gradQL, d_sgeo, mesh.realelems, d_vmapM, d_vmapP, d_elemtobndy)\n            endupdate solution and scale RHS            updatesolution!(Val(dim), Val(N), d_rhsL, d_QL, d_rhs_gradQL, d_vgeoL, mesh.realelems,\n                            RKA[s%length(RKA)+1], RKB[s], dt, advection, visc)\n        end\n        if step == 1\n            @hascuda synchronize()\n            start_time = time_ns()\n        end\n        if mpirank == 0 && (time_ns() - t1)*1e-9 > tout\n            @hascuda synchronize()\n            t1 = time_ns()\n            avg_stage_time = (time_ns() - start_time) * 1e-9 /\n            ((step-1) * length(RKA))\n            @show (step, nsteps, avg_stage_time)\n        endTODO: Fix VTK for 1-D        if plotstep > 0 && step % plotstep == 0\n            Q .= d_QL\n            X = ntuple(j->reshape((@view vgeo[:, _x+j-1, :]), ntuple(j->N+1,dim)...,\n                                  nelem), dim)\n            h = reshape((@view Q[:, _h, :]), ntuple(j->(N+1),dim)..., nelem)\n            b = reshape((@view Q[:, _b, :]), ntuple(j->(N+1),dim)..., nelem)\n            U = reshape((@view Q[:, _U, :]), ntuple(j->(N+1),dim)..., nelem) ./ (h.+b)\n            writemesh(@sprintf(\"viz/swe%dD_%s_rank_%04d_step_%05d\",\n                               dim, ArrType, mpirank, step), X...;\n                      fields=((\"h\", h),(\"b\",b),(\"U\",U)),realelems=mesh.realelems)\n        end\n    end\n\nQ .= d_QL\nrhs .= d_rhsL\nend}}}{{{ SWE driverfunction swe(::Val{dim}, ::Val{N}, mpicomm, ic, mesh, tend, gravity, δnl,\n             advection, visc; meshwarp=(x...)->identity(x), tout = 60, ArrType=Array,\n             plotstep=0) where {dim, N}\n    DFloat = typeof(tend)\n\n    mpirank = MPI.Comm_rank(mpicomm)\n    mpisize = MPI.Comm_size(mpicomm)Partion the mesh using a Hilbert curve based partitioning    mpirank == 0 && println(\"[CPU] partitioning mesh...\")\n    mesh = partition(mpicomm, mesh...)Connect the mesh in parallel    mpirank == 0 && println(\"[CPU] connecting mesh...\")\n    mesh = connectmesh(mpicomm, mesh...)Get the vmaps    mpirank == 0 && println(\"[CPU] computing mappings...\")\n    (vmapM, vmapP) = mappings(N, mesh.elemtoelem, mesh.elemtoface,\n                              mesh.elemtoordr)Create 1-D operators    (ξ, ω) = lglpoints(DFloat, N)\n    D = spectralderivative(ξ)Compute the geometry    mpirank == 0 && println(\"[CPU] computing metrics...\")\n    (vgeo, sgeo) = computegeometry(Val(dim), mesh, D, ξ, ω, meshwarp, vmapM)\n    (nface, nelem) = size(mesh.elemtoelem)Storage for the solution, rhs, and error    mpirank == 0 && println(\"[CPU] creating fields (CPU)...\")\n    Q = zeros(DFloat, (N+1)^dim, _nstate, nelem)\n    Qexact = zeros(DFloat, (N+1)^dim, _nstate, nelem)\n    rhs = zeros(DFloat, (N+1)^dim, _nstate, nelem)setup the initial condition    mpirank == 0 && println(\"[CPU] computing initial conditions (CPU)...\")\n    @inbounds for e = 1:nelem, i = 1:(N+1)^dim\n        x = vgeo[i, _x, e]\n        h, b, U = ic(x)\n        Q[i, _h, e] = h\n        Q[i, _b, e] = b\n        Q[i, _U, e] = U\n        Qexact[i, _h, e] = h\n        Qexact[i, _b, e] = b\n        Qexact[i, _U, e] = U\n    endCompute time step    mpirank == 0 && println(\"[CPU] computing dt (CPU)...\")\n    (base_dt,Courant) = courantnumber(Val(dim), Val(N), vgeo, Q, mpicomm, gravity, δnl, advection)\n    mpirank == 0 && @show (base_dt,Courant)\n\n    nsteps = ceil(Int64, tend / base_dt)\n    dt = tend / nsteps\n    mpirank == 0 && @show (dt, nsteps, dt * nsteps, tend)Do time stepping    stats = zeros(DFloat, 3)\n    mpirank == 0 && println(\"[CPU] computing initial energy...\")\n    stats[1] = L2energysquared(Val(dim), Val(N), Q, vgeo, mesh.realelems)plot initial condition    mkpath(\"viz\")\n    X = ntuple(j->reshape((@view vgeo[:, _x+j-1, :]), ntuple(j->N+1,dim)...,\n                          nelem), dim)\n    h = reshape((@view Q[:, _h, :]), ntuple(j->(N+1),dim)..., nelem)\n    b = reshape((@view Q[:, _b, :]), ntuple(j->(N+1),dim)..., nelem)\n    U = reshape((@view Q[:, _U, :]), ntuple(j->(N+1),dim)..., nelem) ./ (h.+b)\n    writemesh(@sprintf(\"viz/swe%dD_%s_rank_%04d_step_%05d\",\n                       dim, ArrType, mpirank, 0), X...;\n              fields=((\"h\", h),(\"b\",b),(\"U\",U)),realelems=mesh.realelems)\n\n    #Call Time-stepping Routine\n    mpirank == 0 && println(\"[DEV] starting time stepper...\")\n    lowstorageRK(Val(dim), Val(N), mesh, vgeo, sgeo, Q, rhs, D, dt, nsteps, tout,\n                 vmapM, vmapP, mpicomm, gravity, δnl, advection, visc;\n                 ArrType=ArrType, plotstep=plotstep)plot final solution    X = ntuple(j->reshape((@view vgeo[:, _x+j-1, :]), ntuple(j->N+1,dim)...,\n                          nelem), dim)\n    h = reshape((@view Q[:, _h, :]), ntuple(j->(N+1),dim)..., nelem)\n    b = reshape((@view Q[:, _b, :]), ntuple(j->(N+1),dim)..., nelem)\n    U = reshape((@view Q[:, _U, :]), ntuple(j->(N+1),dim)..., nelem) ./ (h.+b)\n    writemesh(@sprintf(\"viz/swe%dD_%s_rank_%04d_step_%05d\",\n                       dim, ArrType, mpirank, nsteps), X...;\n              fields=((\"h\", h),(\"b\",b),(\"U\",U)),realelems=mesh.realelems)\n\n    mpirank == 0 && println(\"[CPU] computing final energy...\")\n    stats[2] = L2energysquared(Val(dim), Val(N), Q, vgeo, mesh.realelems)\n    stats[3] = L2errorsquared(Val(dim), Val(N), Q, vgeo, mesh.realelems, Qexact,\n                              tend)\n\n    stats = sqrt.(MPI.allreduce(stats, MPI.SUM, mpicomm))\n\n    if  mpirank == 0\n        @show eng0 = stats[1]\n        @show engf = stats[2]\n        @show Δeng = engf - eng0\n        @show err = stats[3]\n    end\nend}}}{{{ mainfunction main()\n    DFloat = Float64\n\n    MPI.Initialized() || MPI.Init()\n    MPI.finalize_atexit()\n\n    mpicomm = MPI.COMM_WORLD\n    mpirank = MPI.Comm_rank(mpicomm)\n    mpisize = MPI.Comm_size(mpicomm)FIXME: query via hostname    @hascuda device!(mpirank % length(devices()))\n\n    #Input Parameters\n    N=8\n    Ne=10\n    visc=0.01\n    iplot=10\n    δnl=1\n    icase=10\n    time_final=DFloat(0.2)\n    hardware=\"cpu\"\n    @show (N,Ne,visc,iplot,δnl,icase,time_final,hardware)\n\n    #Initial Conditions\n    if icase == 1 #advection\n        function ic1(x...)\n            r = sqrt( (x[1]-0.5)^2)\n            h = 0.5 * exp(-100.0 * r^2)\n            b = 1.0\n            H = h + b\n            U = H*(1.0)\n            h, b, U\n        end\n        ic = ic1\n        periodic = (true, )\n        advection = true\n        gravity = 0\n    elseif icase == 10 #shallow water with Periodic BCs\n        function ic10(x...)\n            r = sqrt( (x[1]-0.5)^2 )\n            h = 0.5 * exp(-100.0 * r^2)\n            b=1.0\n            H = h + b\n            U = H*(0.0)\n            h, b, U\n        end\n        ic = ic10\n        periodic = (true, )\n        advection = false\n        gravity = 10\n    elseif icase == 100 #shallow water with NFBC\n        function ic100(x...)\n            r = sqrt( (x[1]-0.5)^2 )\n            h = 0.5 * exp(-100.0 * r^2)\n            b=1.0\n            H = h + b\n            U = H*(0.0)\n            h, b, U\n        end\n        ic = ic100\n        periodic = (false, )\n        advection = false\n        gravity = 10\n    end\n\n    mesh = brickmesh((range(DFloat(0); length=Ne+1, stop=1),), periodic; part=mpirank+1, numparts=mpisize)\n\n    if hardware == \"cpu\"\n        mpirank == 0 && println(\"Running (CPU)...\")\n        swe(Val(1), Val(N), mpicomm, ic, mesh, time_final, gravity, δnl, advection, visc;\n            ArrType=Array, tout = 10, plotstep = iplot)\n        mpirank == 0 && println()\n    elseif hardware == \"gpu\"\n        @hascuda begin\n            mpirank == 0 && println(\"Running (GPU)...\")\n            swe(Val(1), Val(N), mpicomm, ic, mesh, time_final, gravity, δnl, advection, visc;\n                ArrType=CuArray, tout = 10, plotstep = iplot)\n            mpirank == 0 && println()\n        end\n    end\n    nothing\nend}}}main()This page was generated using Literate.jl."
},

{
    "location": "examples/generated/2d_kernels/LDG2d/#",
    "page": "2D Diffusion Equation Example",
    "title": "2D Diffusion Equation Example",
    "category": "page",
    "text": "EditURL = \"https://github.com/climate-machine/Canary.jl/blob/master/examples/2d_kernels/LDG2d.jl\""
},

{
    "location": "examples/generated/2d_kernels/LDG2d/#D-Diffusion-Equation-Example-1",
    "page": "2D Diffusion Equation Example",
    "title": "2D Diffusion Equation Example",
    "category": "section",
    "text": ""
},

{
    "location": "examples/generated/2d_kernels/LDG2d/#Introduction-1",
    "page": "2D Diffusion Equation Example",
    "title": "Introduction",
    "category": "section",
    "text": "This example shows how to construct a 2nd derivative with DG using LDG."
},

{
    "location": "examples/generated/2d_kernels/LDG2d/#Continuous-Governing-Equations-1",
    "page": "2D Diffusion Equation Example",
    "title": "Continuous Governing Equations",
    "category": "section",
    "text": "We discretize the operator:nabla^2 q(xy)   (1)in the following two-step process. First we discretizemathbfQ(xy) = nabla q(xy)   (2)followed bynabla cdot mathbfQ (xy) =  nabla^2 q(xy)   (3)"
},

{
    "location": "examples/generated/2d_kernels/LDG2d/#Local-Discontinous-Galerkin-(LDG)-Method-1",
    "page": "2D Diffusion Equation Example",
    "title": "Local Discontinous Galerkin (LDG) Method",
    "category": "section",
    "text": "Discretizing Eq. (2) we getint_Omega_e mathbfPsi cdot mathbfQ^(e)_N dOmega_e = int_Omega_e mathbfPsi cdot nabla q^(e)_N dOmega_e   (4)where q^(e)_N=sum_i=1^(N+1)^dim psi_i(mathbfx) q_i and  mathbfQ^(e)_N=sum_i=1^(N+1)^dim psi_i(mathbfx) mathbfQ_i are the finite dimensional expansion with basis functions psi(mathbfx) and mathbfPsi is the block diagonal tensor with blocks comprised of psi as followsmathbfPsi = left(beginarraycc\npsi  0 \n0 psi\nendarray\nright)Integrating Eq. (4) by parts yieldsint_Omega_e mathbfPsi cdot mathbfQ^(e)_N dOmega_e = int_Gamma_e left( mathbfn cdot mathbfPsi right) q^(*e)_N dGamma_e - int_Omega_e left( nabla cdot mathbfPsi right) q^(e)_N dOmega_e   (5)Equation (5) represents the approximation of the gradient operator on the variable q where the first term on the right denotes the flux integral term (computed in \"function fluxgrad\") and the second term on the right denotes the volume integral term (computed in \"function volumegrad\").  The superscript (*e) in the flux integral term denotes the numerical flux. Here we use the average flux. In matrix form, Eq. (5) becomesM^(e)_ij mathbfQ^(e)_j = mathbfF_ij q^(*e)_j - widetildemathbfD^(e) q^(e)_j   (6)Next, integrating Eq. (3) by parts gives a similar form to Eq. (6) as followsM^(e)_ij left( nabla^2 q^(e) right)_j = mathbfF_ij^T mathbfQ^(*e)_j - left( widetildemathbfD^(e)right)^T mathbfQ^(e)_j   (7)Equation (7) represents the approximation of the divergence operator on the vector mathbfQ where the first term on the right denotes the flux integral (computed in \"function flux_div\") and the second term on the right denotes the volume integral term (computed in \"function volume_div\")."
},

{
    "location": "examples/generated/2d_kernels/LDG2d/#Commented-Program-1",
    "page": "2D Diffusion Equation Example",
    "title": "Commented Program",
    "category": "section",
    "text": "include(joinpath(@__DIR__,\"vtk.jl\"))\nusing MPI\nusing Canary\nusing Printf: @sprintf\nconst HAVE_CUDA = try\n    using CUDAnative\n    using CUDAdrv\n    true\ncatch\n    false\nend\nif HAVE_CUDA\n    macro hascuda(ex)\n        return :($(esc(ex)))\n    end\nelse\n    macro hascuda(ex)\n        return :()\n    end\nend{{{ reshape for CuArray@hascuda function Base.reshape(A::CuArray, dims::NTuple{N, Int}) where {N}\n    @assert prod(dims) == prod(size(A))\n    CuArray{eltype(A), length(dims)}(dims, A.buf)\nend}}}{{{ constants note the order of the fields below is also assumed in the code.const _nstate = 4\nconst _U, _V, _h, _b = 1:_nstate\nconst stateid = (U = _U, V = _V, h = _h, b = _b)\n\nconst _nvgeo = 8\nconst _ξx, _ηx, _ξy, _ηy, _MJ, _MJI, _x, _y, = 1:_nvgeo\n\nconst _nsgeo = 4\nconst _nx, _ny, _sMJ, _vMJI = 1:_nsgeo}}}{{{ courantfunction courantnumber(::Val{dim}, ::Val{N}, vgeo, Q, mpicomm, gravity, δnl, advection) where {dim, N}\n    DFloat = eltype(Q)\n    Np = (N+1)^dim\n    (~, ~, nelem) = size(Q)\n    dt = [floatmax(DFloat)]\n    Courant = - [floatmax(DFloat)]\n    δ_wave=1\n    if advection\n        δ_wave=0\n    end\n\n    @inbounds for e = 1:nelem, n = 1:Np\n        h, b, U, V = Q[n, _h, e],  Q[n, _b, e], Q[n, _U, e], Q[n, _V, e]\n        ξx, ξy, ηx, ηy = vgeo[n, _ξx, e], vgeo[n, _ξy, e],\n        vgeo[n, _ηx, e], vgeo[n, _ηy, e]\n        H = h+b\n        u=U/H\n        v=V/H\n        dx=sqrt( (1.0/(2*ξx))^2 + (1.0/(2*ηy))^2 )\n        vel=sqrt( u^2 + v^2 )\n        wave_speed = (vel + δ_wave*sqrt(gravity*H)*δnl + gravity*b*(1-δnl))\n        loc_dt = 1.0*dx/wave_speed/N\n        dt[1] = min(dt[1], loc_dt)\n    end\n    dt_min=MPI.Allreduce(dt[1], MPI.MIN, mpicomm)\n\n        #Compute Courant\n    @inbounds for e = 1:nelem, n = 1:Np\n        h, b, U, V = Q[n, _h, e],  Q[n, _b, e], Q[n, _U, e], Q[n, _V, e]\n        ξx, ξy, ηx, ηy = vgeo[n, _ξx, e], vgeo[n, _ξy, e],\n        vgeo[n, _ηx, e], vgeo[n, _ηy, e]\n        H = h+b\n        u=U/H\n        v=V/H\n        dx=sqrt( (1.0/(2*ξx))^2 + (1.0/(2*ηy))^2 )\n        vel=sqrt( u^2 + v^2 )\n        wave_speed = (vel + δ_wave*sqrt(gravity*H)*δnl + gravity*b*(1-δnl))\n        loc_Courant = wave_speed*dt_min/dx*N\n        Courant[1] = max(Courant[1], loc_Courant)\n    end\n    Courant_max=MPI.Allreduce(Courant[1], MPI.MAX, mpicomm)\n\n    (dt_min, Courant_max)\nend}}}{{{ compute geometryfunction computegeometry(::Val{dim}, mesh, D, ξ, ω, meshwarp, vmapM) where dimCompute metric terms    Nq = size(D, 1)\n    DFloat = eltype(D)\n\n    (nface, nelem) = size(mesh.elemtoelem)\n\n    crd = creategrid(Val(dim), mesh.elemtocoord, ξ)\n\n    vgeo = zeros(DFloat, Nq^dim, _nvgeo, nelem)\n    sgeo = zeros(DFloat, _nsgeo, Nq^(dim-1), nface, nelem)\n\n    (ξx, ηx, ξy, ηy, MJ, MJI, x, y) =\n        ntuple(j->(@view vgeo[:, j, :]), _nvgeo)\n    J = similar(x)\n    (nx, ny, sMJ, vMJI) = ntuple(j->(@view sgeo[ j, :, :, :]), _nsgeo)\n    sJ = similar(sMJ)\n\n    X = ntuple(j->(@view vgeo[:, _x+j-1, :]), dim)\n    creategrid!(X..., mesh.elemtocoord, ξ)\n\n    @inbounds for j = 1:length(x)\n        (x[j], y[j]) = meshwarp(x[j], y[j])\n    endCompute the metric terms    computemetric!(x, y, J, ξx, ηx, ξy, ηy, sJ, nx, ny, D)\n\n    M = kron(1, ntuple(j->ω, dim)...)\n    MJ .= M .* J\n    MJI .= 1 ./ MJ\n    vMJI .= MJI[vmapM]\n\n    sM = dim > 1 ? kron(1, ntuple(j->ω, dim-1)...) : one(DFloat)\n    sMJ .= sM .* sJ\n\n    (vgeo, sgeo)\nend}}}{{{ CPU Kernels {{{ Volume RHS for 2Dfunction volumerhs!(::Val{dim}, ::Val{N}, rhs::Array, Q, vgeo, D, elems, gravity, δnl) where {dim, N}\n    DFloat = eltype(Q)\n    Nq = N + 1\n    nelem = size(Q)[end]\n\n    Q = reshape(Q, Nq, Nq, _nstate, nelem)\n    rhs = reshape(rhs, Nq, Nq, _nstate, nelem)\n    vgeo = reshape(vgeo, Nq, Nq, _nvgeo, nelem)\n\n    #Allocate Arrays\n    s_F = Array{DFloat}(undef, Nq, Nq, _nstate-1)\n    s_G = Array{DFloat}(undef, Nq, Nq, _nstate-1)\n\n    @inbounds for e in elems\n        for j = 1:Nq, i = 1:Nq\n            MJ = vgeo[i, j, _MJ, e]\n            ξx, ξy = vgeo[i,j,_ξx,e], vgeo[i,j,_ξy,e]\n            ηx, ηy = vgeo[i,j,_ηx,e], vgeo[i,j,_ηy,e]\n            U, V = Q[i, j, _U, e], Q[i, j, _V, e]\n            h, b = Q[i, j, _h, e], Q[i, j, _b, e]\n\n            #Get primitive variables and fluxes\n            H=h + b\n            u=U/H\n            v=V/H\n\n            #Compute fluxes\n            fluxh_x = U\n            fluxh_y = V\n            fluxU_x = (H * u * u + 0.5 * gravity * h^2) * δnl + gravity * h * b\n            fluxU_y = (H * u * v) * δnl\n            fluxV_x = (H * v * u) * δnl\n            fluxV_y = (H * v * v + 0.5 * gravity * h^2) * δnl + gravity * h * b\n\n            s_F[i, j, _h] = MJ * (ξx * fluxh_x + ξy * fluxh_y)\n            s_F[i, j, _U] = MJ * (ξx * fluxU_x + ξy * fluxU_y)\n            s_F[i, j, _V] = MJ * (ξx * fluxV_x + ξy * fluxV_y)\n\n            s_G[i, j, _h] = MJ * (ηx * fluxh_x + ηy * fluxh_y)\n            s_G[i, j, _U] = MJ * (ηx * fluxU_x + ηy * fluxU_y)\n            s_G[i, j, _V] = MJ * (ηx * fluxV_x + ηy * fluxV_y)\n        endloop of ξ-grid lines        for s = 1:_nstate-1, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, s, e] += D[n, i] * s_F[n, j, s]\n        endloop of η-grid lines        for s = 1:_nstate-1, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, s, e] += D[n, j] * s_G[i, n, s]\n        end\n    end\nend}}}Flux RHS for 2Dfunction fluxrhs!(::Val{dim}, ::Val{N}, rhs::Array,  Q, sgeo, elems, vmapM, vmapP, elemtobndy, gravity, δnl) where {dim, N}\n    DFloat = eltype(Q)\n    Np = (N+1)^dim\n    Nfp = (N+1)^(dim-1)\n    nface = 2*dim\n\n    @inbounds for e in elems\n        for f = 1:nface\n            for n = 1:Nfp\n                nxM, nyM, sMJ = sgeo[_nx, n, f, e], sgeo[_ny, n, f, e], sgeo[_sMJ, n, f, e]\n                idM, idP = vmapM[n, f, e], vmapP[n, f, e]\n\n                eM, eP = e, ((idP - 1) ÷ Np) + 1\n                vidM, vidP = ((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1\n\n                hM = Q[vidM, _h, eM]\n                bM = Q[vidM, _b, eM]\n                UM = Q[vidM, _U, eM]\n                VM = Q[vidM, _V, eM]\n\n                bc = elemtobndy[f, e]\n                hP = bP = UP = VP = zero(eltype(Q))\n                if bc == 0\n                    hP = Q[vidP, _h, eP]\n                    bP = Q[vidP, _b, eM]\n                    UP = Q[vidP, _U, eP]\n                    VP = Q[vidP, _V, eP]\n                elseif bc == 1\n                    UnM = nxM * UM + nyM * VM\n                    UP = UM - 2 * UnM * nxM\n                    VP = VM - 2 * UnM * nyM\n                    hP = hM\n                    bP = bM\n                else\n                    error(\"Invalid boundary conditions $bc on face $f of element $e\")\n                end\n                HM=hM + bM\n                uM = UM / HM\n                vM = VM / HM\n                HP=hP + bP\n                uP = UP / HP\n                vP = VP / HP\n\n                #Left Fluxes\n                fluxhM_x = UM\n                fluxhM_y = VM\n                fluxUM_x = (HM * uM * uM + 0.5 * gravity * hM^2) * δnl +\n                gravity * hM * bM\n                fluxUM_y = HM * uM * vM * δnl\n                fluxVM_x = HM * vM * uM * δnl\n                fluxVM_y = (HM * vM * vM + 0.5 * gravity * hM^2) * δnl +\n                gravity * hM * bM\n\n                #Right Fluxes\n                fluxhP_x = UP\n                fluxhP_y = VP\n                fluxUP_x = (HP * uP * uP + 0.5 * gravity * hP^2) * δnl +\n                gravity * hP * bP\n                fluxUP_y = HP * uP * vP * δnl\n                fluxVP_x = HP * vP * uP * δnl\n                fluxVP_y = (HP * vP * vP + 0.5 * gravity * hP^2) * δnl +\n                gravity * hP * bP\n\n                #Compute wave speed\n                λM=( abs(nxM * uM + nyM * vM) + sqrt(gravity*HM) ) * δnl +\n                ( sqrt(gravity*bM) ) * (1.0-δnl)\n                λP=( abs(nxM * uP + nyM * vP) + sqrt(gravity*HP) ) * δnl +\n                ( sqrt(gravity*bP) ) * (1.0-δnl)\n                λ = max( λM, λP )\n\n                #Compute Numerical Flux and Update\n                fluxhS = (nxM * (fluxhM_x + fluxhP_x) + nyM * (fluxhM_y + fluxhP_y) +\n                          - λ * (hP - hM)) / 2\n                fluxUS = (nxM * (fluxUM_x + fluxUP_x) + nyM * (fluxUM_y + fluxUP_y) +\n                          - λ * (UP - UM)) / 2\n                fluxVS = (nxM * (fluxVM_x + fluxVP_x) + nyM * (fluxVM_y + fluxVP_y) +\n                          - λ * (VP - VM)) / 2\n\n                #Update RHS\n                rhs[vidM, _h, eM] -= sMJ * fluxhS\n                rhs[vidM, _U, eM] -= sMJ * fluxUS\n                rhs[vidM, _V, eM] -= sMJ * fluxVS\n            end\n        end\n    end\nend}}}{{{ Volume grad(Q)function volume_grad!(::Val{dim}, ::Val{N}, rhs::Array, Q, vgeo, D, elems) where {dim, N}\n    DFloat = eltype(Q)\n    Nq = N + 1\n    nelem = size(Q)[end]\n\n    Q = reshape(Q, Nq, Nq, _nstate, nelem)\n    rhs = reshape(rhs, Nq, Nq, _nstate, dim, nelem)\n    vgeo = reshape(vgeo, Nq, Nq, _nvgeo, nelem)\n\n    #Initialize RHS vector\n    fill!( rhs, zero(rhs[1]))\n\n    #Allocate Arrays\n    s_F = Array{DFloat}(undef, Nq, Nq, _nstate-1, dim)\n    s_G = Array{DFloat}(undef, Nq, Nq, _nstate-1, dim)\n\n    @inbounds for e in elems\n        for j = 1:Nq, i = 1:Nq\n            MJ = vgeo[i, j, _MJ, e]\n            ξx, ξy = vgeo[i,j,_ξx,e], vgeo[i,j,_ξy,e]\n            ηx, ηy = vgeo[i,j,_ηx,e], vgeo[i,j,_ηy,e]\n            U, V = Q[i, j, _U, e], Q[i, j, _V, e]\n            h, b = Q[i, j, _h, e], Q[i, j, _b, e]\n\n            #Compute fluxes\n            fluxh = h\n            fluxU = U\n            fluxV = V\n\n            s_F[i, j, _h, 1] = MJ * (ξx * fluxh)\n            s_F[i, j, _h, 2] = MJ * (ξy * fluxh)\n            s_F[i, j, _U, 1] = MJ * (ξx * fluxU)\n            s_F[i, j, _U, 2] = MJ * (ξy * fluxU)\n            s_F[i, j, _V, 1] = MJ * (ξx * fluxV)\n            s_F[i, j, _V, 2] = MJ * (ξy * fluxV)\n\n            s_G[i, j, _h, 1] = MJ * (ηx * fluxh)\n            s_G[i, j, _h, 2] = MJ * (ηy * fluxh)\n            s_G[i, j, _U, 1] = MJ * (ηx * fluxU)\n            s_G[i, j, _U, 2] = MJ * (ηy * fluxU)\n            s_G[i, j, _V, 1] = MJ * (ηx * fluxV)\n            s_G[i, j, _V, 2] = MJ * (ηy * fluxV)\n        endloop of ξ-grid lines        for s = 1:_nstate-1, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, s, 1, e] -= D[n, i] * s_F[n, j, s, 1]\n            rhs[i, j, s, 2, e] -= D[n, i] * s_F[n, j, s, 2]\n        endloop of η-grid lines        for s = 1:_nstate-1, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, s, 1, e] -= D[n, j] * s_G[i, n, s, 1]\n            rhs[i, j, s, 2, e] -= D[n, j] * s_G[i, n, s, 2]\n        end\n    end\nend}}}{{{ Volume div(grad(Q))function volume_div!(::Val{dim}, ::Val{N}, rhs::Array, Q, vgeo, D, elems) where {dim, N}\n    DFloat = eltype(Q)\n    Nq = N + 1\n    nelem = size(Q)[end]\n\n    Q = reshape(Q, Nq, Nq, _nstate, dim, nelem)\n    rhs = reshape(rhs, Nq, Nq, _nstate, dim, nelem)\n    vgeo = reshape(vgeo, Nq, Nq, _nvgeo, nelem)\n\n    #Initialize RHS vector\n    fill!( rhs, zero(rhs[1]))\n\n    #Allocate Arrays\n    s_F = Array{DFloat}(undef, Nq, Nq, _nstate-1)\n    s_G = Array{DFloat}(undef, Nq, Nq, _nstate-1)\n\n    @inbounds for e in elems\n        for j = 1:Nq, i = 1:Nq\n            MJ = vgeo[i, j, _MJ, e]\n            ξx, ξy = vgeo[i,j,_ξx,e], vgeo[i,j,_ξy,e]\n            ηx, ηy = vgeo[i,j,_ηx,e], vgeo[i,j,_ηy,e]\n            hx, hy = Q[i, j, _h, 1, e], Q[i, j, _h, 2, e]\n            Ux, Uy = Q[i, j, _U, 1, e], Q[i, j, _U, 2, e]\n            Vx, Vy = Q[i, j, _V, 1, e], Q[i, j, _V, 2, e]\n\n            #Compute fluxes\n            fluxh_x = hx\n            fluxh_y = hy\n            fluxU_x = Ux\n            fluxU_y = Uy\n            fluxV_x = Vx\n            fluxV_y = Vy\n\n            s_F[i, j, _h] = MJ * (ξx * fluxh_x + ξy * fluxh_y)\n            s_F[i, j, _U] = MJ * (ξx * fluxU_x + ξy * fluxU_y)\n            s_F[i, j, _V] = MJ * (ξx * fluxV_x + ξy * fluxV_y)\n\n            s_G[i, j, _h] = MJ * (ηx * fluxh_x + ηy * fluxh_y)\n            s_G[i, j, _U] = MJ * (ηx * fluxU_x + ηy * fluxU_y)\n            s_G[i, j, _V] = MJ * (ηx * fluxV_x + ηy * fluxV_y)\n        endloop of ξ-grid lines        for s = 1:_nstate-1, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, s, 1, e] -= D[n, i] * s_F[n, j, s]\n        endloop of η-grid lines        for s = 1:_nstate-1, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, s, 1, e] -= D[n, j] * s_G[i, n, s]\n        end\n    end\nend}}}Flux grad(Q)function flux_grad!(::Val{dim}, ::Val{N}, rhs::Array,  Q, sgeo, elems, vmapM, vmapP, elemtobndy) where {dim, N}\n    Np = (N+1)^dim\n    Nfp = (N+1)^(dim-1)\n    nface = 2*dim\n\n    @inbounds for e in elems\n        for f = 1:nface\n            for n = 1:Nfp\n                nxM, nyM, sMJ = sgeo[_nx, n, f, e], sgeo[_ny, n, f, e], sgeo[_sMJ, n, f, e]\n                idM, idP = vmapM[n, f, e], vmapP[n, f, e]\n\n                eM, eP = e, ((idP - 1) ÷ Np) + 1\n                vidM, vidP = ((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1\n\n                hM = Q[vidM, _h, eM]\n                bM = Q[vidM, _b, eM]\n                UM = Q[vidM, _U, eM]\n                VM = Q[vidM, _V, eM]\n\n                bc = elemtobndy[f, e]\n                hP = bP = UP = VP = zero(eltype(Q))\n                if bc == 0\n                    hP = Q[vidP, _h, eP]\n                    bP = Q[vidP, _b, eP]\n                    UP = Q[vidP, _U, eP]\n                    VP = Q[vidP, _V, eP]\n                elseif bc == 1\n                    UnM = nxM * UM +  nyM * VM\n                    UP = UM - 2 * UnM * nxM\n                    VP = VM - 2 * UnM * nyM\n                    hP = hM\n                    bP = bM\n                else\n                    error(\"Invalid boundary conditions $bc on face $f of element $e\")\n                end\n                HM=hM + bM\n                uM = UM / HM\n                vM = VM / HM\n                HP=hP + bP\n                uP = UP / HP\n                vP = VP / HP\n\n                #Left Fluxes\n                fluxhM = hM\n                fluxUM = UM\n                fluxVM = VM\n\n                #Right Fluxes\n                fluxhP = hP\n                fluxUP = UP\n                fluxVP = VP\n\n                #Compute Numerical/Rusanov Flux\n                fluxhS = 0.5*(fluxhM + fluxhP)\n                fluxUS = 0.5*(fluxUM + fluxUP)\n                fluxVS = 0.5*(fluxVM + fluxVP)\n\n                #Update RHS\n                rhs[vidM, _h, 1, eM] += sMJ * nxM*fluxhS\n                rhs[vidM, _h, 2, eM] += sMJ * nyM*fluxhS\n                rhs[vidM, _U, 1, eM] += sMJ * nxM*fluxUS\n                rhs[vidM, _U, 2, eM] += sMJ * nyM*fluxUS\n                rhs[vidM, _V, 1, eM] += sMJ * nxM*fluxVS\n                rhs[vidM, _V, 2, eM] += sMJ * nyM*fluxVS\n            end\n        end\n    end\nend}}}Flux div( grad(Q) )function flux_div!(::Val{dim}, ::Val{N}, rhs::Array,  Q, sgeo, elems, vmapM, vmapP, elemtobndy) where {dim, N}\n    Np = (N+1)^dim\n    Nfp = (N+1)^(dim-1)\n    nface = 2*dim\n\n    @inbounds for e in elems\n        for f = 1:nface\n            for n = 1:Nfp\n                nxM, nyM, sMJ = sgeo[_nx, n, f, e], sgeo[_ny, n, f, e], sgeo[_sMJ, n, f, e]\n                idM, idP = vmapM[n, f, e], vmapP[n, f, e]\n\n                eM, eP = e, ((idP - 1) ÷ Np) + 1\n                vidM, vidP = ((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1\n\n                hxM = Q[vidM, _h, 1, eM]\n                hyM = Q[vidM, _h, 2, eM]\n                UxM = Q[vidM, _U, 1, eM]\n                UyM = Q[vidM, _U, 2, eM]\n                VxM = Q[vidM, _V, 1, eM]\n                VyM = Q[vidM, _V, 2, eM]\n\n                bc = elemtobndy[f, e]\n                hxP = hyP = UxP = UyP = VxP = VyP = zero(eltype(Q))\n                if bc == 0\n                    hxP = Q[vidP, _h, 1, eP]\n                    hyP = Q[vidP, _h, 2, eP]\n                    UxP = Q[vidP, _U, 1, eP]\n                    UyP = Q[vidP, _U, 2, eP]\n                    VxP = Q[vidP, _V, 1, eP]\n                    VyP = Q[vidP, _V, 2, eP]\n                elseif bc == 1\n                    hnM = nxM * hxM +  nyM * hyM\n                    hxP = hxM - 2 * hnM * nxM\n                    hyP = hyM - 2 * hnM * nyM\n                    UnM = nxM * UxM +  nyM * UyM\n                    UxP = UxM - 2 * UnM * nxM\n                    UyP = UyM - 2 * UnM * nyM\n                    VnM = nxM * VxM +  nyM * VyM\n                    VxP = VxM - 2 * VnM * nxM\n                    VyP = VyM - 2 * VnM * nyM\n                else\n                    error(\"Invalid boundary conditions $bc on face $f of element $e\")\n                end\n\n                #Left Fluxes\n                fluxhM_x = hxM\n                fluxhM_y = hyM\n                fluxUM_x = UxM\n                fluxUM_y = UyM\n                fluxVM_x = VxM\n                fluxVM_y = VyM\n\n                #Right Fluxes\n                fluxhP_x = hxP\n                fluxhP_y = hyP\n                fluxUP_x = UxP\n                fluxUP_y = UyP\n                fluxVP_x = VxP\n                fluxVP_y = VyP\n\n                #Compute Numerical/Rusanov Flux\n                fluxhS = 0.5*(nxM * (fluxhM_x + fluxhP_x) + nyM * (fluxhM_y + fluxhP_y))\n                fluxUS = 0.5*(nxM * (fluxUM_x + fluxUP_x) + nyM * (fluxUM_y + fluxUP_y))\n                fluxVS = 0.5*(nxM * (fluxVM_x + fluxVP_x) + nyM * (fluxVM_y + fluxVP_y))\n\n                #Update RHS\n                rhs[vidM, _h, 1, eM] += sMJ * fluxhS\n                rhs[vidM, _U, 1, eM] += sMJ * fluxUS\n                rhs[vidM, _V, 1, eM] += sMJ * fluxVS\n            end\n        end\n    end\nend}}}{{{ Update grad Q solutionfunction update_gradQ!(::Val{dim}, ::Val{N}, Q, rhs, vgeo, elems) where {dim, N}\n\n    Nq=(N+1)^dim\n\n    @inbounds for e = elems, s = 1:_nstate-1, i = 1:Nq\n        Q[i, s, 1, e] = rhs[i, s, 1, e] * vgeo[i, _MJI, e] / π\n        Q[i, s, 2, e] = rhs[i, s, 2, e] * vgeo[i, _MJI, e] / π\n    end\n\nend}}}{{{ Update grad Q solutionfunction update_divgradQ!(::Val{dim}, ::Val{N}, Q, rhs, vgeo, elems) where {dim, N}\n\n    Nq=(N+1)^dim\n\n    @inbounds for e = elems, s = 1:_nstate-1, i = 1:Nq\n        Q[i, s, e] = rhs[i, s, 1, e] * vgeo[i, _MJI, e] / (2*π)\n    end\n\nend}}}{{{ Update solutionfunction updatesolution!(::Val{dim}, ::Val{N}, rhs, rhs_gradQ, Q, vgeo, elems, rka, rkb, dt, advection, visc) where {dim, N}\n\n    DFloat = eltype(Q)\n    Nq=(N+1)^dim\n    (~, ~, nelem) = size(Q)\n\n    #Store Velocity\n    if (advection)\n\n        #Allocate local arrays\n        u=Array{DFloat,2}(undef,Nq,nelem)\n        v=Array{DFloat,2}(undef,Nq,nelem)\n\n        @inbounds for e = elems, i = 1:Nq\n            u[i,e] = Q[i,_U,e] / ( Q[i,_h,e] + Q[i,_b,e] )\n            v[i,e] = Q[i,_V,e] / ( Q[i,_h,e] + Q[i,_b,e] )\n        end\n    end\n\n    @inbounds for e = elems, s = 1:_nstate, i = 1:Nq\n        rhs[i, s, e] += visc*rhs_gradQ[i,s,1,e]\n        Q[i, s, e] += rkb * dt * rhs[i, s, e] * vgeo[i, _MJI, e]\n        rhs[i, s, e] *= rka\n    end\n\n    #Reset Velocity\n    if (advection)\n        @inbounds for e = elems, i = 1:Nq\n            Q[i,_U,e] = ( Q[i,_h,e] + Q[i,_b,e] ) * u[i,e]\n            Q[i,_V,e] = ( Q[i,_h,e] + Q[i,_b,e] ) * v[i,e]\n        end\n    end\nend}}}{{{ GPU kernels {{{ Volume RHS for 2D@hascuda function knl_volumerhs!(::Val{2}, ::Val{N}, rhs, Q, vgeo, D, nelem, gravity, δnl) where N\n\n    DFloat = eltype(D)\n    Nq = N + 1\n\n    #Point Thread to DOF and Block to element\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    #Allocate Arrays\n    s_D = @cuStaticSharedMem(eltype(D), (Nq, Nq))\n    s_F = @cuStaticSharedMem(eltype(Q), (Nq, Nq, _nstate))\n    s_G = @cuStaticSharedMem(eltype(Q), (Nq, Nq, _nstate))\n\n    rhsU = rhsV = rhsh = zero(eltype(rhs))\n    if i <= Nq && j <= Nq && k == 1 && e <= nelemLoad derivative into shared memory        if k == 1\n            s_D[i, j] = D[i, j]\n        endLoad values needed into registers        MJ = vgeo[i, j, _MJ, e]\n        ξx, ξy = vgeo[i, j, _ξx, e], vgeo[i, j, _ξy, e]\n        ηx, ηy = vgeo[i, j, _ηx, e], vgeo[i, j, _ηy, e]\n        U, V = Q[i, j, _U, e], Q[i, j, _V, e]\n        h, b = Q[i, j, _h, e], Q[i, j, _b, e]\n        rhsh, rhsU, rhsV = rhs[i, j, _h, e], rhs[i, j, _U, e], rhs[i, j, _V, e]\n\n        #Get primitive variables and fluxes\n        H=h + b\n        u=U/H\n        v=V/H\n\n        fluxh_x = U\n        fluxh_y = V\n        fluxU_x = (H * u * u + 0.5 * gravity * h^2) * δnl + gravity * h * b\n        fluxU_y = (H * u * v) * δnl\n        fluxV_x = (H * v * u) * δnl\n        fluxV_y = (H * v * v + 0.5 * gravity * h^2) * δnl + gravity * h * b\n\n        s_F[i, j, _h] = MJ * (ξx * fluxh_x + ξy * fluxh_y)\n        s_F[i, j, _U] = MJ * (ξx * fluxU_x + ξy * fluxU_y)\n        s_F[i, j, _V] = MJ * (ξx * fluxV_x + ξy * fluxV_y)\n\n        s_G[i, j, _h] = MJ * (ηx * fluxh_x + ηy * fluxh_y)\n        s_G[i, j, _U] = MJ * (ηx * fluxU_x + ηy * fluxU_y)\n        s_G[i, j, _V] = MJ * (ηx * fluxV_x + ηy * fluxV_y)\n    end\n\n    sync_threads()\n\n    @inbounds if i <= Nq && j <= Nq && k == 1 && e <= nelem\n        for n = 1:Nq\n\n            #ξ-grid lines\n            Dni = s_D[n, i]\n            rhsh += Dni * s_F[n, j, _h]\n            rhsU += Dni * s_F[n, j, _U]\n            rhsV += Dni * s_F[n, j, _V]\n\n            #η-grid lines\n            Dnj = s_D[n, j]\n            rhsh += Dnj * s_G[i, n, _h]\n            rhsU += Dnj * s_G[i, n, _U]\n            rhsV += Dnj * s_G[i, n, _V]\n        end\n\n        rhs[i, j, _U, e] = rhsU\n        rhs[i, j, _V, e] = rhsV\n        rhs[i, j, _h, e] = rhsh\n    end\n    nothing\nend}}}{{{ Face RHS (all dimensions)@hascuda function knl_fluxrhs!(::Val{dim}, ::Val{N}, rhs, Q, sgeo, nelem, vmapM, vmapP, elemtobndy, gravity, δnl) where {dim, N}\n    DFloat = eltype(Q)\n    Np = (N+1)^dim\n    nface = 2*dim\n\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    Nq = N+1\n    half = convert(eltype(Q), 0.5)\n\n    @inbounds if i <= Nq && j <= Nq && k == 1 && e <= nelem\n        n = i + (j-1) * Nq\n        for lf = 1:2:nface\n            for f = lf:lf+1\n                nxM, nyM, sMJ = sgeo[_nx, n, f, e], sgeo[_ny, n, f, e], sgeo[_sMJ, n, f, e]\n                (idM, idP) = (vmapM[n, f, e], vmapP[n, f, e])\n\n                (eM, eP) = (e, ((idP - 1) ÷ Np) + 1)\n                (vidM, vidP) = (((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1)\n\n                hM = Q[vidM, _h, eM]\n                bM = Q[vidM, _b, eM]\n                UM = Q[vidM, _U, eM]\n                VM = Q[vidM, _V, eM]\n\n                bc = elemtobndy[f, e]\n                hP = bP = UP = VP = zero(eltype(Q))\n                if bc == 0\n                    hP = Q[vidP, _h, eP]\n                    bP = Q[vidP, _b, eM]\n                    UP = Q[vidP, _U, eP]\n                    VP = Q[vidP, _V, eP]\n                elseif bc == 1\n                    UnM = nxM * UM + nyM * VM\n                    UP = UM - 2 * UnM * nxM\n                    VP = VM - 2 * UnM * nyM\n                    hP = hM\n                    bP = bM         else\n             error(\"Invalid boundary conditions $bc on face $f of element $e\")                end\n                HM=hM + bM\n                uM = UM / HM\n                vM = VM / HM\n                HP=hP + bP\n                uP = UP / HP\n                vP = VP / HP\n\n                #Left Fluxes\n                fluxhM_x = UM\n                fluxhM_y = VM\n                fluxUM_x = (HM * uM * uM + 0.5 * gravity * hM * hM) * δnl +\n                gravity * hM * bM\n                fluxUM_y = HM * uM * vM * δnl\n                fluxVM_x = HM * vM * uM * δnl\n                fluxVM_y = (HM * vM * vM + 0.5 * gravity * hM * hM) * δnl +\n                gravity * hM * bM\n\n                #Right Fluxes\n                fluxhP_x = UP\n                fluxhP_y = VP\n                fluxUP_x = (HP * uP * uP + 0.5 * gravity * hP * hP) * δnl +\n                gravity * hP * bP\n                fluxUP_y = HP * uP * vP * δnl\n                fluxVP_x = HP * vP * uP * δnl\n                fluxVP_y = (HP * vP * vP + 0.5 * gravity * hP * hP) * δnl +\n                gravity * hP * bP\n\n                #Compute wave speed\n                λM=( abs(nxM * uM + nyM * vM) + CUDAnative.sqrt(gravity*HM) ) * δnl +\n                ( CUDAnative.sqrt(gravity*bM) ) * (1.0-δnl)\n                λP=( abs(nxM * uP + nyM * vP) + CUDAnative.sqrt(gravity*HP) ) * δnl +\n                ( CUDAnative.sqrt(gravity*bP) ) * (1.0-δnl)\n                λ = max( λM, λP )\n\n                #Compute Numerical Flux and Update\n                fluxhS = (nxM * (fluxhM_x + fluxhP_x) + nyM * (fluxhM_y + fluxhP_y) +\n                          - λ * (hP - hM)) / 2\n                fluxUS = (nxM * (fluxUM_x + fluxUP_x) + nyM * (fluxUM_y + fluxUP_y) +\n                          - λ * (UP - UM)) / 2\n                fluxVS = (nxM * (fluxVM_x + fluxVP_x) + nyM * (fluxVM_y + fluxVP_y) +\n                          - λ * (VP - VM)) / 2\n\n                #Update RHS\n                rhs[vidM, _h, eM] -= sMJ * fluxhS\n                rhs[vidM, _U, eM] -= sMJ * fluxUS\n                rhs[vidM, _V, eM] -= sMJ * fluxVS\n            end\n            sync_threads()\n        end\n    end\n    nothing\nend}}}{{{ Update solution (for all dimensions)@hascuda function knl_updatesolution!(::Val{dim}, ::Val{N}, rhs, Q, vgeo, nelem, rka, rkb, dt, advection) where {dim, N}\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    Nq = N+1\n    @inbounds if i <= Nq && j <= Nq && k <= Nq && e <= nelem\n        n = i + (j-1) * Nq + (k-1) * Nq * Nq\n        MJI = vgeo[n, _MJI, e]\n        for s = 1:_nstate-1\n            Q[n, s, e] += rkb * dt * rhs[n, s, e] * MJI\n            rhs[n, s, e] *= rka\n        end\n    end\n    nothing\nend}}}{{{ Fill sendQ on device with Q (for all dimensions)@hascuda function knl_fillsendQ!(::Val{dim}, ::Val{N}, sendQ, Q, sendelems) where {N, dim}\n    Nq = N + 1\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    @inbounds if i <= Nq && j <= Nq && k <= Nq && e <= length(sendelems)\n        n = i + (j-1) * Nq + (k-1) * Nq * Nq\n        re = sendelems[e]\n        for s = 1:_nstate\n            sendQ[n, s, e] = Q[n, s, re]\n        end\n    end\n    nothing\nend}}}{{{ Fill Q on device with recvQ (for all dimensions)@hascuda function knl_transferrecvQ!(::Val{dim}, ::Val{N}, Q, recvQ, nelem, nrealelem) where {N, dim}\n    Nq = N + 1\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    @inbounds if i <= Nq && j <= Nq && k <= Nq && e <= nelem\n        n = i + (j-1) * Nq + (k-1) * Nq * Nq\n        for s = 1:_nstate\n            Q[n, s, nrealelem + e] = recvQ[n, s, e]\n        end\n    end\n    nothing\nend}}}{{{ MPI Buffer handlingfunction fillsendQ!(::Val{dim}, ::Val{N}, sendQ, d_sendQ::Array, Q,\n                    sendelems) where {dim, N}\n    sendQ[:, :, :] .= Q[:, :, sendelems]\nend\n\n@hascuda function fillsendQ!(::Val{dim}, ::Val{N}, sendQ, d_sendQ::CuArray,\n                             d_QL, d_sendelems) where {dim, N}\n    nsendelem = length(d_sendelems)\n    if nsendelem > 0\n        @cuda(threads=ntuple(j->N+1, dim), blocks=nsendelem,\n              knl_fillsendQ!(Val(dim), Val(N), d_sendQ, d_QL, d_sendelems))\n        sendQ .= d_sendQ\n    end\nend\n\n@hascuda function transferrecvQ!(::Val{dim}, ::Val{N}, d_recvQ::CuArray, recvQ,\n                                 d_QL, nrealelem) where {dim, N}\n    nrecvelem = size(recvQ)[end]\n    if nrecvelem > 0\n        d_recvQ .= recvQ\n        @cuda(threads=ntuple(j->N+1, dim), blocks=nrecvelem,\n              knl_transferrecvQ!(Val(dim), Val(N), d_QL, d_recvQ, nrecvelem,\n                                 nrealelem))\n    end\nend\n\nfunction transferrecvQ!(::Val{dim}, ::Val{N}, d_recvQ::Array, recvQ, Q,\n                        nrealelem) where {dim, N}\n    Q[:, :, nrealelem+1:end] .= recvQ[:, :, :]\nend}}}{{{ GPU kernel wrappers@hascuda function volumerhs!(::Val{dim}, ::Val{N}, d_rhsC::CuArray, d_QC, d_vgeoC, d_D, elems, gravity, δnl) where {dim, N}\n    nelem = length(elems)\n    @cuda(threads=ntuple(j->N+1, dim), blocks=nelem,\n          knl_volumerhs!(Val(dim), Val(N), d_rhsC, d_QC, d_vgeoC, d_D, nelem, gravity, δnl))\nend\n\n@hascuda function fluxrhs!(::Val{dim}, ::Val{N}, d_rhsL::CuArray, d_QL, d_sgeo, elems, d_vmapM, d_vmapP, d_elemtobndy, gravity, δnl) where {dim, N}\n    nelem = length(elems)\n    @cuda(threads=(ntuple(j->N+1, dim-1)..., 1), blocks=nelem,\n          knl_fluxrhs!(Val(dim), Val(N), d_rhsL, d_QL, d_sgeo, nelem, d_vmapM, d_vmapP, d_elemtobndy, gravity, δnl))\nend\n\n@hascuda function updatesolution!(::Val{dim}, ::Val{N}, d_rhsL::CuArray, d_QL, d_vgeoL, elems, rka, rkb, dt, advection) where {dim, N}\n    nelem = length(elems)\n    @cuda(threads=ntuple(j->N+1, dim), blocks=nelem,\n          knl_updatesolution!(Val(dim), Val(N), d_rhsL, d_QL, d_vgeoL, nelem, rka, rkb, dt, advection))\nend}}}{{{ L2 Error (for all dimensions)function L2errorsquared(::Val{dim}, ::Val{N}, Q, vgeo, elems, Qex, t) where {dim, N}\n    DFloat = eltype(Q)\n    Np = (N+1)^dim\n    err = zero(DFloat)\n\n    @inbounds for e = elems, i = 1:Np\n        X = ntuple(j -> vgeo[i, _x-1+j, e] - Q[i, _U-1+j, e]*t, Val(dim))\n        diff = Q[i, _h, e] - Qex[i, _h, e]\n        err += vgeo[i, _MJ, e] * diff^2\n    end\n\n    err\nend}}}{{{ L2 Energy (for all dimensions)function L2energysquared(::Val{dim}, ::Val{N}, Q, vgeo, elems) where {dim, N}\n    DFloat = eltype(Q)\n    Np = (N+1)^dim\n    (~, nstate, nelem) = size(Q)\n\n    energy = zero(DFloat)\n\n    @inbounds for e = elems, q = 1:nstate, i = 1:Np\n        energy += vgeo[i, _MJ, e] * Q[i, q, e]^2\n    end\n\n    energy\nend}}}{{{ Send Data Qfunction senddata_Q(::Val{dim}, ::Val{N}, mesh, sendreq, recvreq, sendQ,\n                  recvQ, d_sendelems, d_sendQ, d_recvQ, d_QL, mpicomm;\n                  ArrType=ArrType) where {dim, N}\n    mpirank = MPI.Comm_rank(mpicomm)Create send and recv request array    nnabr = length(mesh.nabrtorank)\n    d_sendelems = ArrType(mesh.sendelems)\n    nrealelem = length(mesh.realelems)post MPI receives    for n = 1:nnabr\n        recvreq[n] = MPI.Irecv!((@view recvQ[:, :, mesh.nabrtorecv[n]]),\n                                mesh.nabrtorank[n], 777, mpicomm)\n    endwait on (prior) MPI sends    MPI.Waitall!(sendreq)pack data from dQL into send buffer    fillsendQ!(Val(dim), Val(N), sendQ, dsendQ, dQL, dsendelems)    sendQ[:, :, :] .= d_QL[:, :, d_sendelems]post MPI sends    for n = 1:nnabr\n        sendreq[n] = MPI.Isend((@view sendQ[:, :, mesh.nabrtosend[n]]),\n                               mesh.nabrtorank[n], 777, mpicomm)\n    end\nend}}}{{{ Send Data Grad(Q)function senddata_gradQ(::Val{dim}, ::Val{N}, mesh, sendreq, recvreq, sendQ,\n                  recvQ, d_sendelems, d_sendQ, d_recvQ, d_QL, mpicomm;\n                  ArrType=ArrType) where {dim, N}\n    mpirank = MPI.Comm_rank(mpicomm)Create send and recv request array    nnabr = length(mesh.nabrtorank)\n    d_sendelems = ArrType(mesh.sendelems)\n    nrealelem = length(mesh.realelems)post MPI receives    for n = 1:nnabr\n        recvreq[n] = MPI.Irecv!((@view recvQ[:, :, :, mesh.nabrtorecv[n]]),\n                                mesh.nabrtorank[n], 777, mpicomm)\n    endwait on (prior) MPI sends    MPI.Waitall!(sendreq)pack data from dQL into send buffer    fillsendQ!(Val(dim), Val(N), sendQ, dsendQ, dQL, dsendelems)    sendQ[:, :, :, :] .= d_QL[:, :, :, d_sendelems]post MPI sends    for n = 1:nnabr\n        sendreq[n] = MPI.Isend((@view sendQ[:, :, :, mesh.nabrtosend[n]]),\n                               mesh.nabrtorank[n], 777, mpicomm)\n    end\nend}}}{{{ Receive Data Qfunction receivedata_Q!(::Val{dim}, ::Val{N}, mesh, recvreq, recvQ,\n                        d_recvQ, d_QL) where {dim, N}\n    nrealelem = length(mesh.realelems)wait on MPI receives    MPI.Waitall!(recvreq)copy data to state vector d_QL    d_QL[:, :, nrealelem+1:end] .= recvQ[:, :, :]\n\nend}}}{{{ Receive Data Grad(Q)function receivedata_gradQ!(::Val{dim}, ::Val{N}, mesh, recvreq, recvQ,\n                            d_recvQ, d_QL) where {dim, N}\n    nrealelem = length(mesh.realelems)wait on MPI receives    MPI.Waitall!(recvreq)copy data to state vector d_QL    d_QL[:, :, :, nrealelem+1:end] .= recvQ[:, :, :, :]\nend}}}{{{ RK loopfunction lowstorageRK(::Val{dim}, ::Val{N}, mesh, vgeo, sgeo, Q, rhs, D,\n                      dt, nsteps, tout, vmapM, vmapP, mpicomm,\n                      gravity, δnl, advection, visc;\n                      ArrType=ArrType, plotstep=0) where {dim, N}\n    DFloat = eltype(Q)\n    mpirank = MPI.Comm_rank(mpicomm)Fourth-order, low-storage, Runge–Kutta scheme of Carpenter and Kennedy (1994) ((5,4) 2N-Storage RK scheme.Ref: @TECHREPORT{CarpenterKennedy1994,   author = {M.~H. Carpenter and C.~A. Kennedy},   title = {Fourth-order {2N-storage} {Runge-Kutta} schemes},   institution = {National Aeronautics and Space Administration},   year = {1994},   number = {NASA TM-109112},   address = {Langley Research Center, Hampton, VA}, }    RKA = (DFloat(0),\n           DFloat(-567301805773)  / DFloat(1357537059087),\n           DFloat(-2404267990393) / DFloat(2016746695238),\n           DFloat(-3550918686646) / DFloat(2091501179385),\n           DFloat(-1275806237668) / DFloat(842570457699 ))\n\n    RKB = (DFloat(1432997174477) / DFloat(9575080441755 ),\n           DFloat(5161836677717) / DFloat(13612068292357),\n           DFloat(1720146321549) / DFloat(2090206949498 ),\n           DFloat(3134564353537) / DFloat(4481467310338 ),\n           DFloat(2277821191437) / DFloat(14882151754819))\n\n    RKC = (DFloat(0),\n           DFloat(1432997174477) / DFloat(9575080441755),\n           DFloat(2526269341429) / DFloat(6820363962896),\n           DFloat(2006345519317) / DFloat(3224310063776),\n           DFloat(2802321613138) / DFloat(2924317926251))Create send and recv request array    nnabr = length(mesh.nabrtorank)\n    sendreq = fill(MPI.REQUEST_NULL, nnabr)\n    recvreq = fill(MPI.REQUEST_NULL, nnabr)Create send and recv buffer    sendQ = zeros(DFloat, (N+1)^dim, size(Q,2), length(mesh.sendelems))\n    recvQ = zeros(DFloat, (N+1)^dim, size(Q,2), length(mesh.ghostelems))\n    sendgradQ = zeros(DFloat, (N+1)^dim, size(Q,2), dim, length(mesh.sendelems))\n    recvgradQ = zeros(DFloat, (N+1)^dim, size(Q,2), dim, length(mesh.ghostelems))Store Constants    nrealelem = length(mesh.realelems)\n    nsendelem = length(mesh.sendelems)\n    nrecvelem = length(mesh.ghostelems)\n    nelem = length(mesh.elems)\n\n    #Create Device Arrays\n    d_QL, d_rhsL = ArrType(Q), ArrType(rhs)\n    d_vgeoL, d_sgeo = ArrType(vgeo), ArrType(sgeo)\n    d_vmapM, d_vmapP = ArrType(vmapM), ArrType(vmapP)\n    d_sendelems, d_elemtobndy = ArrType(mesh.sendelems), ArrType(mesh.elemtobndy)\n    d_sendQ, d_recvQ = ArrType(sendQ), ArrType(recvQ)\n    d_D = ArrType(D)\n    #Create Device LDG Arrays\n    d_gradQL = zeros(DFloat, (N+1)^dim, _nstate, dim, nelem)\n    d_rhs_gradQL = zeros(DFloat, (N+1)^dim, _nstate, dim, nelem)\n    d_sendgradQ, d_recvgradQ = ArrType(sendgradQ), ArrType(recvgradQ)\n\n    #Template Reshape Arrays\n    Qshape    = (fill(N+1, dim)..., size(Q, 2), size(Q, 3))\n    vgeoshape = (fill(N+1, dim)..., _nvgeo, size(Q, 3))\n    gradQshape = (fill(N+1, dim)..., size(d_gradQL,2), size(d_gradQL,3), size(d_gradQL,4))\n\n    #Reshape Device Arrays\n    d_QC = reshape(d_QL, Qshape)\n    d_rhsC = reshape(d_rhsL, Qshape...)\n    d_vgeoC = reshape(d_vgeoL, vgeoshape)\n    d_gradQC = reshape(d_gradQL, gradQshape)\n    d_rhs_gradQC = reshape(d_rhs_gradQL, gradQshape...)Send Data Q    senddata_Q(Val(dim), Val(N), mesh, sendreq, recvreq, sendQ,\n             recvQ, d_sendelems, d_sendQ, d_recvQ, d_QL, mpicomm;\n             ArrType=ArrType)Receive Data Q    receivedata_Q!(Val(dim), Val(N), mesh, recvreq, recvQ, d_recvQ, d_QL)volume grad Q computation    volume_grad!(Val(dim), Val(N), d_rhs_gradQC, d_QC, d_vgeoC, d_D, mesh.realelems)face grad Q computation    flux_grad!(Val(dim), Val(N), d_rhs_gradQL, d_QL, d_sgeo, mesh.realelems, d_vmapM, d_vmapP, d_elemtobndy)Construct grad Q    update_gradQ!(Val(dim), Val(N), d_gradQL, d_rhs_gradQL, d_vgeoL, mesh.realelems)Send Data grad(Q)    senddata_gradQ(Val(dim), Val(N), mesh, sendreq, recvreq, sendgradQ,\n                   recvgradQ, d_sendelems, d_sendgradQ, d_recvgradQ,\n                   d_gradQL, mpicomm;ArrType=ArrType)volume div (grad Q) computation    volume_div!(Val(dim), Val(N), d_rhs_gradQC, d_gradQC, d_vgeoC, d_D, mesh.realelems)Receive Data grad(Q)    receivedata_gradQ!(Val(dim), Val(N), mesh, recvreq, recvgradQ, d_recvgradQ, d_gradQL)face div (grad Q) computation    flux_div!(Val(dim), Val(N), d_rhs_gradQL, d_gradQL, d_sgeo, mesh.realelems, d_vmapM, d_vmapP, d_elemtobndy)Construct grad Q    update_divgradQ!(Val(dim), Val(N), d_QL, d_rhs_gradQL, d_vgeoL, mesh.realelems)\n\n    Q[:,:,:]=d_gradQL[:,:,1,:] #1st derivativesQ .= d_QL #2nd derivativesend}}}{{{ LDG driverfunction LDG(::Val{dim}, ::Val{N}, mpicomm, ic, mesh, tend, gravity, δnl,\n             advection, visc; meshwarp=(x...)->identity(x), tout = 60, ArrType=Array,\n             plotstep=0) where {dim, N}\n    DFloat = typeof(tend)\n\n    mpirank = MPI.Comm_rank(mpicomm)\n    mpisize = MPI.Comm_size(mpicomm)Partion the mesh using a Hilbert curve based partitioning    mpirank == 0 && println(\"[CPU] partitioning mesh...\")\n    mesh = partition(mpicomm, mesh...)Connect the mesh in parallel    mpirank == 0 && println(\"[CPU] connecting mesh...\")\n    mesh = connectmesh(mpicomm, mesh...)Get the vmaps    mpirank == 0 && println(\"[CPU] computing mappings...\")\n    (vmapM, vmapP) = mappings(N, mesh.elemtoelem, mesh.elemtoface,\n                              mesh.elemtoordr)Create 1-D operators    (ξ, ω) = lglpoints(DFloat, N)\n    D = spectralderivative(ξ)Compute the geometry    mpirank == 0 && println(\"[CPU] computing metrics...\")\n    (vgeo, sgeo) = computegeometry(Val(dim), mesh, D, ξ, ω, meshwarp, vmapM)\n    (nface, nelem) = size(mesh.elemtoelem)Storage for the solution, rhs, and error    mpirank == 0 && println(\"[CPU] creating fields (CPU)...\")\n    Q = zeros(DFloat, (N+1)^dim, _nstate, nelem)\n    Qexact = zeros(DFloat, (N+1)^dim, _nstate, nelem)\n    rhs = zeros(DFloat, (N+1)^dim, _nstate, nelem)setup the initial condition    mpirank == 0 && println(\"[CPU] computing initial conditions (CPU)...\")\n    @inbounds for e = 1:nelem, i = 1:(N+1)^dim\n        x, y = vgeo[i, _x, e], vgeo[i, _y, e]\n        h, hexact = ic(x, y)\n        Q[i, _h, e] = h\n        Q[i, _b, e] = h\n        Q[i, _U, e] = h\n        Q[i, _V, e] = h\n        Qexact[i, _h, e] = hexact\n        Qexact[i, _b, e] = hexact\n        Qexact[i, _U, e] = hexact\n        Qexact[i, _V, e] = hexact\n    endCompute time step    mpirank == 0 && println(\"[CPU] computing dt (CPU)...\")\n    #(base_dt, Courant) = courantnumber(Val(dim), Val(N), vgeo, Q, mpicomm, gravity, δnl, advection)\n    base_dt=0.01\n    Courant=1.0\n    mpirank == 0 && @show (base_dt,Courant)\n\n    nsteps = ceil(Int64, tend / base_dt)\n    dt = tend / nsteps\n    mpirank == 0 && @show (dt, nsteps, dt * nsteps, tend)Do time stepping    stats = zeros(DFloat, 3)\n    mpirank == 0 && println(\"[CPU] computing initial energy...\")\n    stats[1] = L2energysquared(Val(dim), Val(N), Q, vgeo, mesh.realelems)plot initial condition    mkpath(\"viz\")\n    X = ntuple(j->reshape((@view vgeo[:, _x+j-1, :]), ntuple(j->N+1,dim)...,\n                          nelem), dim)\n    h = reshape((@view Q[:, _h, :]), ntuple(j->(N+1),dim)..., nelem)\n    b = reshape((@view Q[:, _b, :]), ntuple(j->(N+1),dim)..., nelem)\n    U = reshape((@view Q[:, _U, :]), ntuple(j->(N+1),dim)..., nelem)\n    V = reshape((@view Q[:, _V, :]), ntuple(j->(N+1),dim)..., nelem)\n    writemesh(@sprintf(\"viz/LDG%dD_%s_rank_%04d_step_%05d\",\n                       dim, ArrType, mpirank, 0), X...;\n              fields=((\"h\", h),(\"b\",b),(\"U\",U),(\"V\",V),),\n              realelems=mesh.realelems)\n\n    #Call Time-stepping Routine\n    mpirank == 0 && println(\"[DEV] starting time stepper...\")\n    lowstorageRK(Val(dim), Val(N), mesh, vgeo, sgeo, Q, rhs, D, dt, nsteps, tout,\n                 vmapM, vmapP, mpicomm, gravity, δnl, advection, visc;\n                 ArrType=ArrType, plotstep=plotstep)plot final solution    X = ntuple(j->reshape((@view vgeo[:, _x+j-1, :]), ntuple(j->N+1,dim)...,\n                          nelem), dim)\n    h = reshape((@view Q[:, _h, :]), ntuple(j->(N+1),dim)..., nelem)\n    b = reshape((@view Q[:, _b, :]), ntuple(j->(N+1),dim)..., nelem)\n    U = reshape((@view Q[:, _U, :]), ntuple(j->(N+1),dim)..., nelem)\n    V = reshape((@view Q[:, _V, :]), ntuple(j->(N+1),dim)..., nelem)\n    writemesh(@sprintf(\"viz/LDG%dD_%s_rank_%04d_step_%05d\",\n                       dim, ArrType, mpirank, nsteps), X...;\n              fields=((\"h\", h),(\"b\",b),(\"U\",U),(\"V\",V),),\n              realelems=mesh.realelems)\n\n    mpirank == 0 && println(\"[CPU] computing final energy...\")\n    stats[2] = L2energysquared(Val(dim), Val(N), Q, vgeo, mesh.realelems)\n    stats[3] = L2errorsquared(Val(dim), Val(N), Q, vgeo, mesh.realelems, Qexact,\n                              tend)\n\n    stats = sqrt.(MPI.allreduce(stats, MPI.SUM, mpicomm))\n\n    if  mpirank == 0\n        @show eng0 = stats[1]\n        @show engf = stats[2]\n        @show Δeng = engf - eng0\n        @show err = stats[3]\n    end\nend}}}{{{ mainfunction main()\n    DFloat = Float64\n\n    MPI.Initialized() ||MPI.Init()\n    MPI.finalize_atexit()\n\n    mpicomm = MPI.COMM_WORLD\n    mpirank = MPI.Comm_rank(mpicomm)\n    mpisize = MPI.Comm_size(mpicomm)FIXME: query via hostname    @hascuda device!(mpirank % length(devices()))\n\n    #Input Parameters\n    N=8\n    Ne=10\n    visc=0.01\n    iplot=10\n    δnl=0\n    icase=10\n    time_final=DFloat(0.32)\n    hardware=\"cpu\"\n    @show (N,Ne,iplot,δnl,icase,time_final,hardware)\n\n    #Initial Conditions\n    ic = (x...) -> (0.0, 0.0, 0.0, 0.0)\n    if icase == 1 #advection\n        function ic1(x...)\n            r = sqrt( (x[1]-0.5)^2 + (x[2]-0.5)^2 )\n            h = 0.5 * exp(-100.0 * r^2)\n            b = 1.0\n            H = h + b\n            U = H*(1.0)\n            V = H*(0.0)\n            h, b, U, V\n        end\n        ic = ic1\n        periodic = (true, true)\n        advection = true\n        gravity = 0\n    elseif icase == 10 #shallow water with Periodic BCs\n        function ic10(x...)\n            r = sqrt( (x[1]-0.5)^2 + (x[2]-0.5)^2 )\n            h = 0.5 * exp(-100.0 * r^2)\n            h = sin( π*x[1] )*sin( π*x[2] )\n            h_x = cos( π*x[1] )*sin( π*x[2] )\n            h_y = sin( π*x[1] )*cos( π*x[2] )\n            h_xx = -sin( π*x[1] )*sin( π*x[2] )\n            h_yy = -sin( π*x[1] )*sin( π*x[2] )\n            hexact=0.5*( h_xx + h_yy )\n            hexact=h_x\n            h, hexact\n        end\n        ic = ic10\n        periodic = (true, true)\n        advection = false\n        gravity = 10\n    elseif icase == 100 #shallow water with NFBC\n        function ic100(x...)\n            r = sqrt( (x[1]-0.5)^2 + (x[2]-0.5)^2 )\n            h = 0.5 * exp(-100.0 * r^2)\n            b=1.0\n            H = h + b\n            U = H*(0.0)\n            V = H*(0.0)\n            h, b, U, V\n        end\n        ic = ic100\n        periodic = (false, false)\n        advection = false\n        gravity = 10\n    end\n\n    mesh = brickmesh((range(DFloat(0); length=Ne+1, stop=2),\n                      range(DFloat(0); length=Ne+1, stop=2)),\n                     periodic; part=mpirank+1, numparts=mpisize)\n\n    if hardware == \"cpu\"\n        mpirank == 0 && println(\"Running (CPU)...\")\n        LDG(Val(2), Val(N), mpicomm, ic, mesh, time_final, gravity, δnl, advection, visc;\n            ArrType=Array, tout = 10, plotstep = iplot)\n        mpirank == 0 && println()\n    elseif hardware == \"gpu\"\n        @hascuda begin\n            mpirank == 0 && println(\"Running (GPU)...\")\n            LDG(Val(2), Val(N), mpicomm, ic, mesh, time_final, gravity, δnl, advection, visc;\n                ArrType=CuArray, tout = 10, plotstep = iplot)\n            mpirank == 0 && println()\n        end\n    end\n    nothing\nend}}}main()This page was generated using Literate.jl."
},

{
    "location": "examples/generated/2d_kernels/swe2d/#",
    "page": "2D Shallow Water Equations",
    "title": "2D Shallow Water Equations",
    "category": "page",
    "text": "EditURL = \"https://github.com/climate-machine/Canary.jl/blob/master/examples/2d_kernels/swe2d.jl\""
},

{
    "location": "examples/generated/2d_kernels/swe2d/#D-Shallow-Water-Equations-1",
    "page": "2D Shallow Water Equations",
    "title": "2D Shallow Water Equations",
    "category": "section",
    "text": ""
},

{
    "location": "examples/generated/2d_kernels/swe2d/#Introduction-1",
    "page": "2D Shallow Water Equations",
    "title": "Introduction",
    "category": "section",
    "text": "This example shows how to solve the 2D shallow water equations using vanilla DG."
},

{
    "location": "examples/generated/2d_kernels/swe2d/#Continuous-Governing-Equations-1",
    "page": "2D Shallow Water Equations",
    "title": "Continuous Governing Equations",
    "category": "section",
    "text": "We solve the following equation:fracpartial h_spartial t + nabla cdot mathbfU = nu nabla^2 h_s   (11)fracpartial mathbfUpartial t + nabla cdot left( fracmathbfU otimes mathbfUh + g (h^2 - h^2_b) mathbfI_2 right) + h_s nabla h_b = nu nabla^2 mathbfU   (12)where mathbfu=(u) and mathbfU=h mathbfu, with h=h_s(mathbfxt) + h_b(mathbfx) being the total water column with h_s and h_b being the height of the water surface and depth of the bathymetry (which we assume to be constant for simplicity), respectively, measured from a zero mean sea-level.  In addition, nu is the artificial viscosity parameter. We employ periodic or no-flux boundary conditions."
},

{
    "location": "examples/generated/2d_kernels/swe2d/#Discontinous-Galerkin-Method-1",
    "page": "2D Shallow Water Equations",
    "title": "Discontinous Galerkin Method",
    "category": "section",
    "text": "To solve Eq. (1) we use the discontinuous Galerkin method with basis functions comprised of Lagrange polynomials based on Lobatto points. Multiplying Eq. (1) by a test function psi and integrating within each element Omega_e such that Omega = bigcup_e=1^N_e Omega_e we getint_Omega_e psi fracpartial mathbfq^(e)_Npartial t dOmega_e + int_Omega_e psi nabla cdot mathbff^(e)_N dOmega_e =  int_Omega_e psi Sleft( q^(e)_N right) dOmega_e   (2)where mathbfq^(e)_N=sum_i=1^(N+1)^dim psi_i(mathbfx) mathbfq_i(t) is the finite dimensional expansion with basis functions psi(mathbfx), where mathbfq=left( h mathbfU^T right)^T andmathbff=left( mathbfU fracmathbfU otimes mathbfUh + g (h^2 - h^2_b) mathbfI_2 right)Integrating Eq. (2) by parts yieldsint_Omega_e psi fracpartial mathbfq^(e)_Npartial t dOmega_e + int_Gamma_e psi mathbfn cdot mathbff^(*e)_N dGamma_e - int_Omega_e nabla psi cdot mathbff^(e)_N dOmega_e = int_Omega_e psi Sleft( q^(e)_N right) dOmega_e   (3)where the second term on the left denotes the flux integral term (computed in \"function fluxrhs\") and the third term denotes the volume integral term (computed in \"function volume_rhs\").  The superscript (*e) in the flux integral term denotes the numerical flux. Here we use the Rusanov flux."
},

{
    "location": "examples/generated/2d_kernels/swe2d/#Local-Discontinous-Galerkin-Method-1",
    "page": "2D Shallow Water Equations",
    "title": "Local Discontinous Galerkin Method",
    "category": "section",
    "text": "To approximate the second order terms on the right hand side of Eq. (1) we use the local discontinuous Galerkin (LDG) method, which we described in LDG2d.jl. We will highlight the main steps below for completeness. The operator nabla^2 is approximated by the following two-step process: first we approximate the gradient of q as followsmathbfQ(xy) = nabla q(xy)   (2)where mathbfQ is an auxiliary vector function, followed bynabla cdot mathbfQ (xy) =  nabla^2 q(xy)   (3)which represents the Laplacian of q."
},

{
    "location": "examples/generated/2d_kernels/swe2d/#Commented-Program-1",
    "page": "2D Shallow Water Equations",
    "title": "Commented Program",
    "category": "section",
    "text": "include(joinpath(@__DIR__,\"vtk.jl\"))\nusing MPI\nusing Canary\nusing Printf: @sprintf\nconst HAVE_CUDA = try\n    using CUDAnative\n    using CUDAdrv\n    true\ncatch\n    false\nend\nif HAVE_CUDA\n    macro hascuda(ex)\n        return :($(esc(ex)))\n    end\nelse\n    macro hascuda(ex)\n        return :()\n    end\nend{{{ reshape for CuArray@hascuda function Base.reshape(A::CuArray, dims::NTuple{N, Int}) where {N}\n    @assert prod(dims) == prod(size(A))\n    CuArray{eltype(A), length(dims)}(dims, A.buf)\nend}}}{{{ constants note the order of the fields below is also assumed in the code.const _nstate = 4\nconst _U, _V, _h, _b = 1:_nstate\nconst stateid = (U = _U, V = _V, h = _h, b = _b)\n\nconst _nvgeo = 8\nconst _ξx, _ηx, _ξy, _ηy, _MJ, _MJI, _x, _y, = 1:_nvgeo\n\nconst _nsgeo = 4\nconst _nx, _ny, _sMJ, _vMJI = 1:_nsgeo}}}{{{ courantfunction courantnumber(::Val{dim}, ::Val{N}, vgeo, Q, mpicomm, gravity, δnl, advection) where {dim, N}\n    DFloat = eltype(Q)\n    Np = (N+1)^dim\n    (~, ~, nelem) = size(Q)\n    dt = [floatmax(DFloat)]\n    Courant = - [floatmax(DFloat)]\n    δ_wave=1\n    if advection\n        δ_wave=0\n    end\n\n    @inbounds for e = 1:nelem, n = 1:Np\n        h, b, U, V = Q[n, _h, e],  Q[n, _b, e], Q[n, _U, e], Q[n, _V, e]\n        ξx, ξy, ηx, ηy = vgeo[n, _ξx, e], vgeo[n, _ξy, e],\n        vgeo[n, _ηx, e], vgeo[n, _ηy, e]\n        H = h+b\n        u=U/H\n        v=V/H\n        dx=sqrt( (1.0/(2*ξx))^2 + (1.0/(2*ηy))^2 )\n        vel=sqrt( u^2 + v^2 )\n        wave_speed = (vel + δ_wave*sqrt(gravity*H)*δnl + gravity*b*(1-δnl))\n        loc_dt = 0.5*dx/wave_speed/N\n        dt[1] = min(dt[1], loc_dt)\n    end\n    dt_min=MPI.Allreduce(dt[1], MPI.MIN, mpicomm)\n\n        #Compute Courant\n    @inbounds for e = 1:nelem, n = 1:Np\n        h, b, U, V = Q[n, _h, e],  Q[n, _b, e], Q[n, _U, e], Q[n, _V, e]\n        ξx, ξy, ηx, ηy = vgeo[n, _ξx, e], vgeo[n, _ξy, e],\n        vgeo[n, _ηx, e], vgeo[n, _ηy, e]\n        H = h+b\n        u=U/H\n        v=V/H\n        dx=sqrt( (1.0/(2*ξx))^2 + (1.0/(2*ηy))^2 )\n        vel=sqrt( u^2 + v^2 )\n        wave_speed = (vel + δ_wave*sqrt(gravity*H)*δnl + gravity*b*(1-δnl))\n        loc_Courant = wave_speed*dt_min/dx*N\n        Courant[1] = max(Courant[1], loc_Courant)\n    end\n    Courant_max=MPI.Allreduce(Courant[1], MPI.MAX, mpicomm)\n\n    (dt_min, Courant_max)\nend}}}{{{ compute geometryfunction computegeometry(::Val{dim}, mesh, D, ξ, ω, meshwarp, vmapM) where dimCompute metric terms    Nq = size(D, 1)\n    DFloat = eltype(D)\n\n    (nface, nelem) = size(mesh.elemtoelem)\n\n    crd = creategrid(Val(dim), mesh.elemtocoord, ξ)\n\n    vgeo = zeros(DFloat, Nq^dim, _nvgeo, nelem)\n    sgeo = zeros(DFloat, _nsgeo, Nq^(dim-1), nface, nelem)\n\n    (ξx, ηx, ξy, ηy, MJ, MJI, x, y) =\n        ntuple(j->(@view vgeo[:, j, :]), _nvgeo)\n    J = similar(x)\n    (nx, ny, sMJ, vMJI) = ntuple(j->(@view sgeo[ j, :, :, :]), _nsgeo)\n    sJ = similar(sMJ)\n\n    X = ntuple(j->(@view vgeo[:, _x+j-1, :]), dim)\n    creategrid!(X..., mesh.elemtocoord, ξ)\n\n    @inbounds for j = 1:length(x)\n        (x[j], y[j]) = meshwarp(x[j], y[j])\n    endCompute the metric terms    computemetric!(x, y, J, ξx, ηx, ξy, ηy, sJ, nx, ny, D)\n\n    M = kron(1, ntuple(j->ω, dim)...)\n    MJ .= M .* J\n    MJI .= 1 ./ MJ\n    vMJI .= MJI[vmapM]\n\n    sM = dim > 1 ? kron(1, ntuple(j->ω, dim-1)...) : one(DFloat)\n    sMJ .= sM .* sJ\n\n    (vgeo, sgeo)\nend}}}{{{ CPU Kernels {{{ Volume RHSfunction volumerhs!(::Val{dim}, ::Val{N}, rhs::Array, Q, vgeo, D, elems, gravity, δnl) where {dim, N}\n    DFloat = eltype(Q)\n    Nq = N + 1\n    nelem = size(Q)[end]\n\n    Q = reshape(Q, Nq, Nq, _nstate, nelem)\n    rhs = reshape(rhs, Nq, Nq, _nstate, nelem)\n    vgeo = reshape(vgeo, Nq, Nq, _nvgeo, nelem)\n\n    #Allocate Arrays\n    s_F = Array{DFloat}(undef, Nq, Nq, _nstate-1)\n    s_G = Array{DFloat}(undef, Nq, Nq, _nstate-1)\n\n    @inbounds for e in elems\n        for j = 1:Nq, i = 1:Nq\n            MJ = vgeo[i, j, _MJ, e]\n            ξx, ξy = vgeo[i,j,_ξx,e], vgeo[i,j,_ξy,e]\n            ηx, ηy = vgeo[i,j,_ηx,e], vgeo[i,j,_ηy,e]\n            U, V = Q[i, j, _U, e], Q[i, j, _V, e]\n            h, b = Q[i, j, _h, e], Q[i, j, _b, e]\n\n            #Get primitive variables and fluxes\n            H=h + b\n            u=U/H\n            v=V/H\n\n            #Compute fluxes\n            fluxh_x = U\n            fluxh_y = V\n            fluxU_x = (H * u * u + 0.5 * gravity * h^2) * δnl + gravity * h * b\n            fluxU_y = (H * u * v) * δnl\n            fluxV_x = (H * v * u) * δnl\n            fluxV_y = (H * v * v + 0.5 * gravity * h^2) * δnl + gravity * h * b\n\n            s_F[i, j, _h] = MJ * (ξx * fluxh_x + ξy * fluxh_y)\n            s_F[i, j, _U] = MJ * (ξx * fluxU_x + ξy * fluxU_y)\n            s_F[i, j, _V] = MJ * (ξx * fluxV_x + ξy * fluxV_y)\n\n            s_G[i, j, _h] = MJ * (ηx * fluxh_x + ηy * fluxh_y)\n            s_G[i, j, _U] = MJ * (ηx * fluxU_x + ηy * fluxU_y)\n            s_G[i, j, _V] = MJ * (ηx * fluxV_x + ηy * fluxV_y)\n        endloop of ξ-grid lines        for s = 1:_nstate-1, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, s, e] += D[n, i] * s_F[n, j, s]\n        endloop of η-grid lines        for s = 1:_nstate-1, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, s, e] += D[n, j] * s_G[i, n, s]\n        end\n    end\nend}}}Flux RHSfunction fluxrhs!(::Val{dim}, ::Val{N}, rhs::Array,  Q, sgeo, elems, vmapM, vmapP, elemtobndy, gravity, δnl) where {dim, N}\n    DFloat = eltype(Q)\n    Np = (N+1)^dim\n    Nfp = (N+1)^(dim-1)\n    nface = 2*dim\n\n    @inbounds for e in elems\n        for f = 1:nface\n            for n = 1:Nfp\n                nxM, nyM, sMJ = sgeo[_nx, n, f, e], sgeo[_ny, n, f, e], sgeo[_sMJ, n, f, e]\n                idM, idP = vmapM[n, f, e], vmapP[n, f, e]\n\n                eM, eP = e, ((idP - 1) ÷ Np) + 1\n                vidM, vidP = ((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1\n\n                hM = Q[vidM, _h, eM]\n                bM = Q[vidM, _b, eM]\n                UM = Q[vidM, _U, eM]\n                VM = Q[vidM, _V, eM]\n\n                bc = elemtobndy[f, e]\n                hP = bP = UP = VP = zero(eltype(Q))\n                if bc == 0\n                    hP = Q[vidP, _h, eP]\n                    bP = Q[vidP, _b, eM]\n                    UP = Q[vidP, _U, eP]\n                    VP = Q[vidP, _V, eP]\n                elseif bc == 1\n                    UnM = nxM * UM + nyM * VM\n                    UP = UM - 2 * UnM * nxM\n                    VP = VM - 2 * UnM * nyM\n                    hP = hM\n                    bP = bM\n                else\n                    error(\"Invalid boundary conditions $bc on face $f of element $e\")\n                end\n                HM=hM + bM\n                uM = UM / HM\n                vM = VM / HM\n                HP=hP + bP\n                uP = UP / HP\n                vP = VP / HP\n\n                #Left Fluxes\n                fluxhM_x = UM\n                fluxhM_y = VM\n                fluxUM_x = (HM * uM * uM + 0.5 * gravity * hM^2) * δnl +\n                gravity * hM * bM\n                fluxUM_y = HM * uM * vM * δnl\n                fluxVM_x = HM * vM * uM * δnl\n                fluxVM_y = (HM * vM * vM + 0.5 * gravity * hM^2) * δnl +\n                gravity * hM * bM\n\n                #Right Fluxes\n                fluxhP_x = UP\n                fluxhP_y = VP\n                fluxUP_x = (HP * uP * uP + 0.5 * gravity * hP^2) * δnl +\n                gravity * hP * bP\n                fluxUP_y = HP * uP * vP * δnl\n                fluxVP_x = HP * vP * uP * δnl\n                fluxVP_y = (HP * vP * vP + 0.5 * gravity * hP^2) * δnl +\n                gravity * hP * bP\n\n                #Compute wave speed\n                λM=( abs(nxM * uM + nyM * vM) + sqrt(gravity*HM) ) * δnl +\n                ( sqrt(gravity*bM) ) * (1.0-δnl)\n                λP=( abs(nxM * uP + nyM * vP) + sqrt(gravity*HP) ) * δnl +\n                ( sqrt(gravity*bP) ) * (1.0-δnl)\n                λ = max( λM, λP )\n\n                #Compute Numerical Flux and Update\n                fluxhS = (nxM * (fluxhM_x + fluxhP_x) + nyM * (fluxhM_y + fluxhP_y) +\n                          - λ * (hP - hM)) / 2\n                fluxUS = (nxM * (fluxUM_x + fluxUP_x) + nyM * (fluxUM_y + fluxUP_y) +\n                          - λ * (UP - UM)) / 2\n                fluxVS = (nxM * (fluxVM_x + fluxVP_x) + nyM * (fluxVM_y + fluxVP_y) +\n                          - λ * (VP - VM)) / 2\n\n                #Update RHS\n                rhs[vidM, _h, eM] -= sMJ * fluxhS\n                rhs[vidM, _U, eM] -= sMJ * fluxUS\n                rhs[vidM, _V, eM] -= sMJ * fluxVS\n            end\n        end\n    end\nend}}}{{{ Volume grad(Q)function volume_grad!(::Val{dim}, ::Val{N}, rhs::Array, Q, vgeo, D, elems) where {dim, N}\n    DFloat = eltype(Q)\n    Nq = N + 1\n    nelem = size(Q)[end]\n\n    Q = reshape(Q, Nq, Nq, _nstate, nelem)\n    rhs = reshape(rhs, Nq, Nq, _nstate, dim, nelem)\n    vgeo = reshape(vgeo, Nq, Nq, _nvgeo, nelem)\n\n    #Initialize RHS vector\n    fill!( rhs, zero(rhs[1]))\n\n    #Allocate Arrays\n    s_F = Array{DFloat}(undef, Nq, Nq, _nstate-1, dim)\n    s_G = Array{DFloat}(undef, Nq, Nq, _nstate-1, dim)\n\n    @inbounds for e in elems\n        for j = 1:Nq, i = 1:Nq\n            MJ = vgeo[i, j, _MJ, e]\n            ξx, ξy = vgeo[i,j,_ξx,e], vgeo[i,j,_ξy,e]\n            ηx, ηy = vgeo[i,j,_ηx,e], vgeo[i,j,_ηy,e]\n            U, V = Q[i, j, _U, e], Q[i, j, _V, e]\n            h, b = Q[i, j, _h, e], Q[i, j, _b, e]\n\n            #Compute fluxes\n            fluxh = h\n            fluxU = U\n            fluxV = V\n\n            s_F[i, j, _h, 1] = MJ * (ξx * fluxh)\n            s_F[i, j, _h, 2] = MJ * (ξy * fluxh)\n            s_F[i, j, _U, 1] = MJ * (ξx * fluxU)\n            s_F[i, j, _U, 2] = MJ * (ξy * fluxU)\n            s_F[i, j, _V, 1] = MJ * (ξx * fluxV)\n            s_F[i, j, _V, 2] = MJ * (ξy * fluxV)\n\n            s_G[i, j, _h, 1] = MJ * (ηx * fluxh)\n            s_G[i, j, _h, 2] = MJ * (ηy * fluxh)\n            s_G[i, j, _U, 1] = MJ * (ηx * fluxU)\n            s_G[i, j, _U, 2] = MJ * (ηy * fluxU)\n            s_G[i, j, _V, 1] = MJ * (ηx * fluxV)\n            s_G[i, j, _V, 2] = MJ * (ηy * fluxV)\n        endloop of ξ-grid lines        for s = 1:_nstate-1, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, s, 1, e] -= D[n, i] * s_F[n, j, s, 1]\n            rhs[i, j, s, 2, e] -= D[n, i] * s_F[n, j, s, 2]\n        endloop of η-grid lines        for s = 1:_nstate-1, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, s, 1, e] -= D[n, j] * s_G[i, n, s, 1]\n            rhs[i, j, s, 2, e] -= D[n, j] * s_G[i, n, s, 2]\n        end\n    end\nend}}}Flux grad(Q)function flux_grad!(::Val{dim}, ::Val{N}, rhs::Array,  Q, sgeo, elems, vmapM, vmapP, elemtobndy) where {dim, N}\n    Np = (N+1)^dim\n    Nfp = (N+1)^(dim-1)\n    nface = 2*dim\n\n    @inbounds for e in elems\n        for f = 1:nface\n            for n = 1:Nfp\n                nxM, nyM, sMJ = sgeo[_nx, n, f, e], sgeo[_ny, n, f, e], sgeo[_sMJ, n, f, e]\n                idM, idP = vmapM[n, f, e], vmapP[n, f, e]\n\n                eM, eP = e, ((idP - 1) ÷ Np) + 1\n                vidM, vidP = ((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1\n\n                hM = Q[vidM, _h, eM]\n                bM = Q[vidM, _b, eM]\n                UM = Q[vidM, _U, eM]\n                VM = Q[vidM, _V, eM]\n\n                bc = elemtobndy[f, e]\n                hP = bP = UP = VP = zero(eltype(Q))\n                if bc == 0\n                    hP = Q[vidP, _h, eP]\n                    bP = Q[vidP, _b, eP]\n                    UP = Q[vidP, _U, eP]\n                    VP = Q[vidP, _V, eP]\n                elseif bc == 1\n                    UnM = nxM * UM +  nyM * VM\n                    UP = UM - 2 * UnM * nxM\n                    VP = VM - 2 * UnM * nyM\n                    hP = hM\n                    bP = bM\n                else\n                    error(\"Invalid boundary conditions $bc on face $f of element $e\")\n                end\n                HM=hM + bM\n                uM = UM / HM\n                vM = VM / HM\n                HP=hP + bP\n                uP = UP / HP\n                vP = VP / HP\n\n                #Left Fluxes\n                fluxhM = hM\n                fluxUM = UM\n                fluxVM = VM\n\n                #Right Fluxes\n                fluxhP = hP\n                fluxUP = UP\n                fluxVP = VP\n\n                #Compute Numerical/Rusanov Flux\n                fluxhS = 0.5*(fluxhM + fluxhP)\n                fluxUS = 0.5*(fluxUM + fluxUP)\n                fluxVS = 0.5*(fluxVM + fluxVP)\n\n                #Update RHS\n                rhs[vidM, _h, 1, eM] += sMJ * nxM*fluxhS\n                rhs[vidM, _h, 2, eM] += sMJ * nyM*fluxhS\n                rhs[vidM, _U, 1, eM] += sMJ * nxM*fluxUS\n                rhs[vidM, _U, 2, eM] += sMJ * nyM*fluxUS\n                rhs[vidM, _V, 1, eM] += sMJ * nxM*fluxVS\n                rhs[vidM, _V, 2, eM] += sMJ * nyM*fluxVS\n            end\n        end\n    end\nend}}}{{{ Volume div(grad(Q))function volume_div!(::Val{dim}, ::Val{N}, rhs::Array, Q, vgeo, D, elems) where {dim, N}\n    DFloat = eltype(Q)\n    Nq = N + 1\n    nelem = size(Q)[end]\n\n    Q = reshape(Q, Nq, Nq, _nstate, dim, nelem)\n    rhs = reshape(rhs, Nq, Nq, _nstate, dim, nelem)\n    vgeo = reshape(vgeo, Nq, Nq, _nvgeo, nelem)\n\n    #Initialize RHS vector\n    fill!( rhs, zero(rhs[1]))\n\n    #Allocate Arrays\n    s_F = Array{DFloat}(undef, Nq, Nq, _nstate-1)\n    s_G = Array{DFloat}(undef, Nq, Nq, _nstate-1)\n\n    @inbounds for e in elems\n        for j = 1:Nq, i = 1:Nq\n            MJ = vgeo[i, j, _MJ, e]\n            ξx, ξy = vgeo[i,j,_ξx,e], vgeo[i,j,_ξy,e]\n            ηx, ηy = vgeo[i,j,_ηx,e], vgeo[i,j,_ηy,e]\n            hx, hy = Q[i, j, _h, 1, e], Q[i, j, _h, 2, e]\n            Ux, Uy = Q[i, j, _U, 1, e], Q[i, j, _U, 2, e]\n            Vx, Vy = Q[i, j, _V, 1, e], Q[i, j, _V, 2, e]\n\n            #Compute fluxes\n            fluxh_x = hx\n            fluxh_y = hy\n            fluxU_x = Ux\n            fluxU_y = Uy\n            fluxV_x = Vx\n            fluxV_y = Vy\n\n            s_F[i, j, _h] = MJ * (ξx * fluxh_x + ξy * fluxh_y)\n            s_F[i, j, _U] = MJ * (ξx * fluxU_x + ξy * fluxU_y)\n            s_F[i, j, _V] = MJ * (ξx * fluxV_x + ξy * fluxV_y)\n\n            s_G[i, j, _h] = MJ * (ηx * fluxh_x + ηy * fluxh_y)\n            s_G[i, j, _U] = MJ * (ηx * fluxU_x + ηy * fluxU_y)\n            s_G[i, j, _V] = MJ * (ηx * fluxV_x + ηy * fluxV_y)\n        endloop of ξ-grid lines        for s = 1:_nstate-1, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, s, 1, e] -= D[n, i] * s_F[n, j, s]\n        endloop of η-grid lines        for s = 1:_nstate-1, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, s, 1, e] -= D[n, j] * s_G[i, n, s]\n        end\n    end\nend}}}Flux div(grad(Q))function flux_div!(::Val{dim}, ::Val{N}, rhs::Array,  Q, sgeo, elems, vmapM, vmapP, elemtobndy) where {dim, N}\n    Np = (N+1)^dim\n    Nfp = (N+1)^(dim-1)\n    nface = 2*dim\n\n    @inbounds for e in elems\n        for f = 1:nface\n            for n = 1:Nfp\n                nxM, nyM, sMJ = sgeo[_nx, n, f, e], sgeo[_ny, n, f, e], sgeo[_sMJ, n, f, e]\n                idM, idP = vmapM[n, f, e], vmapP[n, f, e]\n\n                eM, eP = e, ((idP - 1) ÷ Np) + 1\n                vidM, vidP = ((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1\n\n                hxM = Q[vidM, _h, 1, eM]\n                hyM = Q[vidM, _h, 2, eM]\n                UxM = Q[vidM, _U, 1, eM]\n                UyM = Q[vidM, _U, 2, eM]\n                VxM = Q[vidM, _V, 1, eM]\n                VyM = Q[vidM, _V, 2, eM]\n\n                bc = elemtobndy[f, e]\n                hxP = hyP = UxP = UyP = VxP = VyP = zero(eltype(Q))\n                if bc == 0\n                    hxP = Q[vidP, _h, 1, eP]\n                    hyP = Q[vidP, _h, 2, eP]\n                    UxP = Q[vidP, _U, 1, eP]\n                    UyP = Q[vidP, _U, 2, eP]\n                    VxP = Q[vidP, _V, 1, eP]\n                    VyP = Q[vidP, _V, 2, eP]\n                elseif bc == 1\n                    hnM = nxM * hxM +  nyM * hyM\n                    hxP = hxM - 2 * hnM * nxM\n                    hyP = hyM - 2 * hnM * nyM\n                    UnM = nxM * UxM +  nyM * UyM\n                    UxP = UxM - 2 * UnM * nxM\n                    UyP = UyM - 2 * UnM * nyM\n                    VnM = nxM * VxM +  nyM * VyM\n                    VxP = VxM - 2 * VnM * nxM\n                    VyP = VyM - 2 * VnM * nyM\n                else\n                    error(\"Invalid boundary conditions $bc on face $f of element $e\")\n                end\n\n                #Left Fluxes\n                fluxhM_x = hxM\n                fluxhM_y = hyM\n                fluxUM_x = UxM\n                fluxUM_y = UyM\n                fluxVM_x = VxM\n                fluxVM_y = VyM\n\n                #Right Fluxes\n                fluxhP_x = hxP\n                fluxhP_y = hyP\n                fluxUP_x = UxP\n                fluxUP_y = UyP\n                fluxVP_x = VxP\n                fluxVP_y = VyP\n\n                #Compute Numerical/Rusanov Flux\n                fluxhS = 0.5*(nxM * (fluxhM_x + fluxhP_x) + nyM * (fluxhM_y + fluxhP_y))\n                fluxUS = 0.5*(nxM * (fluxUM_x + fluxUP_x) + nyM * (fluxUM_y + fluxUP_y))\n                fluxVS = 0.5*(nxM * (fluxVM_x + fluxVP_x) + nyM * (fluxVM_y + fluxVP_y))\n\n                #Update RHS\n                rhs[vidM, _h, 1, eM] += sMJ * fluxhS\n                rhs[vidM, _U, 1, eM] += sMJ * fluxUS\n                rhs[vidM, _V, 1, eM] += sMJ * fluxVS\n            end\n        end\n    end\nend}}}{{{ Update grad Q solutionfunction update_gradQ!(::Val{dim}, ::Val{N}, Q, rhs, vgeo, elems) where {dim, N}\n\n    Nq=(N+1)^dim\n\n    @inbounds for e = elems, s = 1:_nstate-1, i = 1:Nq\n        Q[i, s, 1, e] = rhs[i, s, 1, e] * vgeo[i, _MJI, e]\n        Q[i, s, 2, e] = rhs[i, s, 2, e] * vgeo[i, _MJI, e]\n    end\n\nend}}}{{{ Update grad Q solutionfunction update_divgradQ!(::Val{dim}, ::Val{N}, Q, rhs, vgeo, elems) where {dim, N}\n\n    Nq=(N+1)^dim\n\n    @inbounds for e = elems, s = 1:_nstate-1, i = 1:Nq\n        Q[i, s, e] = rhs[i, s, 1, e] * vgeo[i, _MJI, e]\n    end\n\nend}}}{{{ Update solutionfunction updatesolution!(::Val{dim}, ::Val{N}, rhs, rhs_gradQ, Q, vgeo, elems, rka, rkb, dt, advection, visc) where {dim, N}\n\n    DFloat = eltype(Q)\n    Nq=(N+1)^dim\n    (~, ~, nelem) = size(Q)\n\n    #Store Velocity\n    if (advection)\n\n        #Allocate local arrays\n        u=Array{DFloat,2}(undef,Nq,nelem)\n        v=Array{DFloat,2}(undef,Nq,nelem)\n\n        @inbounds for e = elems, i = 1:Nq\n            u[i,e] = Q[i,_U,e] / ( Q[i,_h,e] + Q[i,_b,e] )\n            v[i,e] = Q[i,_V,e] / ( Q[i,_h,e] + Q[i,_b,e] )\n        end\n    end\n\n    @inbounds for e = elems, s = 1:_nstate, i = 1:Nq\n        rhs[i, s, e] += visc*rhs_gradQ[i,s,1,e]\n        Q[i, s, e] += rkb * dt * rhs[i, s, e] * vgeo[i, _MJI, e]\n        rhs[i, s, e] *= rka\n    end\n\n    #Reset Velocity\n    if (advection)\n        @inbounds for e = elems, i = 1:Nq\n            Q[i,_U,e] = ( Q[i,_h,e] + Q[i,_b,e] ) * u[i,e]\n            Q[i,_V,e] = ( Q[i,_h,e] + Q[i,_b,e] ) * v[i,e]\n        end\n    end\nend}}}{{{ GPU kernels {{{ Volume RHS for 2D@hascuda function knl_volumerhs!(::Val{2}, ::Val{N}, rhs, Q, vgeo, D, nelem, gravity, δnl) where N\n    DFloat = eltype(D)\n    Nq = N + 1\n\n    #Point Thread to DOF and Block to element\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    #Allocate Arrays\n    s_D = @cuStaticSharedMem(eltype(D), (Nq, Nq))\n    s_F = @cuStaticSharedMem(eltype(Q), (Nq, Nq, _nstate))\n    s_G = @cuStaticSharedMem(eltype(Q), (Nq, Nq, _nstate))\n\n    rhsU = rhsV = rhsh = zero(eltype(rhs))\n    if i <= Nq && j <= Nq && k == 1 && e <= nelemLoad derivative into shared memory        if k == 1\n            s_D[i, j] = D[i, j]\n        endLoad values needed into registers        MJ = vgeo[i, j, _MJ, e]\n        ξx, ξy = vgeo[i, j, _ξx, e], vgeo[i, j, _ξy, e]\n        ηx, ηy = vgeo[i, j, _ηx, e], vgeo[i, j, _ηy, e]\n        U, V = Q[i, j, _U, e], Q[i, j, _V, e]\n        h, b = Q[i, j, _h, e], Q[i, j, _b, e]\n        rhsh, rhsU, rhsV = rhs[i, j, _h, e], rhs[i, j, _U, e], rhs[i, j, _V, e]\n\n        #Get primitive variables and fluxes\n        H=h + b\n        u=U/H\n        v=V/H\n\n        fluxh_x = U\n        fluxh_y = V\n        fluxU_x = (H * u * u + 0.5 * gravity * h^2) * δnl + gravity * h * b\n        fluxU_y = (H * u * v) * δnl\n        fluxV_x = (H * v * u) * δnl\n        fluxV_y = (H * v * v + 0.5 * gravity * h^2) * δnl + gravity * h * b\n\n        s_F[i, j, _h] = MJ * (ξx * fluxh_x + ξy * fluxh_y)\n        s_F[i, j, _U] = MJ * (ξx * fluxU_x + ξy * fluxU_y)\n        s_F[i, j, _V] = MJ * (ξx * fluxV_x + ξy * fluxV_y)\n\n        s_G[i, j, _h] = MJ * (ηx * fluxh_x + ηy * fluxh_y)\n        s_G[i, j, _U] = MJ * (ηx * fluxU_x + ηy * fluxU_y)\n        s_G[i, j, _V] = MJ * (ηx * fluxV_x + ηy * fluxV_y)\n    end\n\n    sync_threads()\n\n    @inbounds if i <= Nq && j <= Nq && k == 1 && e <= nelem\n        for n = 1:Nq\n\n            #ξ-grid lines\n            Dni = s_D[n, i]\n            rhsh += Dni * s_F[n, j, _h]\n            rhsU += Dni * s_F[n, j, _U]\n            rhsV += Dni * s_F[n, j, _V]\n\n            #η-grid lines\n            Dnj = s_D[n, j]\n            rhsh += Dnj * s_G[i, n, _h]\n            rhsU += Dnj * s_G[i, n, _U]\n            rhsV += Dnj * s_G[i, n, _V]\n        end\n\n        rhs[i, j, _U, e] = rhsU\n        rhs[i, j, _V, e] = rhsV\n        rhs[i, j, _h, e] = rhsh\n    end\n    nothing\nend}}}{{{ Face RHS (all dimensions)@hascuda function knl_fluxrhs!(::Val{dim}, ::Val{N}, rhs, Q, sgeo, nelem, vmapM, vmapP, elemtobndy, gravity, δnl) where {dim, N}\n    DFloat = eltype(Q)\n    Np = (N+1)^dim\n    nface = 2*dim\n\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    Nq = N+1\n    half = convert(eltype(Q), 0.5)\n\n    @inbounds if i <= Nq && j <= Nq && k == 1 && e <= nelem\n        n = i + (j-1) * Nq\n        for lf = 1:2:nface\n            for f = lf:lf+1\n                nxM, nyM, sMJ = sgeo[_nx, n, f, e], sgeo[_ny, n, f, e], sgeo[_sMJ, n, f, e]\n                (idM, idP) = (vmapM[n, f, e], vmapP[n, f, e])\n\n                (eM, eP) = (e, ((idP - 1) ÷ Np) + 1)\n                (vidM, vidP) = (((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1)\n\n                hM = Q[vidM, _h, eM]\n                bM = Q[vidM, _b, eM]\n                UM = Q[vidM, _U, eM]\n                VM = Q[vidM, _V, eM]\n\n                bc = elemtobndy[f, e]\n                hP = bP = UP = VP = zero(eltype(Q))\n                if bc == 0\n                    hP = Q[vidP, _h, eP]\n                    bP = Q[vidP, _b, eM]\n                    UP = Q[vidP, _U, eP]\n                    VP = Q[vidP, _V, eP]\n                elseif bc == 1\n                    UnM = nxM * UM + nyM * VM\n                    UP = UM - 2 * UnM * nxM\n                    VP = VM - 2 * UnM * nyM\n                    hP = hM\n                    bP = bM         else\n             error(\"Invalid boundary conditions $bc on face $f of element $e\")                end\n                HM=hM + bM\n                uM = UM / HM\n                vM = VM / HM\n                HP=hP + bP\n                uP = UP / HP\n                vP = VP / HP\n\n                #Left Fluxes\n                fluxhM_x = UM\n                fluxhM_y = VM\n                fluxUM_x = (HM * uM * uM + 0.5 * gravity * hM * hM) * δnl +\n                gravity * hM * bM\n                fluxUM_y = HM * uM * vM * δnl\n                fluxVM_x = HM * vM * uM * δnl\n                fluxVM_y = (HM * vM * vM + 0.5 * gravity * hM * hM) * δnl +\n                gravity * hM * bM\n\n                #Right Fluxes\n                fluxhP_x = UP\n                fluxhP_y = VP\n                fluxUP_x = (HP * uP * uP + 0.5 * gravity * hP * hP) * δnl +\n                gravity * hP * bP\n                fluxUP_y = HP * uP * vP * δnl\n                fluxVP_x = HP * vP * uP * δnl\n                fluxVP_y = (HP * vP * vP + 0.5 * gravity * hP * hP) * δnl +\n                gravity * hP * bP\n\n                #Compute wave speed\n                λM=( abs(nxM * uM + nyM * vM) + CUDAnative.sqrt(gravity*HM) ) * δnl +\n                ( CUDAnative.sqrt(gravity*bM) ) * (1.0-δnl)\n                λP=( abs(nxM * uP + nyM * vP) + CUDAnative.sqrt(gravity*HP) ) * δnl +\n                ( CUDAnative.sqrt(gravity*bP) ) * (1.0-δnl)\n                λ = max( λM, λP )\n\n                #Compute Numerical Flux and Update\n                fluxhS = (nxM * (fluxhM_x + fluxhP_x) + nyM * (fluxhM_y + fluxhP_y) +\n                          - λ * (hP - hM)) / 2\n                fluxUS = (nxM * (fluxUM_x + fluxUP_x) + nyM * (fluxUM_y + fluxUP_y) +\n                          - λ * (UP - UM)) / 2\n                fluxVS = (nxM * (fluxVM_x + fluxVP_x) + nyM * (fluxVM_y + fluxVP_y) +\n                          - λ * (VP - VM)) / 2\n\n                #Update RHS\n                rhs[vidM, _h, eM] -= sMJ * fluxhS\n                rhs[vidM, _U, eM] -= sMJ * fluxUS\n                rhs[vidM, _V, eM] -= sMJ * fluxVS\n            end\n            sync_threads()\n        end\n    end\n    nothing\nend}}}{{{ Update solution (for all dimensions)@hascuda function knl_updatesolution!(::Val{dim}, ::Val{N}, rhs, Q, vgeo, nelem, rka, rkb, dt, advection) where {dim, N}\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    Nq = N+1\n    @inbounds if i <= Nq && j <= Nq && k <= Nq && e <= nelem\n        n = i + (j-1) * Nq + (k-1) * Nq * Nq\n        MJI = vgeo[n, _MJI, e]\n        for s = 1:_nstate-1\n            Q[n, s, e] += rkb * dt * rhs[n, s, e] * MJI\n            rhs[n, s, e] *= rka\n        end\n    end\n    nothing\nend}}}{{{ Fill sendQ on device with Q (for all dimensions)@hascuda function knl_fillsendQ!(::Val{dim}, ::Val{N}, sendQ, Q, sendelems) where {N, dim}\n    Nq = N + 1\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    @inbounds if i <= Nq && j <= Nq && k <= Nq && e <= length(sendelems)\n        n = i + (j-1) * Nq + (k-1) * Nq * Nq\n        re = sendelems[e]\n        for s = 1:_nstate\n            sendQ[n, s, e] = Q[n, s, re]\n        end\n    end\n    nothing\nend}}}{{{ Fill Q on device with recvQ (for all dimensions)@hascuda function knl_transferrecvQ!(::Val{dim}, ::Val{N}, Q, recvQ, nelem, nrealelem) where {N, dim}\n    Nq = N + 1\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    @inbounds if i <= Nq && j <= Nq && k <= Nq && e <= nelem\n        n = i + (j-1) * Nq + (k-1) * Nq * Nq\n        for s = 1:_nstate\n            Q[n, s, nrealelem + e] = recvQ[n, s, e]\n        end\n    end\n    nothing\nend}}}{{{ MPI Buffer handlingfunction fillsendQ!(::Val{dim}, ::Val{N}, sendQ, d_sendQ::Array, Q,\n                    sendelems) where {dim, N}\n    sendQ[:, :, :] .= Q[:, :, sendelems]\nend\n\n@hascuda function fillsendQ!(::Val{dim}, ::Val{N}, sendQ, d_sendQ::CuArray,\n                             d_QL, d_sendelems) where {dim, N}\n    nsendelem = length(d_sendelems)\n    if nsendelem > 0\n        @cuda(threads=ntuple(j->N+1, dim), blocks=nsendelem,\n              knl_fillsendQ!(Val(dim), Val(N), d_sendQ, d_QL, d_sendelems))\n        sendQ .= d_sendQ\n    end\nend\n\n@hascuda function transferrecvQ!(::Val{dim}, ::Val{N}, d_recvQ::CuArray, recvQ,\n                                 d_QL, nrealelem) where {dim, N}\n    nrecvelem = size(recvQ)[end]\n    if nrecvelem > 0\n        d_recvQ .= recvQ\n        @cuda(threads=ntuple(j->N+1, dim), blocks=nrecvelem,\n              knl_transferrecvQ!(Val(dim), Val(N), d_QL, d_recvQ, nrecvelem,\n                                 nrealelem))\n    end\nend\n\nfunction transferrecvQ!(::Val{dim}, ::Val{N}, d_recvQ::Array, recvQ, Q,\n                        nrealelem) where {dim, N}\n    Q[:, :, nrealelem+1:end] .= recvQ[:, :, :]\nend}}}{{{ GPU kernel wrappers@hascuda function volumerhs!(::Val{dim}, ::Val{N}, d_rhsC::CuArray, d_QC, d_vgeoC, d_D, elems, gravity, δnl) where {dim, N}\n    nelem = length(elems)\n    @cuda(threads=ntuple(j->N+1, dim), blocks=nelem,\n          knl_volumerhs!(Val(dim), Val(N), d_rhsC, d_QC, d_vgeoC, d_D, nelem, gravity, δnl))\nend\n\n@hascuda function fluxrhs!(::Val{dim}, ::Val{N}, d_rhsL::CuArray, d_QL, d_sgeo, elems, d_vmapM, d_vmapP, d_elemtobndy, gravity, δnl) where {dim, N}\n    nelem = length(elems)\n    @cuda(threads=(ntuple(j->N+1, dim-1)..., 1), blocks=nelem,\n          knl_fluxrhs!(Val(dim), Val(N), d_rhsL, d_QL, d_sgeo, nelem, d_vmapM, d_vmapP, d_elemtobndy, gravity, δnl))\nend\n\n@hascuda function updatesolution!(::Val{dim}, ::Val{N}, d_rhsL::CuArray, d_QL, d_vgeoL, elems, rka, rkb, dt, advection) where {dim, N}\n    nelem = length(elems)\n    @cuda(threads=ntuple(j->N+1, dim), blocks=nelem,\n          knl_updatesolution!(Val(dim), Val(N), d_rhsL, d_QL, d_vgeoL, nelem, rka, rkb, dt, advection))\nend}}}{{{ L2 Error (for all dimensions)function L2errorsquared(::Val{dim}, ::Val{N}, Q, vgeo, elems, Qex, t) where {dim, N}\n    DFloat = eltype(Q)\n    Np = (N+1)^dim\n    (~, nstate, nelem) = size(Q)\n\n    err = zero(DFloat)\n\n    @inbounds for e = elems, i = 1:Np\n        X = ntuple(j -> vgeo[i, _x-1+j, e] - Q[i, _U-1+j, e]*t, Val(dim))\n        diff = Q[i, _h, e] - Qex[i, _h, e]\n\n        err += vgeo[i, _MJ, e] * diff^2\n    end\n\n    err\nend}}}{{{ L2 Energy (for all dimensions)function L2energysquared(::Val{dim}, ::Val{N}, Q, vgeo, elems) where {dim, N}\n    DFloat = eltype(Q)\n    Np = (N+1)^dim\n    (~, nstate, nelem) = size(Q)\n\n    energy = zero(DFloat)\n\n    @inbounds for e = elems, q = 1:nstate, i = 1:Np\n        energy += vgeo[i, _MJ, e] * Q[i, q, e]^2\n    end\n\n    energy\nend}}}{{{ Send Data Qfunction senddata_Q(::Val{dim}, ::Val{N}, mesh, sendreq, recvreq, sendQ,\n                  recvQ, d_sendelems, d_sendQ, d_recvQ, d_QL, mpicomm;\n                  ArrType=ArrType) where {dim, N}\n    mpirank = MPI.Comm_rank(mpicomm)Create send and recv request array    nnabr = length(mesh.nabrtorank)\n    d_sendelems = ArrType(mesh.sendelems)\n    nrealelem = length(mesh.realelems)post MPI receives    for n = 1:nnabr\n        recvreq[n] = MPI.Irecv!((@view recvQ[:, :, mesh.nabrtorecv[n]]),\n                                mesh.nabrtorank[n], 777, mpicomm)\n    endwait on (prior) MPI sends    MPI.Waitall!(sendreq)pack data from dQL into send buffer    fillsendQ!(Val(dim), Val(N), sendQ, dsendQ, dQL, dsendelems)    sendQ[:, :, :] .= d_QL[:, :, d_sendelems]post MPI sends    for n = 1:nnabr\n        sendreq[n] = MPI.Isend((@view sendQ[:, :, mesh.nabrtosend[n]]),\n                               mesh.nabrtorank[n], 777, mpicomm)\n    end\nend}}}{{{ Send Data Grad(Q)function senddata_gradQ(::Val{dim}, ::Val{N}, mesh, sendreq, recvreq, sendQ,\n                  recvQ, d_sendelems, d_sendQ, d_recvQ, d_QL, mpicomm;\n                  ArrType=ArrType) where {dim, N}\n    mpirank = MPI.Comm_rank(mpicomm)Create send and recv request array    nnabr = length(mesh.nabrtorank)\n    d_sendelems = ArrType(mesh.sendelems)\n    nrealelem = length(mesh.realelems)post MPI receives    for n = 1:nnabr\n        recvreq[n] = MPI.Irecv!((@view recvQ[:, :, :, mesh.nabrtorecv[n]]),\n                                mesh.nabrtorank[n], 777, mpicomm)\n    endwait on (prior) MPI sends    MPI.Waitall!(sendreq)pack data from dQL into send buffer    fillsendQ!(Val(dim), Val(N), sendQ, dsendQ, dQL, dsendelems)    sendQ[:, :, :, :] .= d_QL[:, :, :, d_sendelems]post MPI sends    for n = 1:nnabr\n        sendreq[n] = MPI.Isend((@view sendQ[:, :, :, mesh.nabrtosend[n]]),\n                               mesh.nabrtorank[n], 777, mpicomm)\n    end\nend}}}{{{ Receive Data Qfunction receivedata_Q!(::Val{dim}, ::Val{N}, mesh, recvreq, recvQ,\n                        d_recvQ, d_QL) where {dim, N}\n    nrealelem = length(mesh.realelems)wait on MPI receives    MPI.Waitall!(recvreq)copy data to state vector d_QL    #transferrecvQ!(Val(dim), Val(N), d_recvQ, recvQ, d_QL, nrealelem)\n    d_QL[:, :, nrealelem+1:end] .= recvQ[:, :, :]\n\nend}}}{{{ Receive Data Grad(Q)function receivedata_gradQ!(::Val{dim}, ::Val{N}, mesh, recvreq, recvQ,\n                            d_recvQ, d_QL) where {dim, N}\n    nrealelem = length(mesh.realelems)wait on MPI receives    MPI.Waitall!(recvreq)copy data to state vector d_QL    #transferrecvQ!(Val(dim), Val(N), d_recvQ, recvQ, d_QL, nrealelem)\n    d_QL[:, :, :, nrealelem+1:end] .= recvQ[:, :, :, :]\nend}}}{{{ RK loopfunction lowstorageRK(::Val{dim}, ::Val{N}, mesh, vgeo, sgeo, Q, rhs, D,\n                      dt, nsteps, tout, vmapM, vmapP, mpicomm,\n                      gravity, δnl, advection, visc;\n                      ArrType=ArrType, plotstep=0) where {dim, N}\n    DFloat = eltype(Q)\n    mpirank = MPI.Comm_rank(mpicomm)Fourth-order, low-storage, Runge–Kutta scheme of Carpenter and Kennedy (1994) ((5,4) 2N-Storage RK scheme.Ref: @TECHREPORT{CarpenterKennedy1994,   author = {M.~H. Carpenter and C.~A. Kennedy},   title = {Fourth-order {2N-storage} {Runge-Kutta} schemes},   institution = {National Aeronautics and Space Administration},   year = {1994},   number = {NASA TM-109112},   address = {Langley Research Center, Hampton, VA}, }    RKA = (DFloat(0),\n           DFloat(-567301805773)  / DFloat(1357537059087),\n           DFloat(-2404267990393) / DFloat(2016746695238),\n           DFloat(-3550918686646) / DFloat(2091501179385),\n           DFloat(-1275806237668) / DFloat(842570457699 ))\n\n    RKB = (DFloat(1432997174477) / DFloat(9575080441755 ),\n           DFloat(5161836677717) / DFloat(13612068292357),\n           DFloat(1720146321549) / DFloat(2090206949498 ),\n           DFloat(3134564353537) / DFloat(4481467310338 ),\n           DFloat(2277821191437) / DFloat(14882151754819))\n\n    RKC = (DFloat(0),\n           DFloat(1432997174477) / DFloat(9575080441755),\n           DFloat(2526269341429) / DFloat(6820363962896),\n           DFloat(2006345519317) / DFloat(3224310063776),\n           DFloat(2802321613138) / DFloat(2924317926251))Create send and recv request array    nnabr = length(mesh.nabrtorank)\n    sendreq = fill(MPI.REQUEST_NULL, nnabr)\n    recvreq = fill(MPI.REQUEST_NULL, nnabr)Create send and recv buffer    sendQ = zeros(DFloat, (N+1)^dim, size(Q,2), length(mesh.sendelems))\n    recvQ = zeros(DFloat, (N+1)^dim, size(Q,2), length(mesh.ghostelems))Create send and recv LDG buffer    sendgradQ = zeros(DFloat, (N+1)^dim, size(Q,2), dim, length(mesh.sendelems))\n    recvgradQ = zeros(DFloat, (N+1)^dim, size(Q,2), dim, length(mesh.ghostelems))Store Constants    nrealelem = length(mesh.realelems)\n    nsendelem = length(mesh.sendelems)\n    nrecvelem = length(mesh.ghostelems)\n    nelem = length(mesh.elems)\n\n    #Create Device Arrays\n    d_QL, d_rhsL = ArrType(Q), ArrType(rhs)\n    d_vgeoL, d_sgeo = ArrType(vgeo), ArrType(sgeo)\n    d_vmapM, d_vmapP = ArrType(vmapM), ArrType(vmapP)\n    d_sendelems, d_elemtobndy = ArrType(mesh.sendelems), ArrType(mesh.elemtobndy)\n    d_sendQ, d_recvQ = ArrType(sendQ), ArrType(recvQ)\n    d_D = ArrType(D)\n    #Create Device LDG Arrays\n    d_gradQL = zeros(DFloat, (N+1)^dim, _nstate, dim, nelem)\n    d_rhs_gradQL = zeros(DFloat, (N+1)^dim, _nstate, dim, nelem)\n    d_sendgradQ, d_recvgradQ = ArrType(sendgradQ), ArrType(recvgradQ)\n\n    #Template Reshape Arrays\n    Qshape    = (fill(N+1, dim)..., size(Q, 2), size(Q, 3))\n    vgeoshape = (fill(N+1, dim)..., _nvgeo, size(Q, 3))\n    gradQshape = (fill(N+1, dim)..., size(d_gradQL,2), size(d_gradQL,3), size(d_gradQL,4))\n\n    #Reshape Device Arrays\n    d_QC = reshape(d_QL, Qshape)\n    d_rhsC = reshape(d_rhsL, Qshape...)\n    d_vgeoC = reshape(d_vgeoL, vgeoshape)\n    #Reshape Device LDG Arrays\n    d_gradQC = reshape(d_gradQL, gradQshape)\n    d_rhs_gradQC = reshape(d_rhs_gradQL, gradQshape...)Begin Time Loop    start_time = t1 = time_ns()\n    for step = 1:nsteps\n        for s = 1:length(RKA)Send Data Q            senddata_Q(Val(dim), Val(N), mesh, sendreq, recvreq, sendQ,\n                       recvQ, d_sendelems, d_sendQ, d_recvQ, d_QL, mpicomm;\n                       ArrType=ArrType)volume RHS computation            volumerhs!(Val(dim), Val(N), d_rhsC, d_QC, d_vgeoC, d_D, mesh.realelems, gravity, δnl)Receive Data Q            receivedata_Q!(Val(dim), Val(N), mesh, recvreq, recvQ, d_recvQ, d_QL)face RHS computation            fluxrhs!(Val(dim), Val(N), d_rhsL, d_QL, d_sgeo, mesh.realelems, d_vmapM, d_vmapP, d_elemtobndy, gravity, δnl)            if (visc > 0)volume grad Q computation                volume_grad!(Val(dim), Val(N), d_rhs_gradQC, d_QC, d_vgeoC, d_D, mesh.realelems)face grad Q computation                flux_grad!(Val(dim), Val(N), d_rhs_gradQL, d_QL, d_sgeo, mesh.realelems, d_vmapM, d_vmapP, d_elemtobndy)Construct grad Q                update_gradQ!(Val(dim), Val(N), d_gradQL, d_rhs_gradQL, d_vgeoL, mesh.realelems)Send Data grad(Q)                senddata_gradQ(Val(dim), Val(N), mesh, sendreq, recvreq, sendgradQ,\n                               recvgradQ, d_sendelems, d_sendgradQ, d_recvgradQ,\n                               d_gradQL, mpicomm;ArrType=ArrType)volume div (grad Q) computation                volume_div!(Val(dim), Val(N), d_rhs_gradQC, d_gradQC, d_vgeoC, d_D, mesh.realelems)Receive Data grad(Q)                receivedata_gradQ!(Val(dim), Val(N), mesh, recvreq, recvgradQ, d_recvgradQ, d_gradQL)face div (grad Q) computation                flux_div!(Val(dim), Val(N), d_rhs_gradQL, d_gradQL, d_sgeo, mesh.realelems, d_vmapM, d_vmapP, d_elemtobndy)\n            endupdate solution and scale RHS            updatesolution!(Val(dim), Val(N), d_rhsL, d_rhs_gradQL, d_QL, d_vgeoL, mesh.realelems,\n                            RKA[s%length(RKA)+1], RKB[s], dt, advection, visc)\n        end\n        if step == 1\n            @hascuda synchronize()\n            start_time = time_ns()\n        end\n        if mpirank == 0 && (time_ns() - t1)*1e-9 > tout\n            @hascuda synchronize()\n            t1 = time_ns()\n            avg_stage_time = (time_ns() - start_time) * 1e-9 /\n            ((step-1) * length(RKA))\n            @show (step, nsteps, avg_stage_time)\n        endWrite VTK file        if plotstep > 0 && step % plotstep == 0\n            Q .= d_QL\n            X = ntuple(j->reshape((@view vgeo[:, _x+j-1, :]), ntuple(j->N+1,dim)...,\n                                  nelem), dim)\n            h = reshape((@view Q[:, _h, :]), ntuple(j->(N+1),dim)..., nelem)\n            b = reshape((@view Q[:, _b, :]), ntuple(j->(N+1),dim)..., nelem)\n            U = reshape((@view Q[:, _U, :]), ntuple(j->(N+1),dim)..., nelem) ./ (h.+b)\n            V = reshape((@view Q[:, _V, :]), ntuple(j->(N+1),dim)..., nelem) ./ (h.+b)\n\n            writemesh(@sprintf(\"viz/swe%dD_%s_rank_%04d_step_%05d\",\n                               dim, ArrType, mpirank, step), X...;\n                      fields=((\"h\", h),(\"b\",b),(\"U\",U),(\"V\",V),),\n                      realelems=mesh.realelems)\n        end\n    end\nQ .= d_QL\nrhs .= d_rhsL\nend}}}{{{ SWE driverfunction swe(::Val{dim}, ::Val{N}, mpicomm, ic, mesh, tend, gravity, δnl,\n             advection, visc; meshwarp=(x...)->identity(x), tout = 60, ArrType=Array,\n             plotstep=0) where {dim, N}\n    DFloat = typeof(tend)\n\n    mpirank = MPI.Comm_rank(mpicomm)\n    mpisize = MPI.Comm_size(mpicomm)Partion the mesh using a Hilbert curve based partitioning    mpirank == 0 && println(\"[CPU] partitioning mesh...\")\n    mesh = partition(mpicomm, mesh...)Connect the mesh in parallel    mpirank == 0 && println(\"[CPU] connecting mesh...\")\n    mesh = connectmesh(mpicomm, mesh...)Get the vmaps    mpirank == 0 && println(\"[CPU] computing mappings...\")\n    (vmapM, vmapP) = mappings(N, mesh.elemtoelem, mesh.elemtoface,\n                              mesh.elemtoordr)Create 1-D operators    (ξ, ω) = lglpoints(DFloat, N)\n    D = spectralderivative(ξ)Compute the geometry    mpirank == 0 && println(\"[CPU] computing metrics...\")\n    (vgeo, sgeo) = computegeometry(Val(dim), mesh, D, ξ, ω, meshwarp, vmapM)\n    (nface, nelem) = size(mesh.elemtoelem)Storage for the solution, rhs, and error    mpirank == 0 && println(\"[CPU] creating fields (CPU)...\")\n    Q = zeros(DFloat, (N+1)^dim, _nstate, nelem)\n    Qexact = zeros(DFloat, (N+1)^dim, _nstate, nelem)\n    rhs = zeros(DFloat, (N+1)^dim, _nstate, nelem)setup the initial condition    mpirank == 0 && println(\"[CPU] computing initial conditions (CPU)...\")\n    @inbounds for e = 1:nelem, i = 1:(N+1)^dim\n        x, y = vgeo[i, _x, e], vgeo[i, _y, e]\n        h, b, U, V = ic(x, y)\n        Q[i, _h, e] = h\n        Q[i, _b, e] = b\n        Q[i, _U, e] = U\n        Q[i, _V, e] = V\n        Qexact[i, _h, e] = h\n        Qexact[i, _b, e] = b\n        Qexact[i, _U, e] = U\n        Qexact[i, _V, e] = V\n    endCompute time step    mpirank == 0 && println(\"[CPU] computing dt (CPU)...\")\n    (base_dt, Courant) = courantnumber(Val(dim), Val(N), vgeo, Q, mpicomm, gravity, δnl, advection)\n    mpirank == 0 && @show (base_dt,Courant)\n\n    nsteps = ceil(Int64, tend / base_dt)\n    dt = tend / nsteps\n    mpirank == 0 && @show (dt, nsteps, dt * nsteps, tend)Do time stepping    stats = zeros(DFloat, 3)\n    mpirank == 0 && println(\"[CPU] computing initial energy...\")\n    stats[1] = L2energysquared(Val(dim), Val(N), Q, vgeo, mesh.realelems)plot initial condition    mkpath(\"viz\")\n    X = ntuple(j->reshape((@view vgeo[:, _x+j-1, :]), ntuple(j->N+1,dim)...,\n                          nelem), dim)\n    h = reshape((@view Q[:, _h, :]), ntuple(j->(N+1),dim)..., nelem)\n    b = reshape((@view Q[:, _b, :]), ntuple(j->(N+1),dim)..., nelem)\n    U = reshape((@view Q[:, _U, :]), ntuple(j->(N+1),dim)..., nelem) ./ (h.+b)\n    V = reshape((@view Q[:, _V, :]), ntuple(j->(N+1),dim)..., nelem) ./ (h.+b)\n    writemesh(@sprintf(\"viz/swe%dD_%s_rank_%04d_step_%05d\",\n                       dim, ArrType, mpirank, 0), X...;\n              fields=((\"h\", h),(\"b\",b),(\"U\",U),(\"V\",V),),\n              realelems=mesh.realelems)\n\n    #Call Time-stepping Routine\n    mpirank == 0 && println(\"[DEV] starting time stepper...\")\n    lowstorageRK(Val(dim), Val(N), mesh, vgeo, sgeo, Q, rhs, D, dt, nsteps, tout,\n                 vmapM, vmapP, mpicomm, gravity, δnl, advection, visc;\n                 ArrType=ArrType, plotstep=plotstep)plot final solution    X = ntuple(j->reshape((@view vgeo[:, _x+j-1, :]), ntuple(j->N+1,dim)...,\n                          nelem), dim)\n    h = reshape((@view Q[:, _h, :]), ntuple(j->(N+1),dim)..., nelem)\n    b = reshape((@view Q[:, _b, :]), ntuple(j->(N+1),dim)..., nelem)\n    U = reshape((@view Q[:, _U, :]), ntuple(j->(N+1),dim)..., nelem) ./ (h.+b)\n    V = reshape((@view Q[:, _V, :]), ntuple(j->(N+1),dim)..., nelem) ./ (h.+b)\n    writemesh(@sprintf(\"viz/swe%dD_%s_rank_%04d_step_%05d\",\n                       dim, ArrType, mpirank, nsteps), X...;\n              fields=((\"h\", h),(\"b\",b),(\"U\",U),(\"V\",V),),\n              realelems=mesh.realelems)\n\n    mpirank == 0 && println(\"[CPU] computing final energy...\")\n    stats[2] = L2energysquared(Val(dim), Val(N), Q, vgeo, mesh.realelems)\n    stats[3] = L2errorsquared(Val(dim), Val(N), Q, vgeo, mesh.realelems, Qexact,\n                              tend)\n\n    stats = sqrt.(MPI.allreduce(stats, MPI.SUM, mpicomm))\n\n    if  mpirank == 0\n        @show eng0 = stats[1]\n        @show engf = stats[2]\n        @show Δeng = engf - eng0\n        @show err = stats[3]\n    end\nend}}}{{{ mainfunction main()\n    DFloat = Float64\n\n    MPI.Initialized() || MPI.Init()\n    MPI.finalize_atexit()\n\n    mpicomm = MPI.COMM_WORLD\n    mpirank = MPI.Comm_rank(mpicomm)\n    mpisize = MPI.Comm_size(mpicomm)FIXME: query via hostname    @hascuda device!(mpirank % length(devices()))\n\n    #Input Parameters\n    N=8\n    Ne=10\n    visc=0.001\n    iplot=10\n    δnl=1\n    icase=20\n    time_final=DFloat(0.32)\n    hardware=\"cpu\"\n    if mpirank == 0\n        @show (N,Ne,visc,iplot,δnl,icase,time_final,hardware,mpisize)\n    end\n\n    #Initial Conditions\n    ic = (x...) -> (0.0, 0.0, 0.0, 0.0)\n    if icase == 1 #advection\n        function ic1(x...)\n            r = sqrt( (x[1]-0.5)^2 + (x[2]-0.5)^2 )\n            h = 0.5 * exp(-100.0 * r^2)\n            b = 1.0\n            H = h + b\n            U = H*(1.0)\n            V = H*(0.0)\n            h, b, U, V\n        end\n        ic = ic1\n        periodic = (true, true)\n        advection = true\n        gravity = 0\n    elseif icase == 10 #shallow water with Periodic BCs\n        function ic10(x...)\n            r = sqrt( (x[1]-0.5)^2 + (x[2]-0.5)^2 )\n            h = 0.5 * exp(-100.0 * r^2)\n            b=1.0\n            H = h + b\n            U = H*(0.0)\n            V = H*(0.0)\n            h, b, U, V\n        end\n        ic = ic10\n        periodic = (true, true)\n        advection = false\n        gravity = 10\n    elseif icase == 20 #shallow water with Periodic BCs\n        function ic20(x...)\n            r = sqrt( (x[1]-0.5)^2 + (x[2]-0.5)^2 )\n            rc=0.25\n            h=0\n            if (r<=rc)\n                h = 0.25\n            end\n            b=1.0\n            H = h + b\n            U = H*(0.0)\n            V = H*(0.0)\n            h, b, U, V\n        end\n        ic = ic20\n        periodic = (true, true)\n        advection = false\n        gravity = 10\n    elseif icase == 100 #shallow water with NFBC\n        function ic100(x...)\n            r = sqrt( (x[1]-0.5)^2 + (x[2]-0.5)^2 )\n            h = 0.5 * exp(-100.0 * r^2)\n            b=1.0\n            H = h + b\n            U = H*(0.0)\n            V = H*(0.0)\n            h, b, U, V\n        end\n        ic = ic100\n        periodic = (false, false)\n        advection = false\n        gravity = 10\n    end\n\n    mesh = brickmesh((range(DFloat(0); length=Ne+1, stop=1),\n                      range(DFloat(0); length=Ne+1, stop=1)),\n                     periodic; part=mpirank+1, numparts=mpisize)\n\n    if hardware == \"cpu\"\n        mpirank == 0 && println(\"Running (CPU)...\")\n        swe(Val(2), Val(N), mpicomm, ic, mesh, time_final, gravity, δnl, advection, visc;\n            ArrType=Array, tout = 10, plotstep = iplot)\n        mpirank == 0 && println()\n    elseif hardware == \"gpu\"\n        @hascuda begin\n            mpirank == 0 && println(\"Running (GPU)...\")\n            swe(Val(2), Val(N), mpicomm, ic, mesh, time_final, gravity, δnl, advection, visc;\n                ArrType=CuArray, tout = 10, plotstep = iplot)\n            mpirank == 0 && println()\n        end\n    end\n    nothing\nend}}}main()This page was generated using Literate.jl."
},

{
    "location": "examples/generated/2d_kernels/nse2d/#",
    "page": "2D Compressible Navier-Stokes Equations",
    "title": "2D Compressible Navier-Stokes Equations",
    "category": "page",
    "text": "EditURL = \"https://github.com/climate-machine/Canary.jl/blob/master/examples/2d_kernels/nse2d.jl\""
},

{
    "location": "examples/generated/2d_kernels/nse2d/#D-Compressible-Navier-Stokes-Equations-1",
    "page": "2D Compressible Navier-Stokes Equations",
    "title": "2D Compressible Navier-Stokes Equations",
    "category": "section",
    "text": ""
},

{
    "location": "examples/generated/2d_kernels/nse2d/#Introduction-1",
    "page": "2D Compressible Navier-Stokes Equations",
    "title": "Introduction",
    "category": "section",
    "text": "This example shows how to solve the 2D compressible Navier-Stokes equations using vanilla DG."
},

{
    "location": "examples/generated/2d_kernels/nse2d/#Continuous-Governing-Equations-1",
    "page": "2D Compressible Navier-Stokes Equations",
    "title": "Continuous Governing Equations",
    "category": "section",
    "text": "We solve the following equation:fracpartial rhopartial t + nabla cdot mathbfU = 0   (11)fracpartial mathbfUpartial t + nabla cdot left( fracmathbfU otimes mathbfUrho + P mathbfI_2 right) + rho g hatmathbfk= nabla cdot mathbfF_U^visc   (12)fracpartial Epartial t + nabla cdot left( fracmathbfU left(E+P right)rho right) = nabla cdot mathbfF_E^visc   (13)where mathbfu=(uv) is the velocity, mathbfU=rho mathbfu, is the momentum, with rho the total density and E=(gamma-1) rho left( c_v T + frac12 mathbfu cdot mathbfu + g z right) the total energy (internal + kinetic + potential). The viscous fluxes are defined as followsmathbfF_U^visc = mu left( nabla mathbfu +  left( nabla mathbfu right)^T + lambda left( nabla cdot mathbfu right)  mathbfI_2 right)andmathbfF_E^visc =  mathbfu cdot mathbfF_U^visc + fracc_pPr nabla Twhere mu is the kinematic (or artificial) viscosity, lambda=-frac23 is the Stokes hypothesis, Pr approx 071 is the Prandtl number for air and T is the temperature. We employ periodic boundary conditions in the horizontaland no-flux boundary conditions in the vertical.  At the bottom and top of the domain, we need to impose no-flux boundary conditions in nabla T to avoid a (artificial) thermal boundary layer."
},

{
    "location": "examples/generated/2d_kernels/nse2d/#Discontinous-Galerkin-Method-1",
    "page": "2D Compressible Navier-Stokes Equations",
    "title": "Discontinous Galerkin Method",
    "category": "section",
    "text": "To solve Eq. (1) we use the discontinuous Galerkin method with basis functions comprised of Lagrange polynomials based on Lobatto points. Multiplying Eq. (1) by a test function psi and integrating within each element Omega_e such that Omega = bigcup_e=1^N_e Omega_e we getint_Omega_e psi fracpartial mathbfq^(e)_Npartial t dOmega_e + int_Omega_e psi nabla cdot mathbfF^(e)_N dOmega_e =  int_Omega_e psi Sleft( q^(e)_N right) dOmega_e   (2)where mathbfq^(e)_N=sum_i=1^(N+1)^dim psi_i(mathbfx) mathbfq_i(t) is the finite dimensional expansion with basis functions psi(mathbfx), where mathbfq=left( rho mathbfU^T E right)^T,mathbfF=left( mathbfU fracmathbfU otimes mathbfUrho + P mathbfI_2   fracmathbfU left(E+P right)rho right)and Sleft( q^(e)_N right)  = left( 0 nabla cdot mathbfF_U^visc nabla cdot mathbfF_E^visc right)Integrating Eq. (2) by parts yieldsint_Omega_e psi fracpartial mathbfq^(e)_Npartial t dOmega_e + int_Gamma_e psi mathbfn cdot mathbfF^(*e)_N dGamma_e - int_Omega_e nabla psi cdot mathbfF^(e)_N dOmega_e = int_Omega_e psi Sleft( q^(e)_N right) dOmega_e   (3)where the second term on the left denotes the flux integral term (computed in \"function flux_rhs\") and the third term denotes the volume integral term (computed in \"function volume_rhs\").  The superscript (*e) in the flux integral term denotes the numerical flux. Here we use the Rusanov flux."
},

{
    "location": "examples/generated/2d_kernels/nse2d/#Local-Discontinous-Galerkin-Method-1",
    "page": "2D Compressible Navier-Stokes Equations",
    "title": "Local Discontinous Galerkin Method",
    "category": "section",
    "text": "To approximate the second order terms on the right hand side of Eq. (1) we use the local discontinuous Galerkin (LDG) method, which we described in LDG2d.jl. We will highlight the main steps below for completeness. We employ the following two-step process: first we approximate the gradient of q as followsmathbfQ(xy) = nabla mathbfq(xy)   (4)where mathbfQ is an auxiliary vector function, followed bynabla cdot  mathbfF^visc left( mathbfQ right)   (5)which completes the approximation of the second order derivatives."
},

{
    "location": "examples/generated/2d_kernels/nse2d/#Commented-Program-1",
    "page": "2D Compressible Navier-Stokes Equations",
    "title": "Commented Program",
    "category": "section",
    "text": "include(joinpath(@__DIR__,\"vtk.jl\"))\nusing MPI\nusing Canary\nusing Printf: @sprintf\nconst HAVE_CUDA = try\n    using CUDAnative\n    using CUDAdrv\n    true\ncatch\n    false\nend\nif HAVE_CUDA\n    macro hascuda(ex)\n        return :($(esc(ex)))\n    end\nelse\n    macro hascuda(ex)\n        return :()\n    end\nend{{{ reshape for CuArray@hascuda function Base.reshape(A::CuArray, dims::NTuple{N, Int}) where {N}\n    @assert prod(dims) == prod(size(A))\n    CuArray{eltype(A), length(dims)}(dims, A.buf)\nend}}}{{{ constants note the order of the fields below is also assumed in the code.const _nstate = 4\nconst _U, _V, _ρ, _E = 1:_nstate\nconst stateid = (U = _U, V = _V, ρ = _ρ, E = _E)\n\nconst _nvgeo = 8\nconst _ξx, _ηx, _ξy, _ηy, _MJ, _MJI,\n_x, _y, = 1:_nvgeo\nconst vgeoid = (ξx = _ξx, ηx = _ηx,\n                ξy = _ξy, ηy = _ηy,\n                MJ = _MJ, MJI = _MJI,\n                x = _x,   y = _y)\n\nconst _nsgeo = 4\nconst _nx, _ny, _sMJ, _vMJI = 1:_nsgeo\nconst sgeoid = (nx = _nx, ny = _ny, sMJ = _sMJ, vMJI = _vMJI)\n\nconst _γ = 14  // 10\nconst _p0 = 100000\nconst _R_gas = 28717 // 100\nconst _c_p = 100467 // 100\nconst _c_v = 7175 // 10\nconst _gravity = 10\nconst _Prandtl = 71 // 10\nconst _Stokes = -2 // 3}}}{{{ courantfunction courantnumber(::Val{dim}, ::Val{N}, vgeo, Q, mpicomm) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1)^dim\n    (~, ~, nelem) = size(Q)\n\n    dt = [floatmax(DFloat)]\n    Courant = - [floatmax(DFloat)]\n\n    #Compute DT\n    @inbounds for e = 1:nelem, n = 1:Np\n        ρ, U, V, E = Q[n, _ρ, e], Q[n, _U, e], Q[n, _V, e], Q[n, _E, e]\n        ξx, ξy, ηx, ηy = vgeo[n, _ξx, e], vgeo[n, _ξy, e], vgeo[n, _ηx, e], vgeo[n, _ηy, e]\n        y = vgeo[n, _y, e]\n        P = (R_gas/c_v)*(E - (U^2 + V^2)/(2*ρ) - ρ*gravity*y)\n        u, v = U/ρ, V/ρ\n        dx=sqrt( (1.0/(2*ξx))^2 + (1.0/(2*ηy))^2 )\n        vel=sqrt( u^2 + v^2 )\n        wave_speed = (vel + sqrt(γ * P / ρ))\n        loc_dt = 1.0*dx/wave_speed/N\n        dt[1] = min(dt[1], loc_dt)\n    end\n    dt_min=MPI.Allreduce(dt[1], MPI.MIN, mpicomm)\n\n    #Compute Courant\n    @inbounds for e = 1:nelem, n = 1:Np\n        ρ, U, V, E = Q[n, _ρ, e], Q[n, _U, e], Q[n, _V, e], Q[n, _E, e]\n        ξx, ξy, ηx, ηy = vgeo[n, _ξx, e], vgeo[n, _ξy, e], vgeo[n, _ηx, e], vgeo[n, _ηy, e]\n        y = vgeo[n, _y, e]\n        P = (R_gas/c_v)*(E - (U^2 + V^2)/(2*ρ) - ρ*gravity*y)\n        u, v = U/ρ, V/ρ\n        dx=sqrt( (1.0/(2*ξx))^2 + (1.0/(2*ηy))^2 )\n        vel=sqrt( u^2 + v^2 )\n        wave_speed = (vel + sqrt(γ * P / ρ))\n        loc_Courant = wave_speed*dt_min/dx*N\n        Courant[1] = max(Courant[1], loc_Courant)\n    end\n    Courant_max=MPI.Allreduce(Courant[1], MPI.MAX, mpicomm)\n\n    (dt_min, Courant_max)\nend}}}{{{ compute geometryfunction computegeometry(::Val{dim}, mesh, D, ξ, ω, meshwarp, vmapM) where dimCompute metric terms    Nq = size(D, 1)\n    DFloat = eltype(D)\n\n    (nface, nelem) = size(mesh.elemtoelem)\n\n    crd = creategrid(Val(dim), mesh.elemtocoord, ξ)\n\n    vgeo = zeros(DFloat, Nq^dim, _nvgeo, nelem)\n    sgeo = zeros(DFloat, _nsgeo, Nq^(dim-1), nface, nelem)\n\n    (ξx, ηx, ξy, ηy, MJ, MJI, x, y) =\n        ntuple(j->(@view vgeo[:, j, :]), _nvgeo)\n    J = similar(x)\n    (nx, ny, sMJ, vMJI) = ntuple(j->(@view sgeo[ j, :, :, :]), _nsgeo)\n    sJ = similar(sMJ)\n\n    X = ntuple(j->(@view vgeo[:, _x+j-1, :]), dim)\n    creategrid!(X..., mesh.elemtocoord, ξ)\n\n    @inbounds for j = 1:length(x)\n        (x[j], y[j]) = meshwarp(x[j], y[j])\n    endCompute the metric terms    computemetric!(x, y, J, ξx, ηx, ξy, ηy, sJ, nx, ny, D)\n\n    M = kron(1, ntuple(j->ω, dim)...)\n    MJ .= M .* J\n    MJI .= 1 ./ MJ\n    vMJI .= MJI[vmapM]\n\n    sM = dim > 1 ? kron(1, ntuple(j->ω, dim-1)...) : one(DFloat)\n    sMJ .= sM .* sJ\n\n    (vgeo, sgeo)\nend}}}{{{ CPU Kernels Volume RHSfunction volume_rhs!(::Val{2}, ::Val{N}, rhs::Array, Q, vgeo, D, elems) where N\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Nq = N + 1\n    nelem = size(Q)[end]\n\n    Q = reshape(Q, Nq, Nq, _nstate, nelem)\n    rhs = reshape(rhs, Nq, Nq, _nstate, nelem)\n    vgeo = reshape(vgeo, Nq, Nq, _nvgeo, nelem)\n\n    #Allocate Arrays\n    s_F = Array{DFloat}(undef, Nq, Nq, _nstate)\n    s_G = Array{DFloat}(undef, Nq, Nq, _nstate)\n\n    @inbounds for e in elems\n        for j = 1:Nq, i = 1:Nq\n            MJ = vgeo[i, j, _MJ, e]\n            ξx, ξy = vgeo[i,j,_ξx,e], vgeo[i,j,_ξy,e]\n            ηx, ηy = vgeo[i,j,_ηx,e], vgeo[i,j,_ηy,e]\n            y = vgeo[i,j,_y,e]\n\n            U, V = Q[i, j, _U, e], Q[i, j, _V, e]\n            ρ, E = Q[i, j, _ρ, e], Q[i, j, _E, e]\n            P = (R_gas/c_v)*(E - (U^2 + V^2)/(2*ρ) - ρ*gravity*y)\n\n            ρinv = 1 / ρ\n            fluxρ_x = U\n            fluxU_x = ρinv * U * U + P\n            fluxV_x = ρinv * V * U\n            fluxE_x = ρinv * U * (E+P)\n\n            fluxρ_y = V\n            fluxU_y = ρinv * U * V\n            fluxV_y = ρinv * V * V + P\n            fluxE_y = ρinv * V * (E+P)\n\n            s_F[i, j, _ρ] = MJ * (ξx * fluxρ_x + ξy * fluxρ_y)\n            s_F[i, j, _U] = MJ * (ξx * fluxU_x + ξy * fluxU_y)\n            s_F[i, j, _V] = MJ * (ξx * fluxV_x + ξy * fluxV_y)\n            s_F[i, j, _E] = MJ * (ξx * fluxE_x + ξy * fluxE_y)\n\n            s_G[i, j, _ρ] = MJ * (ηx * fluxρ_x + ηy * fluxρ_y)\n            s_G[i, j, _U] = MJ * (ηx * fluxU_x + ηy * fluxU_y)\n            s_G[i, j, _V] = MJ * (ηx * fluxV_x + ηy * fluxV_y)\n            s_G[i, j, _E] = MJ * (ηx * fluxE_x + ηy * fluxE_y)buoyancy term            rhs[i, j, _V, e] -= MJ * ρ * gravity\n        endloop of ξ-grid lines        for s = 1:_nstate, j = 1:Nq, i = 1:Nq, k = 1:Nq\n            rhs[i, j, s, e] += D[k, i] * s_F[k, j, s]\n        endloop of η-grid lines        for s = 1:_nstate, j = 1:Nq, i = 1:Nq, k = 1:Nq\n            rhs[i, j, s, e] += D[k, j] * s_G[i, k, s]\n        end\n    end\nendFlux RHSfunction flux_rhs!(::Val{dim}, ::Val{N}, rhs::Array, Q, sgeo, vgeo, elems, vmapM,\n                  vmapP, elemtobndy) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1)^dim\n    Nfp = (N+1)^(dim-1)\n    nface = 2*dim\n\n    @inbounds for e in elems\n        for f = 1:nface\n            for n = 1:Nfp\n                nxM, nyM, sMJ = sgeo[_nx, n, f, e], sgeo[_ny, n, f, e], sgeo[_sMJ, n, f, e]\n                idM, idP = vmapM[n, f, e], vmapP[n, f, e]\n\n                eM, eP = e, ((idP - 1) ÷ Np) + 1\n                vidM, vidP = ((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1\n\n                #Left variables\n                ρM = Q[vidM, _ρ, eM]\n                UM = Q[vidM, _U, eM]\n                VM = Q[vidM, _V, eM]\n                EM = Q[vidM, _E, eM]\n                yM = vgeo[vidM, _y, eM]\n                PM = (R_gas/c_v)*(EM - (UM^2 + VM^2)/(2*ρM) - ρM*gravity*yM)\n\n                #Right variables\n                bc = elemtobndy[f, e]\n                ρP = UP = VP = EP = PP = zero(eltype(Q))\n                if bc == 0\n                    ρP = Q[vidP, _ρ, eP]\n                    UP = Q[vidP, _U, eP]\n                    VP = Q[vidP, _V, eP]\n                    EP = Q[vidP, _E, eP]\n                    yP = vgeo[vidP, _y, eP]\n                    PP = (R_gas/c_v)*(EP - (UP^2 + VP^2)/(2*ρP) - ρP*gravity*yP)\n                elseif bc == 1\n                    UnM = nxM * UM + nyM * VM\n                    UP = UM - 2 * UnM * nxM\n                    VP = VM - 2 * UnM * nyM\n                    ρP = ρM\n                    EP = EM\n                    PP = PM\n                else\n                    error(\"Invalid boundary conditions $bc on face $f of element $e\")\n                end\n\n                #Left fluxes\n                ρMinv = 1 / ρM\n                fluxρM_x = UM\n                fluxUM_x = ρMinv * UM * UM + PM\n                fluxVM_x = ρMinv * VM * UM\n                fluxEM_x = ρMinv * UM * (EM+PM)\n\n                fluxρM_y = VM\n                fluxUM_y = ρMinv * UM * VM\n                fluxVM_y = ρMinv * VM * VM + PM\n                fluxEM_y = ρMinv * VM * (EM+PM)\n\n                #Right fluxes\n                ρPinv = 1 / ρP\n                fluxρP_x = UP\n                fluxUP_x = ρPinv * UP * UP + PP\n                fluxVP_x = ρPinv * VP * UP\n                fluxEP_x = ρPinv * UP * (EP+PP)\n\n                fluxρP_y = VP\n                fluxUP_y = ρPinv * UP * VP\n                fluxVP_y = ρPinv * VP * VP + PP\n                fluxEP_y = ρPinv * VP * (EP+PP)\n\n                #Compute Wave Speed\n                λM = ρMinv * abs(nxM * UM + nyM * VM) + sqrt(ρMinv * γ * PM)\n                λP = ρPinv * abs(nxM * UP + nyM * VP) + sqrt(ρPinv * γ * PP)\n                λ  =  max(λM, λP)\n\n                #Compute Numerical Flux\n                fluxρS = (nxM * (fluxρM_x + fluxρP_x) + nyM * (fluxρM_y + fluxρP_y) +\n                          - λ * (ρP - ρM)) / 2\n                fluxUS = (nxM * (fluxUM_x + fluxUP_x) + nyM * (fluxUM_y + fluxUP_y) +\n                          - λ * (UP - UM)) / 2\n                fluxVS = (nxM * (fluxVM_x + fluxVP_x) + nyM * (fluxVM_y + fluxVP_y) +\n                          - λ * (VP - VM)) / 2\n                fluxES = (nxM * (fluxEM_x + fluxEP_x) + nyM * (fluxEM_y + fluxEP_y) +\n                          - λ * (EP - EM)) / 2\n\n\n                #Update RHS\n                rhs[vidM, _ρ, eM] -= sMJ * fluxρS\n                rhs[vidM, _U, eM] -= sMJ * fluxUS\n                rhs[vidM, _V, eM] -= sMJ * fluxVS\n                rhs[vidM, _E, eM] -= sMJ * fluxES\n            end\n        end\n    end\nend}}}{{{ Volume grad(Q)function volume_grad!(::Val{dim}, ::Val{N}, rhs::Array, Q, vgeo, D, elems) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Nq = N + 1\n    nelem = size(Q)[end]\n\n    Q = reshape(Q, Nq, Nq, _nstate, nelem)\n    rhs = reshape(rhs, Nq, Nq, _nstate, dim, nelem)\n    vgeo = reshape(vgeo, Nq, Nq, _nvgeo, nelem)\n\n    #Initialize RHS vector\n    fill!( rhs, zero(rhs[1]))\n\n    #Allocate Arrays\n    s_F = Array{DFloat}(undef, Nq, Nq, _nstate, dim)\n    s_G = Array{DFloat}(undef, Nq, Nq, _nstate, dim)\n\n    @inbounds for e in elems\n        for j = 1:Nq, i = 1:Nq\n            MJ = vgeo[i, j, _MJ, e]\n            ξx, ξy = vgeo[i,j,_ξx,e], vgeo[i,j,_ξy,e]\n            ηx, ηy = vgeo[i,j,_ηx,e], vgeo[i,j,_ηy,e]\n            y = vgeo[i,j,_y,e]\n\n            U, V = Q[i, j, _U, e], Q[i, j, _V, e]\n            ρ, E = Q[i, j, _ρ, e], Q[i, j, _E, e]\n            P = (R_gas/c_v)*(E - (U^2 + V^2)/(2*ρ) - ρ*gravity*y)\n\n            #Primitive variables\n            u=U/ρ\n            v=V/ρ\n            T=P/(R_gas*ρ)\n\n            #Compute fluxes\n            fluxρ = ρ\n            fluxU = u\n            fluxV = v\n            fluxE = T\n\n            s_F[i, j, _ρ, 1], s_F[i, j, _ρ, 2] = MJ * (ξx * fluxρ), MJ * (ξy * fluxρ)\n            s_F[i, j, _U, 1], s_F[i, j, _U, 2] = MJ * (ξx * fluxU), MJ * (ξy * fluxU)\n            s_F[i, j, _V, 1], s_F[i, j, _V, 2] = MJ * (ξx * fluxV), MJ * (ξy * fluxV)\n            s_F[i, j, _E, 1], s_F[i, j, _E, 2] = MJ * (ξx * fluxE), MJ * (ξy * fluxE)\n\n            s_G[i, j, _ρ, 1], s_G[i, j, _ρ, 2] = MJ * (ηx * fluxρ), MJ * (ηy * fluxρ)\n            s_G[i, j, _U, 1], s_G[i, j, _U, 2] = MJ * (ηx * fluxU), MJ * (ηy * fluxU)\n            s_G[i, j, _V, 1], s_G[i, j, _V, 2] = MJ * (ηx * fluxV), MJ * (ηy * fluxV)\n            s_G[i, j, _E, 1], s_G[i, j, _E, 2] = MJ * (ηx * fluxE), MJ * (ηy * fluxE)\n        endloop of ξ-grid lines        for s = 1:_nstate, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, s, 1, e] -= D[n, i] * s_F[n, j, s, 1]\n            rhs[i, j, s, 2, e] -= D[n, i] * s_F[n, j, s, 2]\n        endloop of η-grid lines        for s = 1:_nstate, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, s, 1, e] -= D[n, j] * s_G[i, n, s, 1]\n            rhs[i, j, s, 2, e] -= D[n, j] * s_G[i, n, s, 2]\n        end\n    end\nend}}}Flux grad(Q)function flux_grad!(::Val{dim}, ::Val{N}, rhs::Array,  Q, sgeo, vgeo, elems, vmapM, vmapP, elemtobndy) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1)^dim\n    Nfp = (N+1)^(dim-1)\n    nface = 2*dim\n\n    @inbounds for e in elems\n        for f = 1:nface\n            for n = 1:Nfp\n                nxM, nyM, sMJ = sgeo[_nx, n, f, e], sgeo[_ny, n, f, e], sgeo[_sMJ, n, f, e]\n                idM, idP = vmapM[n, f, e], vmapP[n, f, e]\n\n                eM, eP = e, ((idP - 1) ÷ Np) + 1\n                vidM, vidP = ((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1\n\n                #Left variables\n                ρM = Q[vidM, _ρ, eM]\n                UM = Q[vidM, _U, eM]\n                VM = Q[vidM, _V, eM]\n                EM = Q[vidM, _E, eM]\n                yM = vgeo[vidM, _y, eM]\n                PM = (R_gas/c_v)*(EM - (UM^2 + VM^2)/(2*ρM) - ρM*gravity*yM)\n                uM=UM/ρM\n                vM=VM/ρM\n                TM=PM/(R_gas*ρM)\n\n                #Right variables\n                bc = elemtobndy[f, e]\n                ρP = UP = VP = EP = PP = zero(eltype(Q))\n                if bc == 0\n                    ρP = Q[vidP, _ρ, eP]\n                    UP = Q[vidP, _U, eP]\n                    VP = Q[vidP, _V, eP]\n                    EP = Q[vidP, _E, eP]\n                    yP = vgeo[vidP, _y, eP]\n                    PP = (R_gas/c_v)*(EP - (UP^2 + VP^2)/(2*ρP) - ρP*gravity*yP)\n                    uP=UP/ρP\n                    vP=VP/ρP\n                    TP=PP/(R_gas*ρP)\n                elseif bc == 1\n                    UnM = nxM * UM +  nyM * VM\n                    UP = UM - 2 * UnM * nxM\n                    VP = VM - 2 * UnM * nyM\n                    ρP = ρM\n                    EP = EM\n                    PP = PM\n                    uP = UP/ρP\n                    vP = VP/ρP\n                    TP = TM\n                else\n                    error(\"Invalid boundary conditions $bc on face $f of element $e\")\n                end\n\n                #Left Fluxes\n                fluxρM = ρM\n                fluxUM = uM\n                fluxVM = vM\n                fluxEM = TM\n\n                #Right Fluxes\n                fluxρP = ρP\n                fluxUP = uP\n                fluxVP = vP\n                fluxEP = TP\n\n                #Compute Numerical Flux\n                fluxρS = 0.5*(fluxρM + fluxρP)\n                fluxUS = 0.5*(fluxUM + fluxUP)\n                fluxVS = 0.5*(fluxVM + fluxVP)\n                fluxES = 0.5*(fluxEM + fluxEP)\n\n                #Update RHS\n                rhs[vidM, _ρ, 1, eM] += sMJ * nxM*fluxρS\n                rhs[vidM, _ρ, 2, eM] += sMJ * nyM*fluxρS\n                rhs[vidM, _U, 1, eM] += sMJ * nxM*fluxUS\n                rhs[vidM, _U, 2, eM] += sMJ * nyM*fluxUS\n                rhs[vidM, _V, 1, eM] += sMJ * nxM*fluxVS\n                rhs[vidM, _V, 2, eM] += sMJ * nyM*fluxVS\n                rhs[vidM, _E, 1, eM] += sMJ * nxM*fluxES\n                rhs[vidM, _E, 2, eM] += sMJ * nyM*fluxES\n            end\n        end\n    end\nend}}}{{{ Volume div(grad(Q))function volume_div!(::Val{dim}, ::Val{N}, rhs::Array, gradQ, Q, vgeo, D, elems) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n    Pr::DFloat = _Prandtl\n    lambda::DFloat = _Stokes\n\n    Nq = N + 1\n    nelem = size(Q)[end]\n\n    Q = reshape(Q, Nq, Nq, _nstate, nelem)\n    gradQ = reshape(gradQ, Nq, Nq, _nstate, dim, nelem)\n    rhs = reshape(rhs, Nq, Nq, _nstate, dim, nelem)\n    vgeo = reshape(vgeo, Nq, Nq, _nvgeo, nelem)\n\n    #Initialize RHS vector\n    fill!( rhs, zero(rhs[1]))\n\n    #Allocate Arrays\n    s_F = Array{DFloat}(undef, Nq, Nq, _nstate)\n    s_G = Array{DFloat}(undef, Nq, Nq, _nstate)\n\n    @inbounds for e in elems\n        for j = 1:Nq, i = 1:Nq\n            MJ = vgeo[i, j, _MJ, e]\n            ξx, ξy = vgeo[i,j,_ξx,e], vgeo[i,j,_ξy,e]\n            ηx, ηy = vgeo[i,j,_ηx,e], vgeo[i,j,_ηy,e]\n            ρx, ρy = gradQ[i, j, _ρ, 1, e], gradQ[i, j, _ρ, 2, e]\n            Ux, Uy = gradQ[i, j, _U, 1, e], gradQ[i, j, _U, 2, e]\n            Vx, Vy = gradQ[i, j, _V, 1, e], gradQ[i, j, _V, 2, e]\n            Ex, Ey = gradQ[i, j, _E, 1, e], gradQ[i, j, _E, 2, e]\n            ρ, U, V = Q[i, j, _ρ, e], Q[i, j, _U, e], Q[i, j, _V, e]\n\n            #Compute primitive variables\n            ux, uy = Ux, Uy\n            vx, vy = Vx, Vy\n            Tx, Ty = Ex, Ey\n            div_u=ux + vy\n            u=U/ρ\n            v=V/ρ\n\n            #Compute fluxes\n            fluxρ_x = 0*ρx\n            fluxρ_y = 0*ρy\n            fluxU_x = 2*ux + lambda*div_u\n            fluxU_y = uy + vx\n            fluxV_x = vx + uy\n            fluxV_y = 2*vy + lambda*div_u\n            fluxE_x = u*(2*ux + lambda*div_u) + v*(uy + vx) + c_p/Pr*Tx\n            fluxE_y = u*(vx + uy) + v*(2*vy + lambda*div_u) + c_p/Pr*Ty\n\n            s_F[i, j, _ρ] = MJ * (ξx * fluxρ_x + ξy * fluxρ_y)\n            s_F[i, j, _U] = MJ * (ξx * fluxU_x + ξy * fluxU_y)\n            s_F[i, j, _V] = MJ * (ξx * fluxV_x + ξy * fluxV_y)\n            s_F[i, j, _E] = MJ * (ξx * fluxE_x + ξy * fluxE_y)\n\n            s_G[i, j, _ρ] = MJ * (ηx * fluxρ_x + ηy * fluxρ_y)\n            s_G[i, j, _U] = MJ * (ηx * fluxU_x + ηy * fluxU_y)\n            s_G[i, j, _V] = MJ * (ηx * fluxV_x + ηy * fluxV_y)\n            s_G[i, j, _E] = MJ * (ηx * fluxE_x + ηy * fluxE_y)\n        endloop of ξ-grid lines        for s = 1:_nstate, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, s, 1, e] -= D[n, i] * s_F[n, j, s]\n        endloop of η-grid lines        for s = 1:_nstate, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, s, 1, e] -= D[n, j] * s_G[i, n, s]\n        end\n    end\nend}}}Flux div(grad(Q))function flux_div!(::Val{dim}, ::Val{N}, rhs::Array,  gradQ, Q, sgeo, elems, vmapM, vmapP, elemtobndy) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n    Pr::DFloat = _Prandtl\n    lambda::DFloat = _Stokes\n\n    Np = (N+1)^dim\n    Nfp = (N+1)^(dim-1)\n    nface = 2*dim\n\n    @inbounds for e in elems\n        for f = 1:nface\n            for n = 1:Nfp\n                nxM, nyM, sMJ = sgeo[_nx, n, f, e], sgeo[_ny, n, f, e], sgeo[_sMJ, n, f, e]\n                idM, idP = vmapM[n, f, e], vmapP[n, f, e]\n\n                eM, eP = e, ((idP - 1) ÷ Np) + 1\n                vidM, vidP = ((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1\n\n                #Left variables\n                ρxM = gradQ[vidM, _ρ, 1, eM]\n                ρyM = gradQ[vidM, _ρ, 2, eM]\n                UxM = gradQ[vidM, _U, 1, eM]\n                UyM = gradQ[vidM, _U, 2, eM]\n                VxM = gradQ[vidM, _V, 1, eM]\n                VyM = gradQ[vidM, _V, 2, eM]\n                ExM = gradQ[vidM, _E, 1, eM]\n                EyM = gradQ[vidM, _E, 2, eM]\n                ρM = Q[vidM, _ρ, eM]\n                UM = Q[vidM, _U, eM]\n                VM = Q[vidM, _V, eM]\n\n                uM=UM/ρM\n                vM=VM/ρM\n                uxM, uyM = UxM, UyM\n                vxM, vyM = VxM, VyM\n                TxM, TyM = ExM, EyM\n\n                #Right variables\n                bc = elemtobndy[f, e]\n                ρxP = ρyP = UxP = UyP = VxP = VyP = ExP = EyP = zero(eltype(Q))\n                if bc == 0\n                    ρxP = gradQ[vidP, _ρ, 1, eP]\n                    ρyP = gradQ[vidP, _ρ, 2, eP]\n                    UxP = gradQ[vidP, _U, 1, eP]\n                    UyP = gradQ[vidP, _U, 2, eP]\n                    VxP = gradQ[vidP, _V, 1, eP]\n                    VyP = gradQ[vidP, _V, 2, eP]\n                    ExP = gradQ[vidP, _E, 1, eP]\n                    EyP = gradQ[vidP, _E, 2, eP]\n\n                    ρP = Q[vidP, _ρ, eP]\n                    UP = Q[vidP, _U, eP]\n                    VP = Q[vidP, _V, eP]\n                    uP=UP/ρP\n                    vP=VP/ρP\n                    uxP, uyP = UxP, UyP\n                    vxP, vyP = VxP, VyP\n                    TxP, TyP = ExP, EyP\n                elseif bc == 1\n                    ρnM = nxM * ρxM +  nyM * ρyM\n                    ρxP = ρxM - 2 * ρnM * nxM\n                    ρyP = ρyM - 2 * ρnM * nyM\n                    UnM = nxM * UxM +  nyM * UyM\n                    UxP = UxM - 2 * UnM * nxM\n                    UyP = UyM - 2 * UnM * nyM\n                    VnM = nxM * VxM +  nyM * VyM\n                    VxP = VxM - 2 * VnM * nxM\n                    VyP = VyM - 2 * VnM * nyM\n                    EnM = nxM * ExM +  nyM * EyM\n                    ExP = ExM - 2 * EnM * nxM\n                    EyP = EyM - 2 * EnM * nyM\n\n                    unM = nxM * uM +  nyM * vM\n                    uP = uM - 2 * unM * nxM\n                    vP = vM - 2 * unM * nyM\n                    uxP, uyP = UxP, UyP #FXG: Not sure about this BC\n                    vxP, vyP = VxP, VyP #FXG: Not sure about this BC\n                    #TxP, TyP = ExP, EyP #Produces thermal boundary layer\n                    TxP, TyP = TxM, TyM\n                else\n                    error(\"Invalid boundary conditions $bc on face $f of element $e\")\n                end\n\n                div_uM=uxM + vyM\n                #Left Fluxes\n                fluxρM_x = 0*ρxM\n                fluxρM_y = 0*ρyM\n                fluxUM_x = 2*uxM + lambda*div_uM\n                fluxUM_y = uyM + vxM\n                fluxVM_x = vxM + uyM\n                fluxVM_y = 2*vyM + lambda*div_uM\n                fluxEM_x = uM*(2*uxM + lambda*div_uM) + vM*(uyM + vxM) + c_p/Pr*TxM\n                fluxEM_y = uM*(vxM + uyM) + vM*(2*vyM + lambda*div_uM) + c_p/Pr*TyM\n\n                div_uP=uxP + vyP\n                #Right Fluxes\n                fluxρP_x = 0*ρxP\n                fluxρP_y = 0*ρyP\n                fluxUP_x = 2*uxP + lambda*div_uP\n                fluxUP_y = uyP + vxP\n                fluxVP_x = vxP + uyP\n                fluxVP_y = 2*vyP + lambda*div_uP\n                fluxEP_x = uP*(2*uxP + lambda*div_uP) + vP*(uyP + vxP) + c_p/Pr*TxP\n                fluxEP_y = uP*(vxP + uyP) + vP*(2*vyP + lambda*div_uP) + c_p/Pr*TyP\n\n                #Compute Numerical Flux\n                fluxρS = 0.5*(nxM * (fluxρM_x + fluxρP_x) + nyM * (fluxρM_y + fluxρP_y))\n                fluxUS = 0.5*(nxM * (fluxUM_x + fluxUP_x) + nyM * (fluxUM_y + fluxUP_y))\n                fluxVS = 0.5*(nxM * (fluxVM_x + fluxVP_x) + nyM * (fluxVM_y + fluxVP_y))\n                fluxES = 0.5*(nxM * (fluxEM_x + fluxEP_x) + nyM * (fluxEM_y + fluxEP_y))\n\n                #Update RHS\n                rhs[vidM, _ρ, 1, eM] += sMJ * fluxρS\n                rhs[vidM, _U, 1, eM] += sMJ * fluxUS\n                rhs[vidM, _V, 1, eM] += sMJ * fluxVS\n                rhs[vidM, _E, 1, eM] += sMJ * fluxES\n            end\n        end\n    end\nend}}}{{{ Update grad Q solutionfunction update_gradQ!(::Val{dim}, ::Val{N}, Q, rhs, vgeo, elems) where {dim, N}\n\n    Nq=(N+1)^dim\n\n    @inbounds for e = elems, s = 1:_nstate, i = 1:Nq\n        Q[i, s, 1, e] = rhs[i, s, 1, e] * vgeo[i, _MJI, e]\n        Q[i, s, 2, e] = rhs[i, s, 2, e] * vgeo[i, _MJI, e]\n    end\n\nend}}}{{{ Update grad Q solutionfunction update_divgradQ!(::Val{dim}, ::Val{N}, Q, rhs, vgeo, elems) where {dim, N}\n\n    Nq=(N+1)^dim\n\n    @inbounds for e = elems, s = 1:_nstate, i = 1:Nq\n        Q[i, s, e] = rhs[i, s, 1, e] * vgeo[i, _MJI, e]\n    end\n\nend}}}{{{ Store residual for dSGSfunction store_residual_dSGS!(::Val{dim}, ::Val{N}, rhs_dSGS::Array,  rhs, vgeo, elems) where {dim, N}\n\n    fill!( rhs_dSGS, zero(rhs_dSGS[1]))\n\n    @inbounds for e = elems, s = 1:_nstate, i = 1:(N+1)^dim\n        rhs_dSGS[i, s, e] += rhs[i,s,e] * vgeo[i, _MJI, e]\n    end\n\nend}}}{{{ Update solution (for all dimensions)function updatesolution!(::Val{dim}, ::Val{N}, rhs::Array,  rhs_gradQ, Q, vgeo, elems, rka, rkb, dt, visc) where {dim, N}\n\n    @inbounds for e = elems, s = 1:_nstate, i = 1:(N+1)^dim\n        rhs[i, s, e] += visc*rhs_gradQ[i,s,1,e]\n        Q[i, s, e] += rkb * dt * rhs[i, s, e] * vgeo[i, _MJI, e]\n        rhs[i, s, e] *= rka\n    end\n\nend}}}{{{ improved GPU kernles {{{ Volume RHS@hascuda function knl_volume_rhs!(::Val{dim}, ::Val{N}, rhs, Q, vgeo, D, nelem) where {dim, N}\n    DFloat = eltype(D)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Nq = N + 1\n\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    s_D = @cuStaticSharedMem(eltype(D), (Nq, Nq))\n    s_F = @cuStaticSharedMem(eltype(Q), (Nq, Nq, _nstate))\n    s_G = @cuStaticSharedMem(eltype(Q), (Nq, Nq, _nstate))\n\n    rhsU = rhsV = rhsρ = rhsE = zero(eltype(rhs))\n    if i <= Nq && j <= Nq && k == 1 && e <= nelemLoad derivative into shared memory        if k == 1\n            s_D[i, j] = D[i, j]\n        endLoad values will need into registers        MJ = vgeo[i, j, _MJ, e]\n        ξx, ξy = vgeo[i, j, _ξx, e], vgeo[i, j, _ξy, e]\n        ηx, ηy = vgeo[i, j, _ηx, e], vgeo[i, j, _ηy, e]\n        y = vgeo[i, j, _y, e]\n\n        U, V = Q[i, j, _U, e], Q[i, j, _V, e]\n        ρ, E = Q[i, j, _ρ, e], Q[i, j, _E, e]\n        P = (R_gas/c_v)*(E - (U*U + V*V)/(2*ρ) - ρ*gravity*y)\n        rhsU, rhsV = rhs[i, j, _U, e], rhs[i, j, _V, e]\n        rhsρ, rhsE = rhs[i, j, _ρ, e], rhs[i, j, _E, e]\n\n        ρinv = 1 / ρ\n        fluxρ_x = U\n        fluxU_x = ρinv * U * U + P\n        fluxV_x = ρinv * V * U\n        fluxE_x = ρinv * U * (E+P)\n\n        fluxρ_y = V\n        fluxU_y = ρinv * U * V\n        fluxV_y = ρinv * V * V + P\n        fluxE_y = ρinv * V * (E+P)\n\n        s_F[i, j, _ρ] = MJ * (ξx * fluxρ_x + ξy * fluxρ_y)\n        s_F[i, j, _U] = MJ * (ξx * fluxU_x + ξy * fluxU_y)\n        s_F[i, j, _V] = MJ * (ξx * fluxV_x + ξy * fluxV_y)\n        s_F[i, j, _E] = MJ * (ξx * fluxE_x + ξy * fluxE_y)\n\n        s_G[i, j, _ρ] = MJ * (ηx * fluxρ_x + ηy * fluxρ_y)\n        s_G[i, j, _U] = MJ * (ηx * fluxU_x + ηy * fluxU_y)\n        s_G[i, j, _V] = MJ * (ηx * fluxV_x + ηy * fluxV_y)\n        s_G[i, j, _E] = MJ * (ηx * fluxE_x + ηy * fluxE_y)buoyancy term        rhsV -= MJ * ρ * gravity\n    end\n\n    sync_threads()\n\n    @inbounds if i <= Nq && j <= Nq && k == 1 && e <= nelemloop of ξ-grid lines        for n = 1:Nq\n            Dni = s_D[n, i]\n            Dnj = s_D[n, j]\n\n            rhsρ += Dni * s_F[n, j, _ρ]\n            rhsρ += Dnj * s_G[i, n, _ρ]\n\n            rhsU += Dni * s_F[n, j, _U]\n            rhsU += Dnj * s_G[i, n, _U]\n\n            rhsV += Dni * s_F[n, j, _V]\n            rhsV += Dnj * s_G[i, n, _V]\n\n            rhsE += Dni * s_F[n, j, _E]\n            rhsE += Dnj * s_G[i, n, _E]\n        end\n\n        rhs[i, j, _U, e] = rhsU\n        rhs[i, j, _V, e] = rhsV\n        rhs[i, j, _ρ, e] = rhsρ\n        rhs[i, j, _E, e] = rhsE\n    end\n    nothing\nend}}}{{{ Flux RHS (all dimensions)@hascuda function knl_flux_rhs!(::Val{dim}, ::Val{N}, rhs, Q, sgeo, vgeo, nelem, vmapM,vmapP, elemtobndy) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1) * (N+1)\n    nface = 4\n\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    Nq = N+1\n    half = convert(eltype(Q), 0.5)\n\n    @inbounds if i <= Nq && j <= Nq && k == 1 && e <= nelem\n        n = i + (j-1) * Nq\n        for lf = 1:2:nface\n            for f = lf:lf+1\n                (nxM, nyM, sMJ) = (sgeo[_nx, n, f, e], sgeo[_ny, n, f, e], sgeo[_sMJ, n, f, e])\n\n                (idM, idP) = (vmapM[n, f, e], vmapP[n, f, e])\n\n                (eM, eP) = (e, ((idP - 1) ÷ Np) + 1)\n                (vidM, vidP) = (((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1)\n\n                #Left variables\n                ρM = Q[vidM, _ρ, eM]\n                UM = Q[vidM, _U, eM]\n                VM = Q[vidM, _V, eM]\n                EM = Q[vidM, _E, eM]\n                yM = vgeo[vidM, _y, eM]\n                PM = (R_gas/c_v)*(EM - (UM*UM + VM*VM)/(2*ρM) - ρM*gravity*yM)\n\n                #Right variables\n                bc = elemtobndy[f, e]\n                ρP = UP = VP = EP = PP = zero(eltype(Q))\n                if bc == 0\n                    ρP = Q[vidP, _ρ, eP]\n                    UP = Q[vidP, _U, eP]\n                    VP = Q[vidP, _V, eP]\n                    EP = Q[vidP, _E, eP]\n                    yP = vgeo[vidP, _y, eP]\n                    PP = (R_gas/c_v)*(EP - (UP*UP + VP*VP)/(2*ρP) - ρP*gravity*yP)\n                elseif bc == 1\n                    UnM = nxM * UM + nyM * VM\n                    UP = UM - 2 * UnM * nxM\n                    VP = VM - 2 * UnM * nyM\n                    ρP = ρM\n                    EP = EM\n                    PP = PM\n                end\n\n                #Left fluxes\n                ρMinv = 1 / ρM\n                fluxρM_x = UM\n                fluxUM_x = ρMinv * UM * UM + PM\n                fluxVM_x = ρMinv * VM * UM\n                fluxEM_x = ρMinv * UM * (EM+PM)\n\n                fluxρM_y = VM\n                fluxUM_y = ρMinv * UM * VM\n                fluxVM_y = ρMinv * VM * VM + PM\n                fluxEM_y = ρMinv * VM * (EM+PM)\n\n                #Right fluxes\n                ρPinv = 1 / ρP\n                fluxρP_x = UP\n                fluxUP_x = ρPinv * UP * UP + PP\n                fluxVP_x = ρPinv * VP * UP\n                fluxEP_x = ρPinv * UP * (EP+PP)\n\n                fluxρP_y = VP\n                fluxUP_y = ρPinv * UP * VP\n                fluxVP_y = ρPinv * VP * VP + PP\n                fluxEP_y = ρPinv * VP * (EP+PP)\n\n                #Compute Wave Speed\n                λM = ρMinv * abs(nxM * UM + nyM * VM) + CUDAnative.sqrt(ρMinv * γ * PM)\n                λP = ρPinv * abs(nxM * UP + nyM * VP) + CUDAnative.sqrt(ρPinv * γ * PP)\n                λ  =  max(λM, λP)\n\n                #Compute Numerical Flux\n                fluxρS = (nxM * (fluxρM_x + fluxρP_x) + nyM * (fluxρM_y + fluxρP_y) - λ * (ρP - ρM)) / 2\n                fluxUS = (nxM * (fluxUM_x + fluxUP_x) + nyM * (fluxUM_y + fluxUP_y) - λ * (UP - UM)) / 2\n                fluxVS = (nxM * (fluxVM_x + fluxVP_x) + nyM * (fluxVM_y + fluxVP_y) - λ * (VP - VM)) / 2\n                fluxES = (nxM * (fluxEM_x + fluxEP_x) + nyM * (fluxEM_y + fluxEP_y) - λ * (EP - EM)) / 2\n\n                #Update RHS\n                rhs[vidM, _ρ, eM] -= sMJ * fluxρS\n                rhs[vidM, _U, eM] -= sMJ * fluxUS\n                rhs[vidM, _V, eM] -= sMJ * fluxVS\n                rhs[vidM, _E, eM] -= sMJ * fluxES\n            end\n            sync_threads()\n        end\n    end\n    nothing\nend}}}{{{ Volume grad(Q)@hascuda function knl_volume_grad!(::Val{dim}, ::Val{N}, rhs, Q, vgeo, D, nelem) where {dim, N}\n    DFloat = eltype(D)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Nq = N + 1\n\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    s_D = @cuStaticSharedMem(eltype(D), (Nq, Nq))\n    s_F = @cuStaticSharedMem(eltype(Q), (Nq, Nq, _nstate, dim))\n    s_G = @cuStaticSharedMem(eltype(Q), (Nq, Nq, _nstate, dim))\n\n    rhsU = rhsV = rhsρ = rhsE = zero(eltype(rhs))\n    if i <= Nq && j <= Nq && k == 1 && e <= nelemLoad derivative into shared memory        if k == 1\n            s_D[i, j] = D[i, j]\n        endLoad values will need into registers        MJ = vgeo[i, j, _MJ, e]\n        ξx, ξy = vgeo[i, j, _ξx, e], vgeo[i, j, _ξy, e]\n        ηx, ηy = vgeo[i, j, _ηx, e], vgeo[i, j, _ηy, e]\n        y = vgeo[i, j, _y, e]\n\n        U, V = Q[i, j, _U, e], Q[i, j, _V, e]\n        ρ, E = Q[i, j, _ρ, e], Q[i, j, _E, e]\n        P = (R_gas/c_v)*(E - (U*U + V*V)/(2*ρ) - ρ*gravity*y)\n\n        #Prefetch\n        rhsρ1, rhsρ2 = rhs[i, j, _ρ, 1, e], rhs[i, j, _ρ, 2, e]\n        rhsU1, rhsU2 = rhs[i, j, _U, 1, e], rhs[i, j, _U, 2, e]\n        rhsV1, rhsV2 = rhs[i, j, _V, 1, e], rhs[i, j, _V, 2, e]\n        rhsE1, rhsE2 = rhs[i, j, _E, 1, e], rhs[i, j, _E, 2, e]\n\n        #Primitive variables\n        u=U/ρ\n        v=V/ρ\n        T=P/(R_gas*ρ)\n\n        #Compute fluxes\n        fluxρ = ρ\n        fluxU = u\n        fluxV = v\n        fluxE = T\n\n        s_F[i, j, _ρ, 1], s_F[i, j, _ρ, 2] = MJ * (ξx * fluxρ), MJ * (ξy * fluxρ)\n        s_F[i, j, _U, 1], s_F[i, j, _U, 2] = MJ * (ξx * fluxU), MJ * (ξy * fluxU)\n        s_F[i, j, _V, 1], s_F[i, j, _V, 2] = MJ * (ξx * fluxV), MJ * (ξy * fluxV)\n        s_F[i, j, _E, 1], s_F[i, j, _E, 2] = MJ * (ξx * fluxE), MJ * (ξy * fluxE)\n\n        s_G[i, j, _ρ, 1], s_G[i, j, _ρ, 2] = MJ * (ηx * fluxρ), MJ * (ηy * fluxρ)\n        s_G[i, j, _U, 1], s_G[i, j, _U, 2] = MJ * (ηx * fluxU), MJ * (ηy * fluxU)\n        s_G[i, j, _V, 1], s_G[i, j, _V, 2] = MJ * (ηx * fluxV), MJ * (ηy * fluxV)\n        s_G[i, j, _E, 1], s_G[i, j, _E, 2] = MJ * (ηx * fluxE), MJ * (ηy * fluxE)\n\n    end\n\n    sync_threads()\n\n    @inbounds if i <= Nq && j <= Nq && k == 1 && e <= nelemloop of ξ-grid lines        for n = 1:Nq\n            Dni = s_D[n, i]\n            Dnj = s_D[n, j]\n\n            rhsρ1 += Dni * s_F[n, j, 1, _ρ]\n            rhsρ1 += Dnj * s_G[i, n, 1, _ρ]\n            rhsρ2 += Dni * s_F[n, j, 2, _ρ]\n            rhsρ2 += Dnj * s_G[i, n, 2, _ρ]\n\n            rhsU1 += Dni * s_F[n, j, 1, _U]\n            rhsU1 += Dnj * s_G[i, n, 1, _U]\n            rhsU2 += Dni * s_F[n, j, 2, _U]\n            rhsU2 += Dnj * s_G[i, n, 2, _U]\n\n            rhsV1 += Dni * s_F[n, j, 1, _V]\n            rhsV1 += Dnj * s_G[i, n, 1, _V]\n            rhsV2 += Dni * s_F[n, j, 2, _V]\n            rhsV2 += Dnj * s_G[i, n, 2, _V]\n\n            rhsE1 += Dni * s_F[n, j, 1, _E]\n            rhsE1 += Dnj * s_G[i, n, 1, _E]\n            rhsE2 += Dni * s_F[n, j, 2, _E]\n            rhsE2 += Dnj * s_G[i, n, 2, _E]\n        end\n\n        rhs[i, j, _ρ, 1, e], rhs[i, j, _ρ, 2, e] = rhsρ1, rhsρ2\n        rhs[i, j, _U, 1, e], rhs[i, j, _U, 2, e] = rhsU1, rhsU2\n        rhs[i, j, _V, 1, e], rhs[i, j, _V, 2, e] = rhsV1, rhsV2\n        rhs[i, j, _E, 1, e], rhs[i, j, _E, 2, e] = rhsE1, rhsE2\n    end\n    nothing\nend}}}{{{ Flux grad(Q)@hascuda function knl_flux_grad!(::Val{dim}, ::Val{N}, rhs, Q, sgeo, vgeo, nelem, vmapM, vmapP, elemtobndy) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1) * (N+1)\n    nface = 4\n\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    Nq = N+1\n    half = convert(eltype(Q), 0.5)\n\n    @inbounds if i <= Nq && j <= Nq && k == 1 && e <= nelem\n        n = i + (j-1) * Nq\n        for lf = 1:2:nface\n            for f = lf:lf+1\n                (nxM, nyM, sMJ) = (sgeo[_nx, n, f, e], sgeo[_ny, n, f, e], sgeo[_sMJ, n, f, e])\n\n                (idM, idP) = (vmapM[n, f, e], vmapP[n, f, e])\n\n                (eM, eP) = (e, ((idP - 1) ÷ Np) + 1)\n                (vidM, vidP) = (((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1)\n\n                #Left variables\n                ρM = Q[vidM, _ρ, eM]\n                UM = Q[vidM, _U, eM]\n                VM = Q[vidM, _V, eM]\n                EM = Q[vidM, _E, eM]\n                yM = vgeo[vidM, _y, eM]\n                PM = (R_gas/c_v)*(EM - (UM*UM + VM*VM)/(2*ρM) - ρM*gravity*yM)\n                uM=UM/ρM\n                vM=VM/ρM\n                TM=PM/(R_gas*ρM)\n\n                #Right variables\n                bc = elemtobndy[f, e]\n                ρP = UP = VP = EP = PP = zero(eltype(Q))\n                if bc == 0\n                    ρP = Q[vidP, _ρ, eP]\n                    UP = Q[vidP, _U, eP]\n                    VP = Q[vidP, _V, eP]\n                    EP = Q[vidP, _E, eP]\n                    yP = vgeo[vidP, _y, eP]\n                    PP = (R_gas/c_v)*(EP - (UP*UP + VP*VP)/(2*ρP) - ρP*gravity*yP)\n                    uP=UP/ρP\n                    vP=VP/ρP\n                    TP=PP/(R_gas*ρP)\n                elseif bc == 1\n                    UnM = nxM * UM + nyM * VM\n                    UP = UM - 2 * UnM * nxM\n                    VP = VM - 2 * UnM * nyM\n                    ρP = ρM\n                    EP = EM\n                    PP = PM\n                    uP = UP/ρP\n                    vP = VP/ρP\n                    TP = TM\n                end\n\n                #Left Fluxes\n                fluxρM = ρM\n                fluxUM = uM\n                fluxVM = vM\n                fluxEM = TM\n\n                #Right Fluxes\n                fluxρP = ρP\n                fluxUP = uP\n                fluxVP = vP\n                fluxEP = TP\n\n                #Compute Numerical Flux\n                fluxρS = 0.5*(fluxρM + fluxρP)\n                fluxUS = 0.5*(fluxUM + fluxUP)\n                fluxVS = 0.5*(fluxVM + fluxVP)\n                fluxES = 0.5*(fluxEM + fluxEP)\n\n                #Update RHS\n                rhs[vidM, _ρ, 1, eM] += sMJ * nxM*fluxρS\n                rhs[vidM, _ρ, 2, eM] += sMJ * nyM*fluxρS\n                rhs[vidM, _U, 1, eM] += sMJ * nxM*fluxUS\n                rhs[vidM, _U, 2, eM] += sMJ * nyM*fluxUS\n                rhs[vidM, _V, 1, eM] += sMJ * nxM*fluxVS\n                rhs[vidM, _V, 2, eM] += sMJ * nyM*fluxVS\n                rhs[vidM, _E, 1, eM] += sMJ * nxM*fluxES\n                rhs[vidM, _E, 2, eM] += sMJ * nyM*fluxES\n            end\n            sync_threads()\n        end\n    end\n    nothing\nend{{{ Volume div(grad(Q))@hascuda function knl_volume_div!(::Val{dim}, ::Val{N}, rhs, gradQ, Q, vgeo, D, nelem) where {dim, N}\n    DFloat = eltype(D)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Nq = N + 1\n\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    s_D = @cuStaticSharedMem(eltype(D), (Nq, Nq))\n    s_F = @cuStaticSharedMem(eltype(Q), (Nq, Nq, _nstate))\n    s_G = @cuStaticSharedMem(eltype(Q), (Nq, Nq, _nstate))\n\n    rhsU = rhsV = rhsρ = rhsE = zero(eltype(rhs))\n    if i <= Nq && j <= Nq && k == 1 && e <= nelemLoad derivative into shared memory        if k == 1\n            s_D[i, j] = D[i, j]\n        endLoad values will need into registers        MJ = vgeo[i, j, _MJ, e]\n        ξx, ξy = vgeo[i, j, _ξx, e], vgeo[i, j, _ξy, e]\n        ηx, ηy = vgeo[i, j, _ηx, e], vgeo[i, j, _ηy, e]\n        ρx, ρy = gradQ[i, j, _ρ, 1, e], gradQ[i, j, _ρ, 2, e]\n        Ux, Uy = gradQ[i, j, _U, 1, e], gradQ[i, j, _U, 2, e]\n        Vx, Vy = gradQ[i, j, _V, 1, e], gradQ[i, j, _V, 2, e]\n        Ex, Ey = gradQ[i, j, _E, 1, e], gradQ[i, j, _E, 2, e]\n        ρ, U, V = Q[i, j, _ρ, e], Q[i, j, _U, e], Q[i, j, _V, e]\n        rhsU, rhsV = rhs[i, j, _U, 1, e], rhs[i, j, _V, 1, e]\n        rhsρ, rhsE = rhs[i, j, _ρ, 1, e], rhs[i, j, _E, 1, e]\n\n        #Compute primitive variables\n        ux, uy = Ux, Uy\n        vx, vy = Vx, Vy\n        Tx, Ty = Ex, Ey\n        div_u=ux + vy\n        u=U/ρ\n        v=V/ρ\n\n        #Compute fluxes\n        fluxρ_x = 0*ρx\n        fluxρ_y = 0*ρy\n        fluxU_x = 2*ux + lambda*div_u\n        fluxU_y = uy + vx\n        fluxV_x = vx + uy\n        fluxV_y = 2*vy + lambda*div_u\n        fluxE_x = u*(2*ux + lambda*div_u) + v*(uy + vx) + c_p/Pr*Tx\n        fluxE_y = u*(vx + uy) + v*(2*vy + lambda*div_u) + c_p/Pr*Ty\n\n        s_F[i, j, _ρ] = MJ * (ξx * fluxρ_x + ξy * fluxρ_y)\n        s_F[i, j, _U] = MJ * (ξx * fluxU_x + ξy * fluxU_y)\n        s_F[i, j, _V] = MJ * (ξx * fluxV_x + ξy * fluxV_y)\n        s_F[i, j, _E] = MJ * (ξx * fluxE_x + ξy * fluxE_y)\n\n        s_G[i, j, _ρ] = MJ * (ηx * fluxρ_x + ηy * fluxρ_y)\n        s_G[i, j, _U] = MJ * (ηx * fluxU_x + ηy * fluxU_y)\n        s_G[i, j, _V] = MJ * (ηx * fluxV_x + ηy * fluxV_y)\n        s_G[i, j, _E] = MJ * (ηx * fluxE_x + ηy * fluxE_y)\n    end\n\n    sync_threads()\n\n    @inbounds if i <= Nq && j <= Nq && k == 1 && e <= nelemloop of ξ-grid lines        for n = 1:Nq\n            Dni = s_D[n, i]\n            Dnj = s_D[n, j]\n\n            rhsρ += Dni * s_F[n, j, _ρ]\n            rhsρ += Dnj * s_G[i, n, _ρ]\n\n            rhsU += Dni * s_F[n, j, _U]\n            rhsU += Dnj * s_G[i, n, _U]\n\n            rhsV += Dni * s_F[n, j, _V]\n            rhsV += Dnj * s_G[i, n, _V]\n\n            rhsE += Dni * s_F[n, j, _E]\n            rhsE += Dnj * s_G[i, n, _E]\n        end\n\n        rhs[i, j, _U, 1, e] = rhsU\n        rhs[i, j, _V, 1, e] = rhsV\n        rhs[i, j, _ρ, 1, e] = rhsρ\n        rhs[i, j, _E, 1, e] = rhsE\n    end\n    nothing\nend}}}{{{ Flux grad(Q)@hascuda function knl_flux_grad!(::Val{dim}, ::Val{N}, rhs, gradQ, Q, sgeo, nelem, vmapM, vmapP, elemtobndy) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1) * (N+1)\n    nface = 4\n\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    Nq = N+1\n    half = convert(eltype(Q), 0.5)\n\n    @inbounds if i <= Nq && j <= Nq && k == 1 && e <= nelem\n        n = i + (j-1) * Nq\n        for lf = 1:2:nface\n            for f = lf:lf+1\n                (nxM, nyM, sMJ) = (sgeo[_nx, n, f, e], sgeo[_ny, n, f, e], sgeo[_sMJ, n, f, e])\n\n                (idM, idP) = (vmapM[n, f, e], vmapP[n, f, e])\n\n                (eM, eP) = (e, ((idP - 1) ÷ Np) + 1)\n                (vidM, vidP) = (((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1)\n\n                #Left variables\n                ρxM = gradQ[vidM, _ρ, 1, eM]\n                ρyM = gradQ[vidM, _ρ, 2, eM]\n                UxM = gradQ[vidM, _U, 1, eM]\n                UyM = gradQ[vidM, _U, 2, eM]\n                VxM = gradQ[vidM, _V, 1, eM]\n                VyM = gradQ[vidM, _V, 2, eM]\n                ExM = gradQ[vidM, _E, 1, eM]\n                EyM = gradQ[vidM, _E, 2, eM]\n                ρM = Q[vidM, _ρ, eM]\n                UM = Q[vidM, _U, eM]\n                VM = Q[vidM, _V, eM]\n\n                uM=UM/ρM\n                vM=VM/ρM\n                uxM, uyM = UxM, UyM\n                vxM, vyM = VxM, VyM\n                TxM, TyM = ExM, EyM\n\n                #Right variables\n                bc = elemtobndy[f, e]\n                ρxP = ρyP = UxP = UyP = VxP = VyP = ExP = EyP = zero(eltype(Q))\n                if bc == 0\n                    ρxP = gradQ[vidP, _ρ, 1, eP]\n                    ρyP = gradQ[vidP, _ρ, 2, eP]\n                    UxP = gradQ[vidP, _U, 1, eP]\n                    UyP = gradQ[vidP, _U, 2, eP]\n                    VxP = gradQ[vidP, _V, 1, eP]\n                    VyP = gradQ[vidP, _V, 2, eP]\n                    ExP = gradQ[vidP, _E, 1, eP]\n                    EyP = gradQ[vidP, _E, 2, eP]\n\n                    ρP = Q[vidP, _ρ, eP]\n                    UP = Q[vidP, _U, eP]\n                    VP = Q[vidP, _V, eP]\n                    uP=UP/ρP\n                    vP=VP/ρP\n                    uxP, uyP = UxP, UyP\n                    vxP, vyP = VxP, VyP\n                    TxP, TyP = ExP, EyP\n                elseif bc == 1\n                    ρnM = nxM * ρxM +  nyM * ρyM\n                    ρxP = ρxM - 2 * ρnM * nxM\n                    ρyP = ρyM - 2 * ρnM * nyM\n                    UnM = nxM * UxM +  nyM * UyM\n                    UxP = UxM - 2 * UnM * nxM\n                    UyP = UyM - 2 * UnM * nyM\n                    VnM = nxM * VxM +  nyM * VyM\n                    VxP = VxM - 2 * VnM * nxM\n                    VyP = VyM - 2 * VnM * nyM\n                    EnM = nxM * ExM +  nyM * EyM\n                    ExP = ExM - 2 * EnM * nxM\n                    EyP = EyM - 2 * EnM * nyM\n\n                    unM = nxM * uM +  nyM * vM\n                    uP = uM - 2 * unM * nxM\n                    vP = vM - 2 * unM * nyM\n                    uxP, uyP = UxP, UyP #FXG: Not sure about this BC\n                    vxP, vyP = VxP, VyP #FXG: Not sure about this BC\n                    #TxP, TyP = ExP, EyP #Produces thermal boundary layer\n                    TxP, TyP = TxM, TyM\n                end\n\n                div_uM=uxM + vyM\n                #Left Fluxes\n                fluxρM_x = 0*ρxM\n                fluxρM_y = 0*ρyM\n                fluxUM_x = 2*uxM + lambda*div_uM\n                fluxUM_y = uyM + vxM\n                fluxVM_x = vxM + uyM\n                fluxVM_y = 2*vyM + lambda*div_uM\n                fluxEM_x = uM*(2*uxM + lambda*div_uM) + vM*(uyM + vxM) + c_p/Pr*TxM\n                fluxEM_y = uM*(vxM + uyM) + vM*(2*vyM + lambda*div_uM) + c_p/Pr*TyM\n\n                div_uP=uxP + vyP\n                #Right Fluxes\n                fluxρP_x = 0*ρxP\n                fluxρP_y = 0*ρyP\n                fluxUP_x = 2*uxP + lambda*div_uP\n                fluxUP_y = uyP + vxP\n                fluxVP_x = vxP + uyP\n                fluxVP_y = 2*vyP + lambda*div_uP\n                fluxEP_x = uP*(2*uxP + lambda*div_uP) + vP*(uyP + vxP) + c_p/Pr*TxP\n                fluxEP_y = uP*(vxP + uyP) + vP*(2*vyP + lambda*div_uP) + c_p/Pr*TyP\n\n                #Compute Numerical Flux\n                fluxρS = 0.5*(nxM * (fluxρM_x + fluxρP_x) + nyM * (fluxρM_y + fluxρP_y))\n                fluxUS = 0.5*(nxM * (fluxUM_x + fluxUP_x) + nyM * (fluxUM_y + fluxUP_y))\n                fluxVS = 0.5*(nxM * (fluxVM_x + fluxVP_x) + nyM * (fluxVM_y + fluxVP_y))\n                fluxES = 0.5*(nxM * (fluxEM_x + fluxEP_x) + nyM * (fluxEM_y + fluxEP_y))\n\n                #Update RHS\n                rhs[vidM, _ρ, 1, eM] += sMJ * fluxρS\n                rhs[vidM, _U, 1, eM] += sMJ * fluxUS\n                rhs[vidM, _V, 1, eM] += sMJ * fluxVS\n                rhs[vidM, _E, 1, eM] += sMJ * fluxES\n            end\n            sync_threads()\n        end\n    end\n    nothing\nend}}}{{{ Update solution (for all dimensions)@hascuda function knl_updatesolution!(::Val{dim}, ::Val{N}, rhs, Q, vgeo, nelem, rka,\n                                      rkb, dt) where {dim, N}\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    Nq = N+1\n    @inbounds if i <= Nq && j <= Nq && k <= Nq && e <= nelem\n        n = i + (j-1) * Nq + (k-1) * Nq * Nq\n        MJI = vgeo[n, _MJI, e]\n        for s = 1:_nstate\n            Q[n, s, e] += rkb * dt * rhs[n, s, e] * MJI\n            rhs[n, s, e] *= rka\n        end\n    end\n    nothing\nend}}}}}}{{{ Fill sendQ on device with Q (for all dimensions)@hascuda function knl_fillsendQ!(::Val{dim}, ::Val{N}, sendQ, Q,\n                                 sendelems) where {N, dim}\n    Nq = N + 1\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    @inbounds if i <= Nq && j <= Nq && k <= Nq && e <= length(sendelems)\n        n = i + (j-1) * Nq + (k-1) * Nq * Nq\n        re = sendelems[e]\n        for s = 1:_nstate\n            sendQ[n, s, e] = Q[n, s, re]\n        end\n    end\n    nothing\nend}}}{{{ Fill Q on device with recvQ (for all dimensions)@hascuda function knl_transferrecvQ!(::Val{dim}, ::Val{N}, Q, recvQ, nelem,\n                                     nrealelem) where {N, dim}\n    Nq = N + 1\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    @inbounds if i <= Nq && j <= Nq && k <= Nq && e <= nelem\n        n = i + (j-1) * Nq + (k-1) * Nq * Nq\n        for s = 1:_nstate\n            Q[n, s, nrealelem + e] = recvQ[n, s, e]\n        end\n    end\n    nothing\nend}}}{{{ MPI Buffer handlingfunction fillsendQ!(::Val{dim}, ::Val{N}, sendQ, d_sendQ::Array, Q,\n                    sendelems) where {dim, N}\n    sendQ[:, :, :] .= Q[:, :, sendelems]\nend\n\n@hascuda function fillsendQ!(::Val{dim}, ::Val{N}, sendQ, d_sendQ::CuArray,\n                             d_QL, d_sendelems) where {dim, N}\n    nsendelem = length(d_sendelems)\n    if nsendelem > 0\n        @cuda(threads=ntuple(j->N+1, dim), blocks=nsendelem,\n              knl_fillsendQ!(Val(dim), Val(N), d_sendQ, d_QL, d_sendelems))\n        sendQ .= d_sendQ\n    end\nend\n\n@hascuda function transferrecvQ!(::Val{dim}, ::Val{N}, d_recvQ::CuArray, recvQ,\n                                 d_QL, nrealelem) where {dim, N}\n    nrecvelem = size(recvQ)[end]\n    if nrecvelem > 0\n        d_recvQ .= recvQ\n        @cuda(threads=ntuple(j->N+1, dim), blocks=nrecvelem,\n              knl_transferrecvQ!(Val(dim), Val(N), d_QL, d_recvQ, nrecvelem,\n                                 nrealelem))\n    end\nend\n\nfunction transferrecvQ!(::Val{dim}, ::Val{N}, d_recvQ::Array, recvQ, Q,\n                        nrealelem) where {dim, N}\n    Q[:, :, nrealelem+1:end] .= recvQ[:, :, :]\nend}}}{{{ GPU kernel wrappers@hascuda function volume_rhs!(::Val{dim}, ::Val{N}, d_rhsL::CuArray, d_QL,\n                              d_vgeoL, d_D, elems) where {dim, N}\n    #Constants\n    Nq = N+1\n    nelem = length(elems)\n\n    #Reshape arrays\n    d_rhsC = reshape(d_rhsL, Nq, Nq, _nstate, nelem)\n    d_QC = reshape(d_QL, Nq, Nq, _nstate, nelem)\n    d_vgeoC = reshape(d_vgeoL, Nq, Nq, _vgeo, nelem)\n\n    @cuda(threads=ntuple(j->N+1, dim), blocks=nelem,\n          knl_volume_rhs!(Val(dim), Val(N), d_rhsC, d_QC, d_vgeoC, d_D, nelem))\nend\n\n@hascuda function flux_rhs!(::Val{dim}, ::Val{N}, d_rhsL::CuArray, d_QL, d_sgeo,\n                           d_vgeoL, elems, d_vmapM, d_vmapP, d_elemtobndy) where {dim, N}\n    nelem = length(elems)\n    @cuda(threads=(ntuple(j->N+1, dim-1)..., 1), blocks=nelem,\n          knl_flux_rhs!(Val(dim), Val(N), d_rhsL, d_QL, d_sgeo, d_vgeoL, nelem, d_vmapM, d_vmapP, d_elemtobndy))\nend\n\n@hascuda function volume_grad!(::Val{dim}, ::Val{N}, d_rhs_gradQL::CuArray, d_QL, d_vgeoL, d_D, elems) where {dim, N}\n\n    #Constants\n    Nq = N+1\n    nelem = length(elems)\n\n    #Reshape arrays\n    d_rhs_gradQC = reshape(d_rhs_gradQL, Nq, Nq, _nstate, dim, nelem)\n    d_QC = reshape(d_QL, Nq, Nq, _nstate, nelem)\n    d_vgeoC = reshape(d_vgeoL, Nq, Nq, _vgeo, nelem)\n\n    @cuda(threads=ntuple(j->N+1, dim), blocks=nelem,\n          knl_volume_grad!(Val(dim), Val(N), d_rhsC, d_QC, d_vgeoC, d_D, nelem))\nend\n\n@hascuda function flux_grad!(::Val{dim}, ::Val{N}, d_rhsL::CuArray, d_QL, d_sgeo,\n                           d_vgeoL, elems, d_vmapM, d_vmapP, d_elemtobndy) where {dim, N}\n    nelem = length(elems)\n    @cuda(threads=(ntuple(j->N+1, dim-1)..., 1), blocks=nelem,\n          knl_flux_grad!(Val(dim), Val(N), d_rhsL, d_QL, d_sgeo, d_vgeoL, nelem, d_vmapM, d_vmapP, d_elemtobndy))\nend\n\n@hascuda function volume_div!(::Val{dim}, ::Val{N}, d_rhs_gradQL::CuArray, d_gradQL, QL, d_vgeoL, d_D, elems) where {dim, N}\n\n    #Constants\n    Nq = N+1\n    nelem = length(elems)\n\n    #Reshape arrays\n    d_rhs_gradQC = reshape(d_rhs_gradQL, Nq, Nq, _nstate, dim, nelem)\n    d_gradQC = reshape(d_gradQL, Nq, Nq, _nstate, dim, nelem)\n    QC = reshape(QL, Nq, Nq, _nstate, nelem)\n    d_vgeoC = reshape(d_vgeoL, Nq, Nq, _vgeo, nelem)\n\n    @cuda(threads=ntuple(j->N+1, dim), blocks=nelem,\n          knl_volume_div!(Val(dim), Val(N), d_rhs_gradQC, d_gradQC, QC, d_vgeoC, d_D, nelem))\nend\n\n@hascuda function flux_div!(::Val{dim}, ::Val{N}, d_rhsL::CuArray, d_gradQL, d_QL, d_sgeo, elems, d_vmapM, d_vmapP, d_elemtobndy) where {dim, N}\n\n    #Constants\n    nelem = length(elems)\n\n    @cuda(threads=(ntuple(j->N+1, dim-1)..., 1), blocks=nelem,\n          knl_flux_div!(Val(dim), Val(N), d_rhsL, d_gradQL, d_QL, d_sgeo, nelem, d_vmapM, d_vmapP, d_elemtobndy))\nend\n\n@hascuda function updatesolution!(::Val{dim}, ::Val{N}, d_rhsL::CuArray, d_QL,\n                                  d_vgeoL, elems, rka, rkb, dt) where {dim, N}\n    nelem = length(elems)\n    @cuda(threads=ntuple(j->N+1, dim), blocks=nelem,\n          knl_updatesolution!(Val(dim), Val(N), d_rhsL, d_QL, d_vgeoL, nelem, rka,\n                              rkb, dt))\nend}}}{{{ L2 Energy (for all dimensions)function L2energysquared(::Val{dim}, ::Val{N}, Q, vgeo, elems) where {dim, N}\n    DFloat = eltype(Q)\n    Np = (N+1)^dim\n    (~, nstate, nelem) = size(Q)\n\n    energy = zero(DFloat)\n\n    @inbounds for e = elems, q = 1:nstate, i = 1:Np\n        energy += vgeo[i, _MJ, e] * Q[i, q, e]^2\n    end\n\n    energy\nend}}}{{{ Send Data Qfunction senddata_Q(::Val{dim}, ::Val{N}, mesh, sendreq, recvreq, sendQ,\n                  recvQ, d_sendelems, d_sendQ, d_recvQ, d_QL, mpicomm;\n                  ArrType=ArrType) where {dim, N}\n    mpirank = MPI.Comm_rank(mpicomm)Create send and recv request array    nnabr = length(mesh.nabrtorank)\n    d_sendelems = ArrType(mesh.sendelems)\n    nrealelem = length(mesh.realelems)post MPI receives    for n = 1:nnabr\n        recvreq[n] = MPI.Irecv!((@view recvQ[:, :, mesh.nabrtorecv[n]]),\n                                mesh.nabrtorank[n], 777, mpicomm)\n    endwait on (prior) MPI sends    MPI.Waitall!(sendreq)pack data from dQL into send buffer    fillsendQ!(Val(dim), Val(N), sendQ, dsendQ, dQL, dsendelems)    sendQ[:, :, :] .= d_QL[:, :, d_sendelems]post MPI sends    for n = 1:nnabr\n        sendreq[n] = MPI.Isend((@view sendQ[:, :, mesh.nabrtosend[n]]),\n                               mesh.nabrtorank[n], 777, mpicomm)\n    end\nend}}}{{{ Send Data Grad(Q)function senddata_gradQ(::Val{dim}, ::Val{N}, mesh, sendreq, recvreq, sendQ,\n                  recvQ, d_sendelems, d_sendQ, d_recvQ, d_QL, mpicomm;\n                  ArrType=ArrType) where {dim, N}\n    mpirank = MPI.Comm_rank(mpicomm)Create send and recv request array    nnabr = length(mesh.nabrtorank)\n    d_sendelems = ArrType(mesh.sendelems)\n    nrealelem = length(mesh.realelems)post MPI receives    for n = 1:nnabr\n        recvreq[n] = MPI.Irecv!((@view recvQ[:, :, :, mesh.nabrtorecv[n]]),\n                                mesh.nabrtorank[n], 777, mpicomm)\n    endwait on (prior) MPI sends    MPI.Waitall!(sendreq)pack data from dQL into send buffer    fillsendQ!(Val(dim), Val(N), sendQ, dsendQ, dQL, dsendelems)    sendQ[:, :, :, :] .= d_QL[:, :, :, d_sendelems]post MPI sends    for n = 1:nnabr\n        sendreq[n] = MPI.Isend((@view sendQ[:, :, :, mesh.nabrtosend[n]]),\n                               mesh.nabrtorank[n], 777, mpicomm)\n    end\nend}}}{{{ Receive Data Qfunction receivedata_Q!(::Val{dim}, ::Val{N}, mesh, recvreq, recvQ,\n                        d_recvQ, d_QL) where {dim, N}\n    nrealelem = length(mesh.realelems)wait on MPI receives    MPI.Waitall!(recvreq)copy data to state vector d_QL    #transferrecvQ!(Val(dim), Val(N), d_recvQ, recvQ, d_QL, nrealelem)\n    d_QL[:, :, nrealelem+1:end] .= recvQ[:, :, :]\n\nend}}}{{{ Receive Data Grad(Q)function receivedata_gradQ!(::Val{dim}, ::Val{N}, mesh, recvreq, recvQ,\n                            d_recvQ, d_QL) where {dim, N}\n    nrealelem = length(mesh.realelems)wait on MPI receives    MPI.Waitall!(recvreq)copy data to state vector d_QL    #transferrecvQ!(Val(dim), Val(N), d_recvQ, recvQ, d_QL, nrealelem)\n    d_QL[:, :, :, nrealelem+1:end] .= recvQ[:, :, :, :]\nend}}}{{{ RK loopfunction lowstorageRK(::Val{dim}, ::Val{N}, mesh, vgeo, sgeo, Q, rhs, D,\n                      dt, nsteps, tout, vmapM, vmapP, mpicomm, iplot, visc;\n                      ArrType=ArrType, plotstep=0) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    mpirank = MPI.Comm_rank(mpicomm)Fourth-order, low-storage, Runge–Kutta scheme of Carpenter and Kennedy (1994) ((5,4) 2N-Storage RK scheme.Ref: @TECHREPORT{CarpenterKennedy1994,   author = {M.~H. Carpenter and C.~A. Kennedy},   title = {Fourth-order {2N-storage} {Runge-Kutta} schemes},   institution = {National Aeronautics and Space Administration},   year = {1994},   number = {NASA TM-109112},   address = {Langley Research Center, Hampton, VA}, }    RKA = (DFloat(0),\n           DFloat(-567301805773)  / DFloat(1357537059087),\n           DFloat(-2404267990393) / DFloat(2016746695238),\n           DFloat(-3550918686646) / DFloat(2091501179385),\n           DFloat(-1275806237668) / DFloat(842570457699 ))\n\n    RKB = (DFloat(1432997174477) / DFloat(9575080441755 ),\n           DFloat(5161836677717) / DFloat(13612068292357),\n           DFloat(1720146321549) / DFloat(2090206949498 ),\n           DFloat(3134564353537) / DFloat(4481467310338 ),\n           DFloat(2277821191437) / DFloat(14882151754819))\n\n    RKC = (DFloat(0),\n           DFloat(1432997174477) / DFloat(9575080441755),\n           DFloat(2526269341429) / DFloat(6820363962896),\n           DFloat(2006345519317) / DFloat(3224310063776),\n           DFloat(2802321613138) / DFloat(2924317926251))Create send and recv request array    nnabr = length(mesh.nabrtorank)\n    sendreq = fill(MPI.REQUEST_NULL, nnabr)\n    recvreq = fill(MPI.REQUEST_NULL, nnabr)Create send and recv buffer    sendQ = zeros(DFloat, (N+1)^dim, size(Q,2), length(mesh.sendelems))\n    recvQ = zeros(DFloat, (N+1)^dim, size(Q,2), length(mesh.ghostelems))Create send and recv LDG buffer    sendgradQ = zeros(DFloat, (N+1)^dim, size(Q,2), dim, length(mesh.sendelems))\n    recvgradQ = zeros(DFloat, (N+1)^dim, size(Q,2), dim, length(mesh.ghostelems))Store Constants    nrealelem = length(mesh.realelems)\n    nsendelem = length(mesh.sendelems)\n    nrecvelem = length(mesh.ghostelems)\n    nelem = length(mesh.elems)\n\n    #Create Device Arrays\n    d_QL, d_rhsL = ArrType(Q), ArrType(rhs)\n    d_vgeoL, d_sgeo = ArrType(vgeo), ArrType(sgeo)\n    d_vmapM, d_vmapP = ArrType(vmapM), ArrType(vmapP)\n    d_sendelems, d_elemtobndy = ArrType(mesh.sendelems), ArrType(mesh.elemtobndy)\n    d_sendQ, d_recvQ = ArrType(sendQ), ArrType(recvQ)\n    d_D = ArrType(D)\n    #Create Device LDG Arrays\n    d_gradQL = zeros(DFloat, (N+1)^dim, _nstate, dim, nelem)\n    d_rhs_gradQL = zeros(DFloat, (N+1)^dim, _nstate, dim, nelem)\n    d_sendgradQ, d_recvgradQ = ArrType(sendgradQ), ArrType(recvgradQ)\n\n    #Template Reshape ArraysQshape    = (fill(N+1, dim)..., size(Q, 2), size(Q, 3))    vgeoshape = (fill(N+1, dim)..., nvgeo, size(Q, 3))    gradQshape = (fill(N+1, dim)..., size(dgradQL,2), size(dgradQL,3), size(dgradQL,4))    start_time = t1 = time_ns()\n    for step = 1:nsteps\n        for s = 1:length(RKA)Send Data Q            senddata_Q(Val(dim), Val(N), mesh, sendreq, recvreq, sendQ,\n                       recvQ, d_sendelems, d_sendQ, d_recvQ, d_QL, mpicomm;\n                       ArrType=ArrType)volume RHS computation            volume_rhs!(Val(dim), Val(N), d_rhsL, d_QL, d_vgeoL, d_D, mesh.realelems)Receive Data Q            receivedata_Q!(Val(dim), Val(N), mesh, recvreq, recvQ, d_recvQ, d_QL)flux RHS computation            flux_rhs!(Val(dim), Val(N), d_rhsL, d_QL, d_sgeo, d_vgeoL, mesh.realelems, d_vmapM, d_vmapP, d_elemtobndy)            if (visc > 0)volume grad Q computation                volume_grad!(Val(dim), Val(N), d_rhs_gradQL, d_QL, d_vgeoL, d_D, mesh.realelems)flux grad Q computation                flux_grad!(Val(dim), Val(N), d_rhs_gradQL, d_QL, d_sgeo, d_vgeoL, mesh.realelems, d_vmapM, d_vmapP, d_elemtobndy)Construct grad Q                update_gradQ!(Val(dim), Val(N), d_gradQL, d_rhs_gradQL, d_vgeoL, mesh.realelems)Send Data grad(Q)                senddata_gradQ(Val(dim), Val(N), mesh, sendreq, recvreq, sendgradQ,\n                               recvgradQ, d_sendelems, d_sendgradQ, d_recvgradQ,\n                               d_gradQL, mpicomm;ArrType=ArrType)volume div(grad Q) computation                volume_div!(Val(dim), Val(N), d_rhs_gradQL, d_gradQL, d_QL, d_vgeoL, d_D, mesh.realelems)Receive Data grad(Q)                receivedata_gradQ!(Val(dim), Val(N), mesh, recvreq, recvgradQ, d_recvgradQ, d_gradQL)flux div(grad Q) computation                flux_div!(Val(dim), Val(N), d_rhs_gradQL, d_gradQL, d_QL, d_sgeo, mesh.realelems, d_vmapM, d_vmapP, d_elemtobndy)\n            endupdate solution and scale RHS            updatesolution!(Val(dim), Val(N), d_rhsL, d_rhs_gradQL, d_QL, d_vgeoL, mesh.realelems,\n                            RKA[s%length(RKA)+1], RKB[s], dt, visc)\n        end\n\n        if step == 1\n            @hascuda synchronize()\n            start_time = time_ns()\n        end\n        if mpirank == 0 && (time_ns() - t1)*1e-9 > tout\n            @hascuda synchronize()\n            t1 = time_ns()\n            avg_stage_time = (time_ns() - start_time) * 1e-9 / ((step-1) * length(RKA))\n            @show (step, nsteps, avg_stage_time)\n        endWrite VTK file        if mod(step,iplot) == 0\n            Q .= d_QL\n            convert_set3c_to_set2nc(Val(dim), Val(N), vgeo, Q)\n            X = ntuple(j->reshape((@view vgeo[:, _x+j-1, :]), ntuple(j->N+1,dim)...,\n                                  nelem), dim)\n            ρ = reshape((@view Q[:, _ρ, :]), ntuple(j->(N+1),dim)..., nelem)\n            U = reshape((@view Q[:, _U, :]), ntuple(j->(N+1),dim)..., nelem)\n            V = reshape((@view Q[:, _V, :]), ntuple(j->(N+1),dim)..., nelem)\n            E = reshape((@view Q[:, _E, :]), ntuple(j->(N+1),dim)..., nelem)\n            E = E .- 300.0\n            writemesh(@sprintf(\"viz/nse%dD_set3c_%s_rank_%04d_step_%05d\",\n                               dim, ArrType, mpirank, step), X...;\n                      fields=((\"ρ\", ρ), (\"U\", U), (\"V\", V), (\"E\", E)),\n                      realelems=mesh.realelems)\n        end\n    end\nif (mpirank == 0)\n    avg_stage_time = (time_ns() - start_time) * 1e-9 / ((nsteps-1) * length(RKA))\n    @show (nsteps, avg_stage_time)\nend\nQ .= d_QL\nrhs .= d_rhsL\nend}}}{{{ convert_variablesfunction convert_set2nc_to_set2c(::Val{dim}, ::Val{N}, vgeo, Q) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1)^dim\n    (~, ~, nelem) = size(Q)\n\n    println(\"[CPU] converting variables (CPU)...\")\n    @inbounds for e = 1:nelem, n = 1:Np\n        ρ, u, v, E = Q[n, _ρ, e], Q[n, _U, e], Q[n, _V, e], Q[n, _E, e]\n        Q[n, _U, e] = ρ*u\n        Q[n, _V, e] = ρ*v\n        Q[n, _E, e] = ρ*E\n    end\nend}}}function convert_set2nc_to_set3c(::Val{dim}, ::Val{N}, vgeo, Q) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1)^dim\n    (~, ~, nelem) = size(Q)\n\n    @inbounds for e = 1:nelem, n = 1:Np\n        ρ, u, v, E = Q[n, _ρ, e], Q[n, _U, e], Q[n, _V, e], Q[n, _E, e]\n        y = vgeo[n, _y, e]\n        P = p0 * (ρ * R_gas * E / p0)^(c_p / c_v)\n        T = P/(ρ*R_gas)\n        E = c_v*T + 0.5*(u^2 + v^2) + gravity*y\n        Q[n, _U, e] = ρ*u\n        Q[n, _V, e] = ρ*v\n        Q[n, _E, e] = ρ*E\n    end\nend}}}function convert_set3c_to_set2nc(::Val{dim}, ::Val{N}, vgeo, Q) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1)^dim\n    (~, ~, nelem) = size(Q)\n\n    @inbounds for e = 1:nelem, n = 1:Np\n        ρ, U, V, E = Q[n, _ρ, e], Q[n, _U, e], Q[n, _V, e], Q[n, _E, e]\n        y = vgeo[n, _y, e]\n        u=U/ρ\n        v=V/ρ\n        E=E/ρ\n        P = (R_gas/c_v)*ρ*(E - 0.5*(u^2 + v^2) - gravity*y)\n        E=p0/(ρ * R_gas)*( P/p0 )^(c_v/c_p)\n        Q[n, _U, e] = u\n        Q[n, _V, e] = v\n        Q[n, _E, e] = E\n    end\nend}}}{{{ nse driverfunction nse(::Val{dim}, ::Val{N}, mpicomm, ic, mesh, tend, iplot, visc;\n               meshwarp=(x...)->identity(x),\n               tout = 1, ArrType=Array, plotstep=0) where {dim, N}\n    DFloat = typeof(tend)\n\n    mpirank = MPI.Comm_rank(mpicomm)\n    mpisize = MPI.Comm_size(mpicomm)Partion the mesh using a Hilbert curve based partitioning    mpirank == 0 && println(\"[CPU] partitioning mesh...\")\n    mesh = partition(mpicomm, mesh...)Connect the mesh in parallel    mpirank == 0 && println(\"[CPU] connecting mesh...\")\n    mesh = connectmesh(mpicomm, mesh...)Get the vmaps    mpirank == 0 && println(\"[CPU] computing mappings...\")\n    (vmapM, vmapP) = mappings(N, mesh.elemtoelem, mesh.elemtoface,\n                              mesh.elemtoordr)Create 1-D operators    (ξ, ω) = lglpoints(DFloat, N)\n    D = spectralderivative(ξ)Compute the geometry    mpirank == 0 && println(\"[CPU] computing metrics...\")\n    (vgeo, sgeo) = computegeometry(Val(dim), mesh, D, ξ, ω, meshwarp, vmapM)\n    (nface, nelem) = size(mesh.elemtoelem)Storage for the solution, rhs, and error    mpirank == 0 && println(\"[CPU] creating fields (CPU)...\")\n    Q = zeros(DFloat, (N+1)^dim, _nstate, nelem)\n    rhs = zeros(DFloat, (N+1)^dim, _nstate, nelem)setup the initial condition    mpirank == 0 && println(\"[CPU] computing initial conditions (CPU)...\")\n    @inbounds for e = 1:nelem, i = 1:(N+1)^dim\n        x, y = vgeo[i, _x, e], vgeo[i, _y, e]\n        ρ, U, V, E = ic(x, y)\n        Q[i, _ρ, e] = ρ\n        Q[i, _U, e] = U\n        Q[i, _V, e] = V\n        Q[i, _E, e] = E\n    endConvert to proper variables    mpirank == 0 && println(\"[CPU] converting variables (CPU)...\")\n    convert_set2nc_to_set3c(Val(dim), Val(N), vgeo, Q)Compute time step    mpirank == 0 && println(\"[CPU] computing dt (CPU)...\")\n    #base_dt = cfl(Val(dim), Val(N), vgeo, Q, mpicomm) / N^√2\n    (base_dt, Courant) = courantnumber(Val(dim), Val(N), vgeo, Q, mpicomm)\n    #base_dt=0.02 #FXG DT\n    mpirank == 0 && @show (base_dt, Courant)\n\n    nsteps = ceil(Int64, tend / base_dt)\n    dt = tend / nsteps\n    mpirank == 0 && @show (dt, nsteps, dt * nsteps, tend)Do time stepping    stats = zeros(DFloat, 2)\n    mpirank == 0 && println(\"[CPU] computing initial energy...\")\n    Q_temp=copy(Q)\n    convert_set3c_to_set2nc(Val(dim), Val(N), vgeo, Q_temp)\n    stats[1] = L2energysquared(Val(dim), Val(N), Q_temp, vgeo, mesh.realelems)plot the initial condition    mkpath(\"viz\")\n    X = ntuple(j->reshape((@view vgeo[:, _x+j-1, :]), ntuple(j->N+1,dim)...,\n                          nelem), dim)\n    ρ = reshape((@view Q_temp[:, _ρ, :]), ntuple(j->(N+1),dim)..., nelem)\n    U = reshape((@view Q_temp[:, _U, :]), ntuple(j->(N+1),dim)..., nelem)\n    V = reshape((@view Q_temp[:, _V, :]), ntuple(j->(N+1),dim)..., nelem)\n    E = reshape((@view Q_temp[:, _E, :]), ntuple(j->(N+1),dim)..., nelem)\n    E = E .- 300.0\n    writemesh(@sprintf(\"viz/nse%dD_set3c_%s_rank_%04d_step_%05d\",\n                       dim, ArrType, mpirank, 0), X...;\n              fields=((\"ρ\", ρ), (\"U\", U), (\"V\", V), (\"E\", E)),\n              realelems=mesh.realelems)\n\n    mpirank == 0 && println(\"[DEV] starting time stepper...\")\n    lowstorageRK(Val(dim), Val(N), mesh, vgeo, sgeo, Q, rhs, D, dt, nsteps, tout,\n                 vmapM, vmapP, mpicomm, iplot, visc; ArrType=ArrType, plotstep=plotstep)plot the final solution    Q_temp=copy(Q)\n    convert_set3c_to_set2nc(Val(dim), Val(N), vgeo, Q_temp)\n    X = ntuple(j->reshape((@view vgeo[:, _x+j-1, :]), ntuple(j->N+1,dim)...,\n                          nelem), dim)\n    ρ = reshape((@view Q_temp[:, _ρ, :]), ntuple(j->(N+1),dim)..., nelem)\n    U = reshape((@view Q_temp[:, _U, :]), ntuple(j->(N+1),dim)..., nelem)\n    V = reshape((@view Q_temp[:, _V, :]), ntuple(j->(N+1),dim)..., nelem)\n    E = reshape((@view Q_temp[:, _E, :]), ntuple(j->(N+1),dim)..., nelem)\n    E = E .- 300.0\n    writemesh(@sprintf(\"viz/nse%dD_set3c_%s_rank_%04d_step_%05d\",\n                       dim, ArrType, mpirank, nsteps), X...;\n              fields=((\"ρ\", ρ), (\"U\", U), (\"V\", V), (\"E\", E)),\n              realelems=mesh.realelems)\n\n    mpirank == 0 && println(\"[CPU] computing final energy...\")\n    stats[2] = L2energysquared(Val(dim), Val(N), Q_temp, vgeo, mesh.realelems)\n\n    stats = sqrt.(MPI.allreduce(stats, MPI.SUM, mpicomm))\n\n    if  mpirank == 0\n        @show eng0 = stats[1]\n        @show engf = stats[2]\n        @show Δeng = engf - eng0\n    end\nend}}}{{{ mainfunction main()\n    DFloat = Float64MPI.Init()    MPI.Initialized() || MPI.Init()\n    MPI.finalize_atexit()\n\n    mpicomm = MPI.COMM_WORLD\n    mpirank = MPI.Comm_rank(mpicomm)\n    mpisize = MPI.Comm_size(mpicomm)FIXME: query via hostname    @hascuda device!(mpirank % length(devices()))\n\n    #Initial Conditions\n    function ic(dim, x...)FIXME: Type generic?        DFloat = eltype(x)\n        γ::DFloat       = _γ\n        p0::DFloat      = _p0\n        R_gas::DFloat   = _R_gas\n        c_p::DFloat     = _c_p\n        c_v::DFloat     = _c_v\n        gravity::DFloat = _gravity\n\n        u0 = 0\n        r = sqrt((x[1]-500)^2 + (x[dim]-350)^2 )\n        rc = 250.0\n        θ_ref=300.0\n        θ_c=0.5\n        Δθ=0.0\n        if r <= rc\n            Δθ = 0.5 * θ_c * (1.0 + cos(π * r/rc))\n        end\n        θ_k=θ_ref + Δθ\n        π_k=1.0 - gravity/(c_p*θ_k)*x[dim]\n        c=c_v/R_gas\n        ρ_k=p0/(R_gas*θ_k)*(π_k)^c\n        ρ = ρ_k\n        U = u0\n        V = 0.0\n        E = θ_k\n        ρ, U, V, E\n    end\n\n    #Input Parameters\n    time_final = DFloat(10.0)\n    iplot=100\n    Ne = 10\n    N  = 4\n    visc = 2.0\n    dim = 2\n    hardware=\"cpu\"\n    if mpirank == 0\n        @show (N,Ne,visc,iplot,time_final,hardware,mpisize)\n    end\n\n    #Mesh Generation\n    mesh2D = brickmesh((range(DFloat(0); length=Ne+1, stop=1000),\n                        range(DFloat(0); length=Ne+1, stop=1000)),\n                       (true, false),\n                       part=mpirank+1, numparts=mpisize)\n\n    #Call Solver\n    if hardware == \"cpu\"\n        mpirank == 0 && println(\"Running 2d (CPU)...\")\n        nse(Val(2), Val(N), mpicomm, (x...)->ic(dim, x...), mesh2D, time_final, iplot, visc;\n              ArrType=Array, tout = 10)\n        mpirank == 0 && println()\n    elseif hardware == \"gpu\"\n        @hascuda begin\n            mpirank == 0 && println(\"Running 2d (GPU)...\")\n            nse(Val(2), Val(N), mpicomm, (x...)->ic(dim, x...), mesh2D, time_final, iplot, visc;\n                  ArrType=CuArray, tout = 10)\n            mpirank == 0 && println()\n        end\n    end\n    nothing\nend}}}main()This page was generated using Literate.jl."
},

{
    "location": "examples/generated/3d_kernels/LDG3d/#",
    "page": "3D Diffusion Equation Example",
    "title": "3D Diffusion Equation Example",
    "category": "page",
    "text": "EditURL = \"https://github.com/climate-machine/Canary.jl/blob/master/examples/3d_kernels/LDG3d.jl\""
},

{
    "location": "examples/generated/3d_kernels/LDG3d/#D-Diffusion-Equation-Example-1",
    "page": "3D Diffusion Equation Example",
    "title": "3D Diffusion Equation Example",
    "category": "section",
    "text": ""
},

{
    "location": "examples/generated/3d_kernels/LDG3d/#Introduction-1",
    "page": "3D Diffusion Equation Example",
    "title": "Introduction",
    "category": "section",
    "text": "This example shows how to construct a 2nd derivative with DG using LDG."
},

{
    "location": "examples/generated/3d_kernels/LDG3d/#Continuous-Governing-Equations-1",
    "page": "3D Diffusion Equation Example",
    "title": "Continuous Governing Equations",
    "category": "section",
    "text": "We discretize the operator:nabla^2 q(xyz)   (1)in the following two-step process. First we discretizemathbfQ(xyz) = nabla q(xyz)   (2)followed bynabla cdot mathbfQ (xyz) =  nabla^2 q(xyz)   (3)"
},

{
    "location": "examples/generated/3d_kernels/LDG3d/#Local-Discontinous-Galerkin-(LDG)-Method-1",
    "page": "3D Diffusion Equation Example",
    "title": "Local Discontinous Galerkin (LDG) Method",
    "category": "section",
    "text": "Discretizing Eq.\\ (2) we getint_Omega_e mathbfPsi cdot mathbfQ^(e)_N dOmega_e = int_Omega_e mathbfPsi cdot nabla q^(e)_N dOmega_e   (4)where q^(e)_N=sum_i=1^(N+1)^dim psi_i(mathbfx) q_i and  mathbfQ^(e)_N=sum_i=1^(N+1)^dim psi_i(mathbfx) mathbfQ_i are the finite dimensional expansion with basis functions psi(mathbfx) and mathbfPsi is the block diagonal tensor with blocks comprised of psi as followsmathbfPsi = left(beginarrayccc\npsi  0  0 \n0  psi  0 \n0  0  psi\nendarray\nright)Integrating Eq. (4) by parts yieldsint_Omega_e mathbfPsi cdot mathbfQ^(e)_N dOmega_e = int_Gamma_e left( mathbfn cdot mathbfPsi right) q^(*e)_N dGamma_e - int_Omega_e left( nabla cdot mathbfPsi right) q^(e)_N dOmega_e   (5)Equation (5) represents the approximation of the gradient operator on the variable q where the first term on the right denotes the flux integral term (computed in \"function flux_grad\") and the second term on the right denotes the volume integral term (computed in \"function volume_grad\").  The superscript (*e) in the flux integral term denotes the numerical flux. Here we use the average flux. In matrix form, Eq. (5) becomesM^(e)_ij mathbfQ^(e)_j = mathbfF_ij q^(*e)_j - widetildemathbfD^(e) q^(e)_j   (6)Next, integrating Eq. (3) by parts gives a similar form to Eq. (6) as followsM^(e)_ij left( nabla^2 q^(e) right)_j = mathbfF_ij^T mathbfQ^(*e)_j - left( widetildemathbfD^(e) right)^T mathbfQ^(e)_j   (7)Equation (7) represents the approximation of the divergence operator on the vector mathbfQ where the first term on the right denotes the flux integral (computed in \"function volume_div\") and the second term on the right denotes the volume integral term (computed in \"function volume_div\")."
},

{
    "location": "examples/generated/3d_kernels/LDG3d/#Commented-Program-1",
    "page": "3D Diffusion Equation Example",
    "title": "Commented Program",
    "category": "section",
    "text": "include(joinpath(@__DIR__,\"vtk.jl\"))\nusing MPI\nusing Canary\nusing Printf: @sprintf\nconst HAVE_CUDA = try\n    using CUDAnative\n    using CUDAdrv\n    true\ncatch\n    false\nend\nif HAVE_CUDA\n    macro hascuda(ex)\n        return :($(esc(ex)))\n    end\nelse\n    macro hascuda(ex)\n        return :()\n    end\nend{{{ reshape for CuArray@hascuda function Base.reshape(A::CuArray, dims::NTuple{N, Int}) where {N}\n    @assert prod(dims) == prod(size(A))\n    CuArray{eltype(A), length(dims)}(dims, A.buf)\nend}}}{{{ constants note the order of the fields below is also assumed in the code.const _nstate = 5\nconst _U, _V, _W, _ρ, _E = 1:_nstate\nconst stateid = (U = _U, V = _V, W = _W, ρ = _ρ, E = _E)\n\nconst _nvgeo = 14\nconst _ξx, _ηx, _ζx, _ξy, _ηy, _ζy, _ξz, _ηz, _ζz, _MJ, _MJI,\n_x, _y, _z = 1:_nvgeo\nconst vgeoid = (ξx = _ξx, ηx = _ηx, ζx = _ζx,\n                ξy = _ξy, ηy = _ηy, ζy = _ζy,\n                ξz = _ξz, ηz = _ηz, ζz = _ζz,\n                MJ = _MJ, MJI = _MJI,\n                x = _x,   y = _y,   z = _z)\n\nconst _nsgeo = 5\nconst _nx, _ny, _nz, _sMJ, _vMJI = 1:_nsgeo\nconst sgeoid = (nx = _nx, ny = _ny, nz = _nz, sMJ = _sMJ, vMJI = _vMJI)\n\nconst _γ = 14  // 10\nconst _p0 = 100000\nconst _R_gas = 28717 // 100\nconst _c_p = 100467 // 100\nconst _c_v = 7175 // 10\nconst _gravity = 10}}}{{{ courantfunction courantnumber(::Val{dim}, ::Val{N}, vgeo, Q, mpicomm) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1)^dim\n    (~, ~, nelem) = size(Q)\n\n    dt = [floatmax(DFloat)]\n    Courant = - [floatmax(DFloat)]\n\n    #Compute DT\n    @inbounds for e = 1:nelem, n = 1:Np\n        ρ, U, V, W, E = Q[n, _ρ, e], Q[n, _U, e], Q[n, _V, e], Q[n, _W, e], Q[n, _E, e]\n        ξx, ξy, ξz = vgeo[n, _ξx, e], vgeo[n, _ξy, e], vgeo[n, _ξz, e]\n        ηx, ηy, ηz = vgeo[n, _ηx, e], vgeo[n, _ηy, e], vgeo[n, _ηz, e]\n        ζx, ζy, ζz = vgeo[n, _ζx, e], vgeo[n, _ζy, e], vgeo[n, _ζz, e]\n        z = vgeo[n, _z, e]\n        P = (R_gas/c_v)*(E - (U^2 + V^2 + W^2)/(2*ρ) - ρ*gravity*z)\n        u, v, w = U/ρ, V/ρ, W/ρ\n        dx=sqrt( (1.0/(2*ξx))^2 + 0*(1.0/(2*ηy))^2  + (1.0/(2*ζz))^2 )\n        vel=sqrt( u^2 + v^2 + w^2)\n        wave_speed = (vel + sqrt(γ * P / ρ))\n        loc_dt = 1.0*dx/wave_speed/N\n        dt[1] = min(dt[1], loc_dt)\n    end\n    dt_min=MPI.Allreduce(dt[1], MPI.MIN, mpicomm)\n\n    #Compute Courant\n    @inbounds for e = 1:nelem, n = 1:Np\n        ρ, U, V, W, E = Q[n, _ρ, e], Q[n, _U, e], Q[n, _V, e], Q[n, _W, e], Q[n, _E, e]\n        ξx, ξy, ξz = vgeo[n, _ξx, e], vgeo[n, _ξy, e], vgeo[n, _ξz, e]\n        ηx, ηy, ηz = vgeo[n, _ηx, e], vgeo[n, _ηy, e], vgeo[n, _ηz, e]\n        ζx, ζy, ζz = vgeo[n, _ζx, e], vgeo[n, _ζy, e], vgeo[n, _ζz, e]\n        z = vgeo[n, _z, e]\n        P = (R_gas/c_v)*(E - (U^2 + V^2 + W^2)/(2*ρ) - ρ*gravity*z)\n        u, v, w = U/ρ, V/ρ, W/ρ\n        dx=sqrt( (1.0/(2*ξx))^2 + 0*(1.0/(2*ηy))^2  + (1.0/(2*ζz))^2 )\n        vel=sqrt( u^2 + v^2 + w^2)\n        wave_speed = (vel + sqrt(γ * P / ρ))\n        loc_Courant = wave_speed*dt_min*N/dx\n        Courant[1] = max(Courant[1], loc_Courant)\n    end\n    Courant_max=MPI.Allreduce(Courant[1], MPI.MAX, mpicomm)\n\n    (dt_min, Courant_max)\nend}}}{{{ cflfunction cfl(::Val{dim}, ::Val{N}, vgeo, Q, mpicomm) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1)^dim\n    (~, ~, nelem) = size(Q)\n\n    dt = [floatmax(DFloat)]\n\n    @inbounds for e = 1:nelem, n = 1:Np\n        ρ, U, V, W = Q[n, _ρ, e], Q[n, _U, e], Q[n, _V, e], Q[n, _W, e]\n        E = Q[n, _E, e]\n        P = p0 * (R_gas * E / p0)^(c_p / c_v)\n\n        ξx, ξy, ξz = vgeo[n, _ξx, e], vgeo[n, _ξy, e], vgeo[n, _ξz, e]\n        ηx, ηy, ηz = vgeo[n, _ηx, e], vgeo[n, _ηy, e], vgeo[n, _ηz, e]\n        ζx, ζy, ζz = vgeo[n, _ζx, e], vgeo[n, _ζy, e], vgeo[n, _ζz, e]\n\n        loc_dt = 2ρ / max(abs(U * ξx + V * ξy + W * ξz) + ρ * sqrt(γ * P / ρ),\n                          abs(U * ηx + V * ηy + W * ηz) + ρ * sqrt(γ * P / ρ),\n                          abs(U * ζx + V * ζy + W * ζz) + ρ * sqrt(γ * P / ρ))\n        dt[1] = min(dt[1], loc_dt)\n    end\n\n    MPI.Allreduce(dt[1], MPI.MIN, mpicomm)\nend}}}{{{ compute geometryfunction computegeometry(::Val{dim}, mesh, D, ξ, ω, meshwarp, vmapM) where dimCompute metric terms    Nq = size(D, 1)\n    DFloat = eltype(D)\n\n    (nface, nelem) = size(mesh.elemtoelem)\n\n    crd = creategrid(Val(dim), mesh.elemtocoord, ξ)\n\n    vgeo = zeros(DFloat, Nq^dim, _nvgeo, nelem)\n    sgeo = zeros(DFloat, _nsgeo, Nq^(dim-1), nface, nelem)\n\n    (ξx, ηx, ζx, ξy, ηy, ζy, ξz, ηz, ζz, MJ, MJI, x, y, z) =\n        ntuple(j->(@view vgeo[:, j, :]), _nvgeo)\n    J = similar(x)\n    (nx, ny, nz, sMJ, vMJI) = ntuple(j->(@view sgeo[ j, :, :, :]), _nsgeo)\n    sJ = similar(sMJ)\n\n    X = ntuple(j->(@view vgeo[:, _x+j-1, :]), dim)\n    creategrid!(X..., mesh.elemtocoord, ξ)\n\n    @inbounds for j = 1:length(x)\n        (x[j], y[j], z[j]) = meshwarp(x[j], y[j], z[j])\n    endCompute the metric terms    computemetric!(x, y, z, J, ξx, ηx, ζx, ξy, ηy, ζy, ξz, ηz, ζz, sJ,\n                   nx, ny, nz, D)\n\n    M = kron(1, ntuple(j->ω, dim)...)\n    MJ .= M .* J\n    MJI .= 1 ./ MJ\n    vMJI .= MJI[vmapM]\n\n    sM = dim > 1 ? kron(1, ntuple(j->ω, dim-1)...) : one(DFloat)\n    sMJ .= sM .* sJ\n\n    (vgeo, sgeo)\nend}}}{{{ CPU Kernels Volume RHSfunction volumerhs!(::Val{dim}, ::Val{N}, rhs::Array, Q, vgeo, D, elems) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Nq = N + 1\n    nelem = size(Q)[end]\n\n    Q = reshape(Q, Nq, Nq, Nq, _nstate, nelem)\n    rhs = reshape(rhs, Nq, Nq, Nq, _nstate, nelem)\n    vgeo = reshape(vgeo, Nq, Nq, Nq, _nvgeo, nelem)\n\n    s_F = Array{DFloat}(undef, Nq, Nq, Nq, _nstate)\n    s_G = Array{DFloat}(undef, Nq, Nq, Nq, _nstate)\n    s_H = Array{DFloat}(undef, Nq, Nq, Nq, _nstate)\n\n    @inbounds for e in elems\n        for k = 1:Nq, j = 1:Nq, i = 1:Nq\n            MJ = vgeo[i, j, k, _MJ, e]\n            ξx, ξy, ξz = vgeo[i,j,k,_ξx,e], vgeo[i,j,k,_ξy,e], vgeo[i,j,k,_ξz,e]\n            ηx, ηy, ηz = vgeo[i,j,k,_ηx,e], vgeo[i,j,k,_ηy,e], vgeo[i,j,k,_ηz,e]\n            ζx, ζy, ζz = vgeo[i,j,k,_ζx,e], vgeo[i,j,k,_ζy,e], vgeo[i,j,k,_ζz,e]\n            z = vgeo[i,j,k,_z,e]\n\n            U, V, W = Q[i, j, k, _U, e], Q[i, j, k, _V, e], Q[i, j, k, _W, e]\n            ρ, E = Q[i, j, k, _ρ, e], Q[i, j, k, _E, e]\n            P = (R_gas/c_v)*(E - (U^2 + V^2 + W^2)/(2*ρ) - ρ*gravity*z)\n\n            ρinv = 1 / ρ\n            fluxρ_x = U\n            fluxU_x = ρinv * U * U + P\n            fluxV_x = ρinv * V * U\n            fluxW_x = ρinv * W * U\n            fluxE_x = ρinv * U * (E+P)\n\n            fluxρ_y = V\n            fluxU_y = ρinv * U * V\n            fluxV_y = ρinv * V * V + P\n            fluxW_y = ρinv * W * V\n            fluxE_y = ρinv * V * (E+P)\n\n            fluxρ_z = W\n            fluxU_z = ρinv * U * W\n            fluxV_z = ρinv * V * W\n            fluxW_z = ρinv * W * W + P\n            fluxE_z = ρinv * W * (E+P)\n\n            s_F[i, j, k, _ρ] = MJ * (ξx * fluxρ_x + ξy * fluxρ_y + ξz * fluxρ_z)\n            s_F[i, j, k, _U] = MJ * (ξx * fluxU_x + ξy * fluxU_y + ξz * fluxU_z)\n            s_F[i, j, k, _V] = MJ * (ξx * fluxV_x + ξy * fluxV_y + ξz * fluxV_z)\n            s_F[i, j, k, _W] = MJ * (ξx * fluxW_x + ξy * fluxW_y + ξz * fluxW_z)\n            s_F[i, j, k, _E] = MJ * (ξx * fluxE_x + ξy * fluxE_y + ξz * fluxE_z)\n\n            s_G[i, j, k, _ρ] = MJ * (ηx * fluxρ_x + ηy * fluxρ_y + ηz * fluxρ_z)\n            s_G[i, j, k, _U] = MJ * (ηx * fluxU_x + ηy * fluxU_y + ηz * fluxU_z)\n            s_G[i, j, k, _V] = MJ * (ηx * fluxV_x + ηy * fluxV_y + ηz * fluxV_z)\n            s_G[i, j, k, _W] = MJ * (ηx * fluxW_x + ηy * fluxW_y + ηz * fluxW_z)\n            s_G[i, j, k, _E] = MJ * (ηx * fluxE_x + ηy * fluxE_y + ηz * fluxE_z)\n\n            s_H[i, j, k, _ρ] = MJ * (ζx * fluxρ_x + ζy * fluxρ_y + ζz * fluxρ_z)\n            s_H[i, j, k, _U] = MJ * (ζx * fluxU_x + ζy * fluxU_y + ζz * fluxU_z)\n            s_H[i, j, k, _V] = MJ * (ζx * fluxV_x + ζy * fluxV_y + ζz * fluxV_z)\n            s_H[i, j, k, _W] = MJ * (ζx * fluxW_x + ζy * fluxW_y + ζz * fluxW_z)\n            s_H[i, j, k, _E] = MJ * (ζx * fluxE_x + ζy * fluxE_y + ζz * fluxE_z)buoyancy term            rhs[i, j, k, _W, e] -= MJ * ρ * gravity\n        endloop of ξ-grid lines        for s = 1:_nstate, k = 1:Nq, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, k, s, e] += D[n, i] * s_F[n, j, k, s]\n        endloop of η-grid lines        for s = 1:_nstate, k = 1:Nq, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, k, s, e] += D[n, j] * s_G[i, n, k, s]\n        endloop of ζ-grid lines        for s = 1:_nstate, k = 1:Nq, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, k, s, e] += D[n, k] * s_H[i, j, n, s]\n        end\n    end\nendflux RHSfunction fluxrhs!(::Val{dim}, ::Val{N}, rhs::Array, Q, sgeo, vgeo, elems, vmapM,\n                  vmapP, elemtobndy) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1)^dim\n    Nfp = (N+1)^(dim-1)\n    nface = 2*dim\n\n    @inbounds for e in elems\n        for f = 1:nface\n            for n = 1:Nfp\n                (nxM, nyM, nzM, sMJ, ~) = sgeo[:, n, f, e]\n                idM, idP = vmapM[n, f, e], vmapP[n, f, e]\n\n                eM, eP = e, ((idP - 1) ÷ Np) + 1\n                vidM, vidP = ((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1\n\n                ρM = Q[vidM, _ρ, eM]\n                UM = Q[vidM, _U, eM]\n                VM = Q[vidM, _V, eM]\n                WM = Q[vidM, _W, eM]\n                EM = Q[vidM, _E, eM]\n                zM = vgeo[vidM, _z, eM]\n\n                bc = elemtobndy[f, e]\n                PM = (R_gas/c_v)*(EM - (UM^2 + VM^2 + WM^2)/(2*ρM) - ρM*gravity*zM)\n                if bc == 0\n                    ρP = Q[vidP, _ρ, eP]\n                    UP = Q[vidP, _U, eP]\n                    VP = Q[vidP, _V, eP]\n                    WP = Q[vidP, _W, eP]\n                    EP = Q[vidP, _E, eP]\n                    zP = vgeo[vidP, _z, eP]\n                    PP = (R_gas/c_v)*(EP - (UP^2 + VP^2 + WP^2)/(2*ρP) - ρP*gravity*zP)\n                elseif bc == 1\n                    UnM = nxM * UM + nyM * VM + nzM * WM\n                    UP = UM - 2 * UnM * nxM\n                    VP = VM - 2 * UnM * nyM\n                    WP = WM - 2 * UnM * nzM\n                    ρP = ρM\n                    EP = EM\n                    PP = PM\n                else\n                    error(\"Invalid boundary conditions $bc on face $f of element $e\")\n                end\n\n                ρMinv = 1 / ρM\n                fluxρM_x = UM\n                fluxUM_x = ρMinv * UM * UM + PM\n                fluxVM_x = ρMinv * VM * UM\n                fluxWM_x = ρMinv * WM * UM\n                fluxEM_x = ρMinv * UM * (EM+PM)\n\n                fluxρM_y = VM\n                fluxUM_y = ρMinv * UM * VM\n                fluxVM_y = ρMinv * VM * VM + PM\n                fluxWM_y = ρMinv * WM * VM\n                fluxEM_y = ρMinv * VM * (EM+PM)\n\n                fluxρM_z = WM\n                fluxUM_z = ρMinv * UM * WM\n                fluxVM_z = ρMinv * VM * WM\n                fluxWM_z = ρMinv * WM * WM + PM\n                fluxEM_z = ρMinv * WM * (EM+PM)\n\n                ρPinv = 1 / ρP\n                fluxρP_x = UP\n                fluxUP_x = ρPinv * UP * UP + PP\n                fluxVP_x = ρPinv * VP * UP\n                fluxWP_x = ρPinv * WP * UP\n                fluxEP_x = ρPinv * UP * (EP+PP)\n\n                fluxρP_y = VP\n                fluxUP_y = ρPinv * UP * VP\n                fluxVP_y = ρPinv * VP * VP + PP\n                fluxWP_y = ρPinv * WP * VP\n                fluxEP_y = ρPinv * VP * (EP+PP)\n\n                fluxρP_z = WP\n                fluxUP_z = ρPinv * UP * WP\n                fluxVP_z = ρPinv * VP * WP\n                fluxWP_z = ρPinv * WP * WP + PP\n                fluxEP_z = ρPinv * WP * (EP+PP)\n\n                λM = ρMinv * abs(nxM * UM + nyM * VM + nzM * WM) + sqrt(ρMinv * γ * PM)\n                λP = ρPinv * abs(nxM * UP + nyM * VP + nzM * WP) + sqrt(ρPinv * γ * PP)\n                λ  =  max(λM, λP)\n\n                #Compute Numerical Flux and Update\n                fluxρS = (nxM * (fluxρM_x + fluxρP_x) + nyM * (fluxρM_y + fluxρP_y) +\n                          nzM * (fluxρM_z + fluxρP_z) - λ * (ρP - ρM)) / 2\n                fluxUS = (nxM * (fluxUM_x + fluxUP_x) + nyM * (fluxUM_y + fluxUP_y) +\n                          nzM * (fluxUM_z + fluxUP_z) - λ * (UP - UM)) / 2\n                fluxVS = (nxM * (fluxVM_x + fluxVP_x) + nyM * (fluxVM_y + fluxVP_y) +\n                          nzM * (fluxVM_z + fluxVP_z) - λ * (VP - VM)) / 2\n                fluxWS = (nxM * (fluxWM_x + fluxWP_x) + nyM * (fluxWM_y + fluxWP_y) +\n                          nzM * (fluxWM_z + fluxWP_z) - λ * (WP - WM)) / 2\n                fluxES = (nxM * (fluxEM_x + fluxEP_x) + nyM * (fluxEM_y + fluxEP_y) +\n                          nzM * (fluxEM_z + fluxEP_z) - λ * (EP - EM)) / 2\n\n\n                #Update RHS\n                rhs[vidM, _ρ, eM] -= sMJ * fluxρS\n                rhs[vidM, _U, eM] -= sMJ * fluxUS\n                rhs[vidM, _V, eM] -= sMJ * fluxVS\n                rhs[vidM, _W, eM] -= sMJ * fluxWS\n                rhs[vidM, _E, eM] -= sMJ * fluxES\n            end\n        end\n    end\nend}}}{{{ Volume grad(Q)function volume_grad!(::Val{dim}, ::Val{N}, rhs::Array, Q, vgeo, D, elems) where {dim, N}\n    DFloat = eltype(Q)\n    Nq = N + 1\n    nelem = size(Q)[end]\n\n    Q = reshape(Q, Nq, Nq, Nq, _nstate, nelem)\n    rhs = reshape(rhs, Nq, Nq, Nq, _nstate, dim, nelem)\n    vgeo = reshape(vgeo, Nq, Nq, Nq, _nvgeo, nelem)\n\n    #Initialize RHS vector\n    fill!( rhs, zero(rhs[1]))\n\n    #Allocate Arrays\n    s_F = Array{DFloat}(undef, Nq, Nq, Nq, _nstate, dim)\n    s_G = Array{DFloat}(undef, Nq, Nq, Nq, _nstate, dim)\n    s_H = Array{DFloat}(undef, Nq, Nq, Nq, _nstate, dim)\n\n    @inbounds for e in elems\n        for k = 1:Nq, j = 1:Nq, i = 1:Nq\n            MJ = vgeo[i, j, k, _MJ, e]\n            ξx, ξy, ξz = vgeo[i,j,k,_ξx,e], vgeo[i,j,k,_ξy,e], vgeo[i,j,k,_ξz,e]\n            ηx, ηy, ηz = vgeo[i,j,k,_ηx,e], vgeo[i,j,k,_ηy,e], vgeo[i,j,k,_ηz,e]\n            ζx, ζy, ζz = vgeo[i,j,k,_ζx,e], vgeo[i,j,k,_ζy,e], vgeo[i,j,k,_ζz,e]\n\n            U, V, W = Q[i, j, k, _U, e], Q[i, j, k, _V, e], Q[i, j, k, _W, e]\n            ρ, E = Q[i, j, k, _ρ, e], Q[i, j, k, _E, e]\n\n            #Compute fluxes\n            fluxρ = ρ\n            fluxU = U\n            fluxV = V\n            fluxW = W\n            fluxE = E\n\n            s_F[i, j, k, _ρ, 1], s_F[i, j, k, _ρ, 2], s_F[i, j, k, _ρ, 3] = MJ * (ξx * fluxρ), MJ * (ξy * fluxρ), MJ * (ξz * fluxρ)\n            s_F[i, j, k, _U, 1], s_F[i, j, k, _U, 2], s_F[i, j, k, _U, 3] = MJ * (ξx * fluxU), MJ * (ξy * fluxU), MJ * (ξz * fluxU)\n            s_F[i, j, k, _V, 1], s_F[i, j, k, _V, 2], s_F[i, j, k, _V, 3] = MJ * (ξx * fluxV), MJ * (ξy * fluxV), MJ * (ξz * fluxV)\n            s_F[i, j, k, _W, 1], s_F[i, j, k, _W, 2], s_F[i, j, k, _W, 3] = MJ * (ξx * fluxW), MJ * (ξy * fluxW), MJ * (ξz * fluxW)\n            s_F[i, j, k, _E, 1], s_F[i, j, k, _E, 2], s_F[i, j, k, _E, 3] = MJ * (ξx * fluxE), MJ * (ξy * fluxE), MJ * (ξz * fluxE)\n\n            s_G[i, j, k, _ρ, 1], s_G[i, j, k, _ρ, 2], s_G[i, j, k, _ρ, 3] = MJ * (ηx * fluxρ), MJ * (ηy * fluxρ), MJ * (ηz * fluxρ)\n            s_G[i, j, k, _U, 1], s_G[i, j, k, _U, 2], s_G[i, j, k, _U, 3] = MJ * (ηx * fluxU), MJ * (ηy * fluxU), MJ * (ηz * fluxU)\n            s_G[i, j, k, _V, 1], s_G[i, j, k, _V, 2], s_G[i, j, k, _V, 3] = MJ * (ηx * fluxV), MJ * (ηy * fluxV), MJ * (ηz * fluxV)\n            s_G[i, j, k, _W, 1], s_G[i, j, k, _W, 2], s_G[i, j, k, _W, 3] = MJ * (ηx * fluxW), MJ * (ηy * fluxW), MJ * (ηz * fluxW)\n            s_G[i, j, k, _E, 1], s_G[i, j, k, _E, 2], s_G[i, j, k, _E, 3] = MJ * (ηx * fluxE), MJ * (ηy * fluxE), MJ * (ηz * fluxE)\n\n            s_H[i, j, k, _ρ, 1], s_H[i, j, k, _ρ, 2], s_H[i, j, k, _ρ, 3] = MJ * (ζx * fluxρ), MJ * (ζy * fluxρ), MJ * (ζz * fluxρ)\n            s_H[i, j, k, _U, 1], s_H[i, j, k, _U, 2], s_H[i, j, k, _U, 3] = MJ * (ζx * fluxU), MJ * (ζy * fluxU), MJ * (ζz * fluxU)\n            s_H[i, j, k, _V, 1], s_H[i, j, k, _V, 2], s_H[i, j, k, _V, 3] = MJ * (ζx * fluxV), MJ * (ζy * fluxV), MJ * (ζz * fluxV)\n            s_H[i, j, k, _W, 1], s_H[i, j, k, _W, 2], s_H[i, j, k, _W, 3] = MJ * (ζx * fluxW), MJ * (ζy * fluxW), MJ * (ζz * fluxW)\n            s_H[i, j, k, _E, 1], s_H[i, j, k, _E, 2], s_H[i, j, k, _E, 3] = MJ * (ζx * fluxE), MJ * (ζy * fluxE), MJ * (ζz * fluxE)\n        endloop of ξ-grid lines        for s = 1:_nstate, k = 1:Nq, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, k, s, 1, e] -= D[n, i] * s_F[n, j, k, s, 1]\n            rhs[i, j, k, s, 2, e] -= D[n, i] * s_F[n, j, k, s, 2]\n            rhs[i, j, k, s, 3, e] -= D[n, i] * s_F[n, j, k, s, 3]\n        endloop of η-grid lines        for s = 1:_nstate, k = 1:Nq, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, k, s, 1, e] -= D[n, j] * s_G[i, n, k, s, 1]\n            rhs[i, j, k, s, 2, e] -= D[n, j] * s_G[i, n, k, s, 2]\n            rhs[i, j, k, s, 3, e] -= D[n, j] * s_G[i, n, k, s, 3]\n        endloop of ζ-grid lines        for s = 1:_nstate, k = 1:Nq, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, k, s, 1, e] -= D[n, k] * s_H[i, j, n, s, 1]\n            rhs[i, j, k, s, 2, e] -= D[n, k] * s_H[i, j, n, s, 2]\n            rhs[i, j, k, s, 3, e] -= D[n, k] * s_H[i, j, n, s, 3]\n        end\n    end\nend}}}Flux grad(Q)function flux_grad!(::Val{dim}, ::Val{N}, rhs::Array,  Q, sgeo, elems, vmapM, vmapP, elemtobndy) where {dim, N}\n    Np = (N+1)^dim\n    Nfp = (N+1)^(dim-1)\n    nface = 2*dim\n\n    @inbounds for e in elems\n        for f = 1:nface\n            for n = 1:Nfp\n                (nxM, nyM, nzM, sMJ, ~) = sgeo[:, n, f, e]\n                idM, idP = vmapM[n, f, e], vmapP[n, f, e]\n\n                eM, eP = e, ((idP - 1) ÷ Np) + 1\n                vidM, vidP = ((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1\n\n                ρM = Q[vidM, _ρ, eM]\n                UM = Q[vidM, _U, eM]\n                VM = Q[vidM, _V, eM]\n                WM = Q[vidM, _W, eM]\n                EM = Q[vidM, _E, eM]\n\n                bc = elemtobndy[f, e]\n                ρP = UP = VP = WP = EP = zero(eltype(Q))\n                if bc == 0\n                    ρP = Q[vidP, _ρ, eP]\n                    UP = Q[vidP, _U, eP]\n                    VP = Q[vidP, _V, eP]\n                    WP = Q[vidP, _W, eP]\n                    EP = Q[vidP, _E, eP]\n                elseif bc == 1\n                    UnM = nxM * UM + nyM * VM + nzM * WM\n                    UP = UM - 2 * UnM * nxM\n                    VP = VM - 2 * UnM * nyM\n                    WP = WM - 2 * UnM * nzM\n                    ρP = ρM\n                    EP = EM\n                else\n                    error(\"Invalid boundary conditions $bc on face $f of element $e\")\n                end\n\n                #Left Fluxes\n                fluxρM = ρM\n                fluxUM = UM\n                fluxVM = VM\n                fluxWM = WM\n                fluxEM = EM\n\n                #Right Fluxes\n                fluxρP = ρP\n                fluxUP = UP\n                fluxVP = VP\n                fluxWP = WP\n                fluxEP = EP\n\n                #Compute Numerical/Rusanov Flux\n                fluxρS = 0.5*(fluxρM + fluxρP)\n                fluxUS = 0.5*(fluxUM + fluxUP)\n                fluxVS = 0.5*(fluxVM + fluxVP)\n                fluxWS = 0.5*(fluxWM + fluxWP)\n                fluxES = 0.5*(fluxEM + fluxEP)\n\n                #Update RHS\n                rhs[vidM, _ρ, 1, eM] += sMJ * nxM*fluxρS\n                rhs[vidM, _ρ, 2, eM] += sMJ * nyM*fluxρS\n                rhs[vidM, _ρ, 3, eM] += sMJ * nzM*fluxρS\n                rhs[vidM, _U, 1, eM] += sMJ * nxM*fluxUS\n                rhs[vidM, _U, 2, eM] += sMJ * nyM*fluxUS\n                rhs[vidM, _U, 3, eM] += sMJ * nzM*fluxUS\n                rhs[vidM, _V, 1, eM] += sMJ * nxM*fluxVS\n                rhs[vidM, _V, 2, eM] += sMJ * nyM*fluxVS\n                rhs[vidM, _V, 3, eM] += sMJ * nzM*fluxVS\n                rhs[vidM, _W, 1, eM] += sMJ * nxM*fluxWS\n                rhs[vidM, _W, 2, eM] += sMJ * nyM*fluxWS\n                rhs[vidM, _W, 3, eM] += sMJ * nzM*fluxWS\n                rhs[vidM, _E, 1, eM] += sMJ * nxM*fluxES\n                rhs[vidM, _E, 2, eM] += sMJ * nyM*fluxES\n                rhs[vidM, _E, 3, eM] += sMJ * nzM*fluxES\n            end\n        end\n    end\nend}}}{{{ Volume div(grad(Q))function volume_div!(::Val{dim}, ::Val{N}, rhs::Array, Q, vgeo, D, elems) where {dim, N}\n    DFloat = eltype(Q)\n    Nq = N + 1\n    nelem = size(Q)[end]\n\n    Q = reshape(Q, Nq, Nq, Nq, _nstate, dim, nelem)\n    rhs = reshape(rhs, Nq, Nq, Nq, _nstate, dim, nelem)\n    vgeo = reshape(vgeo, Nq, Nq, Nq, _nvgeo, nelem)\n\n    #Initialize RHS vector\n    fill!( rhs, zero(rhs[1]))\n\n    #Allocate Arrays\n    s_F = Array{DFloat}(undef, Nq, Nq, Nq, _nstate)\n    s_G = Array{DFloat}(undef, Nq, Nq, Nq, _nstate)\n    s_H = Array{DFloat}(undef, Nq, Nq, Nq, _nstate)\n\n    @inbounds for e in elems\n        for k = 1:Nq, j = 1:Nq, i = 1:Nq\n            MJ = vgeo[i, j, k, _MJ, e]\n            ξx, ξy, ξz = vgeo[i,j,k,_ξx,e], vgeo[i,j,k,_ξy,e], vgeo[i,j,k,_ξz,e]\n            ηx, ηy, ηz = vgeo[i,j,k,_ηx,e], vgeo[i,j,k,_ηy,e], vgeo[i,j,k,_ηz,e]\n            ζx, ζy, ζz = vgeo[i,j,k,_ζx,e], vgeo[i,j,k,_ζy,e], vgeo[i,j,k,_ζz,e]\n\n            ρx, ρy, ρz = Q[i,j,k,_ρ,1,e], Q[i,j,k,_ρ,2,e], Q[i,j,k,_ρ,3,e]\n            Ux, Uy, Uz = Q[i,j,k,_U,1,e], Q[i,j,k,_U,2,e], Q[i,j,k,_U,3,e]\n            Vx, Vy, Vz = Q[i,j,k,_V,1,e], Q[i,j,k,_V,2,e], Q[i,j,k,_V,3,e]\n            Wx, Wy, Wz = Q[i,j,k,_W,1,e], Q[i,j,k,_W,2,e], Q[i,j,k,_W,3,e]\n            Ex, Ey, Ez = Q[i,j,k,_E,1,e], Q[i,j,k,_E,2,e], Q[i,j,k,_E,3,e]\n\n            #Compute fluxes\n            fluxρ_x = ρx\n            fluxρ_y = ρy\n            fluxρ_z = ρz\n            fluxU_x = Ux\n            fluxU_y = Uy\n            fluxU_z = Uz\n            fluxV_x = Vx\n            fluxV_y = Vy\n            fluxV_z = Vz\n            fluxW_x = Wx\n            fluxW_y = Wy\n            fluxW_z = Wz\n            fluxE_x = Ex\n            fluxE_y = Ey\n            fluxE_z = Ez\n\n            s_F[i, j, k, _ρ] = MJ * (ξx * fluxρ_x + ξy * fluxρ_y + ξz * fluxρ_z)\n            s_F[i, j, k, _U] = MJ * (ξx * fluxU_x + ξy * fluxU_y + ξz * fluxU_z)\n            s_F[i, j, k, _V] = MJ * (ξx * fluxV_x + ξy * fluxV_y + ξz * fluxV_z)\n            s_F[i, j, k, _W] = MJ * (ξx * fluxW_x + ξy * fluxW_y + ξz * fluxW_z)\n            s_F[i, j, k, _E] = MJ * (ξx * fluxE_x + ξy * fluxE_y + ξz * fluxE_z)\n\n            s_G[i, j, k, _ρ] = MJ * (ηx * fluxρ_x + ηy * fluxρ_y + ηz * fluxρ_z)\n            s_G[i, j, k, _U] = MJ * (ηx * fluxU_x + ηy * fluxU_y + ηz * fluxU_z)\n            s_G[i, j, k, _V] = MJ * (ηx * fluxV_x + ηy * fluxV_y + ηz * fluxV_z)\n            s_G[i, j, k, _W] = MJ * (ηx * fluxW_x + ηy * fluxW_y + ηz * fluxW_z)\n            s_G[i, j, k, _E] = MJ * (ηx * fluxE_x + ηy * fluxE_y + ηz * fluxE_z)\n\n            s_H[i, j, k, _ρ] = MJ * (ζx * fluxρ_x + ζy * fluxρ_y + ζz * fluxρ_z)\n            s_H[i, j, k, _U] = MJ * (ζx * fluxU_x + ζy * fluxU_y + ζz * fluxU_z)\n            s_H[i, j, k, _V] = MJ * (ζx * fluxV_x + ζy * fluxV_y + ζz * fluxV_z)\n            s_H[i, j, k, _W] = MJ * (ζx * fluxW_x + ζy * fluxW_y + ζz * fluxW_z)\n            s_H[i, j, k, _E] = MJ * (ζx * fluxE_x + ζy * fluxE_y + ζz * fluxE_z)\n        endloop of ξ-grid lines        for s = 1:_nstate, k = 1:Nq, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, k, s, 1, e] -= D[n, i] * s_F[n, j, k, s]\n        endloop of η-grid lines        for s = 1:_nstate, k = 1:Nq, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, k, s, 1, e] -= D[n, j] * s_G[i, n, k, s]\n        endloop of ζ-grid lines        for s = 1:_nstate, k = 1:Nq, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, k, s, 1, e] -= D[n, k] * s_H[i, j, n, s]\n        end\n    end\nend}}}Flux div(grad(Q))function flux_div!(::Val{dim}, ::Val{N}, rhs::Array,  Q, sgeo, elems, vmapM, vmapP, elemtobndy) where {dim, N}\n    Np = (N+1)^dim\n    Nfp = (N+1)^(dim-1)\n    nface = 2*dim\n\n    @inbounds for e in elems\n        for f = 1:nface\n            for n = 1:Nfp\n                (nxM, nyM, nzM, sMJ, ~) = sgeo[:, n, f, e]\n                idM, idP = vmapM[n, f, e], vmapP[n, f, e]\n\n                eM, eP = e, ((idP - 1) ÷ Np) + 1\n                vidM, vidP = ((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1\n\n                ρxM = Q[vidM, _ρ, 1, eM]\n                ρyM = Q[vidM, _ρ, 2, eM]\n                ρzM = Q[vidM, _ρ, 3, eM]\n                UxM = Q[vidM, _U, 1, eM]\n                UyM = Q[vidM, _U, 2, eM]\n                UzM = Q[vidM, _U, 3, eM]\n                VxM = Q[vidM, _V, 1, eM]\n                VyM = Q[vidM, _V, 2, eM]\n                VzM = Q[vidM, _V, 3, eM]\n                WxM = Q[vidM, _W, 1, eM]\n                WyM = Q[vidM, _W, 2, eM]\n                WzM = Q[vidM, _W, 3, eM]\n                ExM = Q[vidM, _E, 1, eM]\n                EyM = Q[vidM, _E, 2, eM]\n                EzM = Q[vidM, _E, 3, eM]\n\n                bc = elemtobndy[f, e]\n                ρxP = ρyP = ρzP = zero(eltype(Q))\n                UxP = UyP = UzP = zero(eltype(Q))\n                VxP = VyP = VzP = zero(eltype(Q))\n                WxP = WyP = WzP = zero(eltype(Q))\n                ExP = EyP = EzP = zero(eltype(Q))\n                if bc == 0\n                    ρxP = Q[vidP, _ρ, 1, eP]\n                    ρyP = Q[vidP, _ρ, 2, eP]\n                    ρzP = Q[vidP, _ρ, 3, eP]\n                    UxP = Q[vidP, _U, 1, eP]\n                    UyP = Q[vidP, _U, 2, eP]\n                    UzP = Q[vidP, _U, 3, eP]\n                    VxP = Q[vidP, _V, 1, eP]\n                    VyP = Q[vidP, _V, 2, eP]\n                    VzP = Q[vidP, _V, 3, eP]\n                    WxP = Q[vidP, _W, 1, eP]\n                    WyP = Q[vidP, _W, 2, eP]\n                    WzP = Q[vidP, _W, 3, eP]\n                    ExP = Q[vidP, _E, 1, eP]\n                    EyP = Q[vidP, _E, 2, eP]\n                    EzP = Q[vidP, _E, 3, eP]\n                elseif bc == 1\n                    ρnM = nxM * ρxM + nyM * ρyM + nzM * ρzM\n                    ρxP = ρxM - 2 * ρnM * nxM\n                    ρyP = ρyM - 2 * ρnM * nyM\n                    ρzP = ρzM - 2 * ρnM * nzM\n                    UnM = nxM * UxM + nyM * UyM + nzM * UzM\n                    UxP = UxM - 2 * UnM * nxM\n                    UyP = UyM - 2 * UnM * nyM\n                    UzP = UzM - 2 * UnM * nzM\n                    VnM = nxM * VxM + nyM * VyM + nzM * VzM\n                    VxP = VxM - 2 * VnM * nxM\n                    VyP = VyM - 2 * VnM * nyM\n                    VzP = VzM - 2 * VnM * nzM\n                    WnM = nxM * WxM + nyM * WyM + nzM * WzM\n                    WxP = WxM - 2 * WnM * nxM\n                    WyP = WyM - 2 * WnM * nyM\n                    WzP = WzM - 2 * WnM * nzM\n                    EnM = nxM * ExM + nyM * EyM + nzM * EzM\n                    ExP = ExM - 2 * EnM * nxM\n                    EyP = EyM - 2 * EnM * nyM\n                    EzP = EzM - 2 * EnM * nzM\n                else\n                    error(\"Invalid boundary conditions $bc on face $f of element $e\")\n                end\n\n                #Left Fluxes\n                fluxρM_x = ρxM\n                fluxρM_y = ρyM\n                fluxρM_z = ρzM\n                fluxUM_x = UxM\n                fluxUM_y = UyM\n                fluxUM_z = UzM\n                fluxVM_x = VxM\n                fluxVM_y = VyM\n                fluxVM_z = VzM\n                fluxWM_x = WxM\n                fluxWM_y = WyM\n                fluxWM_z = WzM\n                fluxEM_x = ExM\n                fluxEM_y = EyM\n                fluxEM_z = EzM\n\n                #Right Fluxes\n                fluxρP_x = ρxP\n                fluxρP_y = ρyP\n                fluxρP_z = ρzP\n                fluxUP_x = UxP\n                fluxUP_y = UyP\n                fluxUP_z = UzP\n                fluxVP_x = VxP\n                fluxVP_y = VyP\n                fluxVP_z = VzP\n                fluxWP_x = WxP\n                fluxWP_y = WyP\n                fluxWP_z = WzP\n                fluxEP_x = ExP\n                fluxEP_y = EyP\n                fluxEP_z = EzP\n\n                #Compute Numerical Flux\n                fluxρS = 0.5*(nxM * (fluxρM_x + fluxρP_x) + nyM * (fluxρM_y + fluxρP_y) + nzM * (fluxρM_z + fluxρP_z))\n                fluxUS = 0.5*(nxM * (fluxUM_x + fluxUP_x) + nyM * (fluxUM_y + fluxUP_y) + nzM * (fluxUM_z + fluxUP_z))\n                fluxVS = 0.5*(nxM * (fluxVM_x + fluxVP_x) + nyM * (fluxVM_y + fluxVP_y) + nzM * (fluxVM_z + fluxVP_z))\n                fluxWS = 0.5*(nxM * (fluxWM_x + fluxWP_x) + nyM * (fluxWM_y + fluxWP_y) + nzM * (fluxWM_z + fluxWP_z))\n                fluxES = 0.5*(nxM * (fluxEM_x + fluxEP_x) + nyM * (fluxEM_y + fluxEP_y) + nzM * (fluxEM_z + fluxEP_z))\n\n                #Update RHS\n                rhs[vidM, _ρ, 1, eM] += sMJ * fluxρS\n                rhs[vidM, _U, 1, eM] += sMJ * fluxUS\n                rhs[vidM, _V, 1, eM] += sMJ * fluxVS\n                rhs[vidM, _W, 1, eM] += sMJ * fluxWS\n                rhs[vidM, _E, 1, eM] += sMJ * fluxES\n            end\n        end\n    end\nend}}}{{{ Update grad Q solutionfunction update_gradQ!(::Val{dim}, ::Val{N}, Q, rhs, vgeo, elems) where {dim, N}\n\n    Nq=(N+1)^dim\n\n    @inbounds for e = elems, s = 1:_nstate, i = 1:Nq\n        Q[i, s, 1, e] = rhs[i, s, 1, e] * vgeo[i, _MJI, e] /π\n        Q[i, s, 2, e] = rhs[i, s, 2, e] * vgeo[i, _MJI, e] /π\n        Q[i, s, 3, e] = rhs[i, s, 3, e] * vgeo[i, _MJI, e] /π\n    end\n\nend}}}{{{ Update grad Q solutionfunction update_divgradQ!(::Val{dim}, ::Val{N}, Q, rhs, vgeo, elems) where {dim, N}\n\n    Nq=(N+1)^dim\n\n    @inbounds for e = elems, s = 1:_nstate, i = 1:Nq\n        Q[i, s, e] = rhs[i, s, 1, e] * vgeo[i, _MJI, e] /(3*π)\n    end\n\nend}}}{{{ Update solution (for all dimensions)function updatesolution!(::Val{dim}, ::Val{N}, rhs::Array, rhs_gradQ, Q, vgeo, elems, rka,\n                         rkb, dt, visc) where {dim, N}\n\n    Nq=(N+1)^dim\n\n    @inbounds for e = elems, s = 1:_nstate, i = 1:Nq\n        rhs[i, s, e] += visc*rhs_gradQ[i,s,1,e]\n        Q[i, s, e] += rkb * dt * rhs[i, s, e] * vgeo[i, _MJI, e]\n        rhs[i, s, e] *= rka\n    end\nend}}} }}}{{{ improved GPU kernles{{{ Volume RHS for 3D@hascuda function knl_volumerhs!(::Val{3}, ::Val{N}, rhs, Q, vgeo, D, nelem) where N\n    DFloat = eltype(D)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Nq = N + 1\n\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    s_D = @cuStaticSharedMem(eltype(D), (Nq, Nq))\n    s_F = @cuStaticSharedMem(eltype(Q), (Nq, Nq, Nq, _nstate))\n    s_G = @cuStaticSharedMem(eltype(Q), (Nq, Nq, Nq, _nstate))\n    s_H = @cuStaticSharedMem(eltype(Q), (Nq, Nq, Nq, _nstate))\n\n    rhsU = rhsV = rhsW = rhsρ = rhsE = zero(eltype(rhs))\n    @inbounds if i <= Nq && j <= Nq && k <= Nq && e <= nelemLoad derivative into shared memory        if k == 1\n            s_D[i, j] = D[i, j]\n        endLoad values will need into registers        MJ = vgeo[i, j, k, _MJ, e]\n        ξx, ξy, ξz = vgeo[i,j,k,_ξx,e], vgeo[i,j,k,_ξy,e], vgeo[i,j,k,_ξz,e]\n        ηx, ηy, ηz = vgeo[i,j,k,_ηx,e], vgeo[i,j,k,_ηy,e], vgeo[i,j,k,_ηz,e]\n        ζx, ζy, ζz = vgeo[i,j,k,_ζx,e], vgeo[i,j,k,_ζy,e], vgeo[i,j,k,_ζz,e]\n\n        U, V, W = Q[i, j, k, _U, e], Q[i, j, k, _V, e], Q[i, j, k, _W, e]\n        ρ, E = Q[i, j, k, _ρ, e], Q[i, j, k, _E, e]\n\n        P = p0 * CUDAnative.pow(R_gas * E / p0, c_p / c_v)\n\n        ρinv = 1 / ρ\n        fluxρ_x = U\n        fluxU_x = ρinv * U * U + P\n        fluxV_x = ρinv * V * U\n        fluxW_x = ρinv * W * U\n        fluxE_x = E * ρinv * U\n\n        fluxρ_y = V\n        fluxU_y = ρinv * U * V\n        fluxV_y = ρinv * V * V + P\n        fluxW_y = ρinv * W * V\n        fluxE_y = E * ρinv * V\n\n        fluxρ_z = W\n        fluxU_z = ρinv * U * W\n        fluxV_z = ρinv * V * W\n        fluxW_z = ρinv * W * W + P\n        fluxE_z = E * ρinv * W\n\n        s_F[i, j, k, _ρ] = MJ * (ξx * fluxρ_x + ξy * fluxρ_y + ξz * fluxρ_z)\n        s_F[i, j, k, _U] = MJ * (ξx * fluxU_x + ξy * fluxU_y + ξz * fluxU_z)\n        s_F[i, j, k, _V] = MJ * (ξx * fluxV_x + ξy * fluxV_y + ξz * fluxV_z)\n        s_F[i, j, k, _W] = MJ * (ξx * fluxW_x + ξy * fluxW_y + ξz * fluxW_z)\n        s_F[i, j, k, _E] = MJ * (ξx * fluxE_x + ξy * fluxE_y + ξz * fluxE_z)\n\n        s_G[i, j, k, _ρ] = MJ * (ηx * fluxρ_x + ηy * fluxρ_y + ηz * fluxρ_z)\n        s_G[i, j, k, _U] = MJ * (ηx * fluxU_x + ηy * fluxU_y + ηz * fluxU_z)\n        s_G[i, j, k, _V] = MJ * (ηx * fluxV_x + ηy * fluxV_y + ηz * fluxV_z)\n        s_G[i, j, k, _W] = MJ * (ηx * fluxW_x + ηy * fluxW_y + ηz * fluxW_z)\n        s_G[i, j, k, _E] = MJ * (ηx * fluxE_x + ηy * fluxE_y + ηz * fluxE_z)\n\n        s_H[i, j, k, _ρ] = MJ * (ζx * fluxρ_x + ζy * fluxρ_y + ζz * fluxρ_z)\n        s_H[i, j, k, _U] = MJ * (ζx * fluxU_x + ζy * fluxU_y + ζz * fluxU_z)\n        s_H[i, j, k, _V] = MJ * (ζx * fluxV_x + ζy * fluxV_y + ζz * fluxV_z)\n        s_H[i, j, k, _W] = MJ * (ζx * fluxW_x + ζy * fluxW_y + ζz * fluxW_z)\n        s_H[i, j, k, _E] = MJ * (ζx * fluxE_x + ζy * fluxE_y + ζz * fluxE_z)\n\n        rhsU, rhsV, rhsW = (rhs[i, j, k, _U, e],\n                            rhs[i, j, k, _V, e],\n                            rhs[i, j, k, _W, e])\n        rhsρ, rhsE = rhs[i, j, k, _ρ, e], rhs[i, j, k, _E, e]buoyancy term        rhsW -= MJ * ρ * gravity\n    end\n\n    sync_threads()\n\n    @inbounds if i <= Nq && j <= Nq && k <= Nq && e <= nelemloop of ξ-grid lines        for n = 1:Nq\n            Dni = s_D[n, i]\n            Dnj = s_D[n, j]\n            Dnk = s_D[n, k]\n\n            rhsρ += Dni * s_F[n, j, k, _ρ]\n            rhsρ += Dnj * s_G[i, n, k, _ρ]\n            rhsρ += Dnk * s_H[i, j, n, _ρ]\n\n            rhsU += Dni * s_F[n, j, k, _U]\n            rhsU += Dnj * s_G[i, n, k, _U]\n            rhsU += Dnk * s_H[i, j, n, _U]\n\n            rhsV += Dni * s_F[n, j, k, _V]\n            rhsV += Dnj * s_G[i, n, k, _V]\n            rhsV += Dnk * s_H[i, j, n, _V]\n\n            rhsW += Dni * s_F[n, j, k, _W]\n            rhsW += Dnj * s_G[i, n, k, _W]\n            rhsW += Dnk * s_H[i, j, n, _W]\n\n            rhsE += Dni * s_F[n, j, k, _E]\n            rhsE += Dnj * s_G[i, n, k, _E]\n            rhsE += Dnk * s_H[i, j, n, _E]\n        end\n\n        rhs[i, j, k, _U, e] = rhsU\n        rhs[i, j, k, _V, e] = rhsV\n        rhs[i, j, k, _W, e] = rhsW\n        rhs[i, j, k, _ρ, e] = rhsρ\n        rhs[i, j, k, _E, e] = rhsE\n    end\n    nothing\nend}}}{{{ Face RHS (all dimensions)@hascuda function knl_fluxrhs!(::Val{dim}, ::Val{N}, rhs, Q, sgeo, vgeo, nelem, vmapM,\n                               vmapP, elemtobndy) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    if dim == 1\n        Np = (N+1)\n        nface = 2\n    elseif dim == 2\n        Np = (N+1) * (N+1)\n        nface = 4\n    elseif dim == 3\n        Np = (N+1) * (N+1) * (N+1)\n        nface = 6\n    end\n\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    Nq = N+1\n    half = convert(eltype(Q), 0.5)\n\n    @inbounds if i <= Nq && j <= Nq && k == 1 && e <= nelem\n        n = i + (j-1) * Nq\n        for lf = 1:2:nface\n            for f = lf:lf+1\n                (nxM, nyM) = (sgeo[_nx, n, f, e], sgeo[_ny, n, f, e])\n                (nzM, sMJ) = (sgeo[_nz, n, f, e], sgeo[_sMJ, n, f, e])\n\n                (idM, idP) = (vmapM[n, f, e], vmapP[n, f, e])\n\n                (eM, eP) = (e, ((idP - 1) ÷ Np) + 1)\n                (vidM, vidP) = (((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1)\n\n                ρM = Q[vidM, _ρ, eM]\n                UM = Q[vidM, _U, eM]\n                VM = Q[vidM, _V, eM]\n                WM = Q[vidM, _W, eM]\n                EM = Q[vidM, _E, eM]\n\n                bc = elemtobndy[f, e]\n                PM = p0 * CUDAnative.pow(R_gas * EM / p0, c_p / c_v)\n                ρP = UP = VP = WP = EP = PP = zero(eltype(Q))\n                if bc == 0\n                    ρP = Q[vidP, _ρ, eP]\n                    UP = Q[vidP, _U, eP]\n                    VP = Q[vidP, _V, eP]\n                    WP = Q[vidP, _W, eP]\n                    EP = Q[vidP, _E, eP]\n                    PP = p0 * CUDAnative.pow(R_gas * EP / p0, c_p / c_v)\n                elseif bc == 1\n                    UnM = nxM * UM + nyM * VM + nzM * WM\n                    UP = UM - 2 * UnM * nxM\n                    VP = VM - 2 * UnM * nyM\n                    WP = WM - 2 * UnM * nzM\n                    ρP = ρM\n                    EP = EM\n                    PP = PM\n                end\n\n                ρMinv = 1 / ρM\n                fluxρM_x = UM\n                fluxUM_x = ρMinv * UM * UM + PM\n                fluxVM_x = ρMinv * VM * UM\n                fluxWM_x = ρMinv * WM * UM\n                fluxEM_x = ρMinv * UM * EM\n\n                fluxρM_y = VM\n                fluxUM_y = ρMinv * UM * VM\n                fluxVM_y = ρMinv * VM * VM + PM\n                fluxWM_y = ρMinv * WM * VM\n                fluxEM_y = ρMinv * VM * EM\n\n                fluxρM_z = WM\n                fluxUM_z = ρMinv * UM * WM\n                fluxVM_z = ρMinv * VM * WM\n                fluxWM_z = ρMinv * WM * WM + PM\n                fluxEM_z = ρMinv * WM * EM\n\n                ρPinv = 1 / ρP\n                fluxρP_x = UP\n                fluxUP_x = ρPinv * UP * UP + PP\n                fluxVP_x = ρPinv * VP * UP\n                fluxWP_x = ρPinv * WP * UP\n                fluxEP_x = ρPinv * UP * EP\n\n                fluxρP_y = VP\n                fluxUP_y = ρPinv * UP * VP\n                fluxVP_y = ρPinv * VP * VP + PP\n                fluxWP_y = ρPinv * WP * VP\n                fluxEP_y = ρPinv * VP * EP\n\n                fluxρP_z = WP\n                fluxUP_z = ρPinv * UP * WP\n                fluxVP_z = ρPinv * VP * WP\n                fluxWP_z = ρPinv * WP * WP + PP\n                fluxEP_z = ρPinv * WP * EP\n\n                λM = ρMinv * abs(nxM * UM + nyM * VM + nzM * WM) + CUDAnative.sqrt(ρMinv * γ * PM)\n                λP = ρPinv * abs(nxM * UP + nyM * VP + nzM * WP) + CUDAnative.sqrt(ρPinv * γ * PP)\n                λ  =  max(λM, λP)\n\n                #Compute Numerical Flux and Update\n                fluxρS = (nxM * (fluxρM_x + fluxρP_x) + nyM * (fluxρM_y + fluxρP_y) +\n                          nzM * (fluxρM_z + fluxρP_z) - λ * (ρP - ρM)) / 2\n                fluxUS = (nxM * (fluxUM_x + fluxUP_x) + nyM * (fluxUM_y + fluxUP_y) +\n                          nzM * (fluxUM_z + fluxUP_z) - λ * (UP - UM)) / 2\n                fluxVS = (nxM * (fluxVM_x + fluxVP_x) + nyM * (fluxVM_y + fluxVP_y) +\n                          nzM * (fluxVM_z + fluxVP_z) - λ * (VP - VM)) / 2\n                fluxWS = (nxM * (fluxWM_x + fluxWP_x) + nyM * (fluxWM_y + fluxWP_y) +\n                          nzM * (fluxWM_z + fluxWP_z) - λ * (WP - WM)) / 2\n                fluxES = (nxM * (fluxEM_x + fluxEP_x) + nyM * (fluxEM_y + fluxEP_y) +\n                          nzM * (fluxEM_z + fluxEP_z) - λ * (EP - EM)) / 2\n\n                #Update RHS\n                rhs[vidM, _ρ, eM] -= sMJ * fluxρS\n                rhs[vidM, _U, eM] -= sMJ * fluxUS\n                rhs[vidM, _V, eM] -= sMJ * fluxVS\n                rhs[vidM, _W, eM] -= sMJ * fluxWS\n                rhs[vidM, _E, eM] -= sMJ * fluxES\n            end\n            sync_threads()\n        end\n    end\nnothing\nend}}}{{{ Update solution (for all dimensions)@hascuda function knl_updatesolution!(::Val{dim}, ::Val{N}, rhs, Q, vgeo, nelem, rka,\n                                      rkb, dt) where {dim, N}\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    Nq = N+1\n    @inbounds if i <= Nq && j <= Nq && k <= Nq && e <= nelem\n        n = i + (j-1) * Nq + (k-1) * Nq * Nq\n        MJI = vgeo[n, _MJI, e]\n        for s = 1:_nstate\n            Q[n, s, e] += rkb * dt * rhs[n, s, e] * MJI\n            rhs[n, s, e] *= rka\n        end\n    end\n    nothing\nend}}}}}}{{{ Fill sendQ on device with Q (for all dimensions)@hascuda function knl_fillsendQ!(::Val{dim}, ::Val{N}, sendQ, Q,\n                                 sendelems) where {N, dim}\n    Nq = N + 1\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    @inbounds if i <= Nq && j <= Nq && k <= Nq && e <= length(sendelems)\n        n = i + (j-1) * Nq + (k-1) * Nq * Nq\n        re = sendelems[e]\n        for s = 1:_nstate\n            sendQ[n, s, e] = Q[n, s, re]\n        end\n    end\n    nothing\nend}}}{{{ Fill Q on device with recvQ (for all dimensions)@hascuda function knl_transferrecvQ!(::Val{dim}, ::Val{N}, Q, recvQ, nelem,\n                                     nrealelem) where {N, dim}\n    Nq = N + 1\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    @inbounds if i <= Nq && j <= Nq && k <= Nq && e <= nelem\n        n = i + (j-1) * Nq + (k-1) * Nq * Nq\n        for s = 1:_nstate\n            Q[n, s, nrealelem + e] = recvQ[n, s, e]\n        end\n    end\n    nothing\nend}}}{{{ MPI Buffer handlingfunction fillsendQ!(::Val{dim}, ::Val{N}, sendQ, d_sendQ::Array, Q,\n                    sendelems) where {dim, N}\n    sendQ[:, :, :] .= Q[:, :, sendelems]\nend\n\n@hascuda function fillsendQ!(::Val{dim}, ::Val{N}, sendQ, d_sendQ::CuArray,\n                             d_QL, d_sendelems) where {dim, N}\n    nsendelem = length(d_sendelems)\n    if nsendelem > 0\n        @cuda(threads=ntuple(j->N+1, dim), blocks=nsendelem,\n              knl_fillsendQ!(Val(dim), Val(N), d_sendQ, d_QL, d_sendelems))\n        sendQ .= d_sendQ\n    end\nend\n\n@hascuda function transferrecvQ!(::Val{dim}, ::Val{N}, d_recvQ::CuArray, recvQ,\n                                 d_QL, nrealelem) where {dim, N}\n    nrecvelem = size(recvQ)[end]\n    if nrecvelem > 0\n        d_recvQ .= recvQ\n        @cuda(threads=ntuple(j->N+1, dim), blocks=nrecvelem,\n              knl_transferrecvQ!(Val(dim), Val(N), d_QL, d_recvQ, nrecvelem,\n                                 nrealelem))\n    end\nend\n\nfunction transferrecvQ!(::Val{dim}, ::Val{N}, d_recvQ::Array, recvQ, Q,\n                        nrealelem) where {dim, N}\n    Q[:, :, nrealelem+1:end] .= recvQ[:, :, :]\nend}}}{{{ GPU kernel wrappers@hascuda function volumerhs!(::Val{dim}, ::Val{N}, d_rhsC::CuArray, d_QC,\n                             d_vgeoC, d_D, elems) where {dim, N}\n    nelem = length(elems)\n    @cuda(threads=ntuple(j->N+1, dim), blocks=nelem,\n          knl_volumerhs!(Val(dim), Val(N), d_rhsC, d_QC, d_vgeoC, d_D, nelem))\nend\n\n@hascuda function fluxrhs!(::Val{dim}, ::Val{N}, d_rhsL::CuArray, d_QL, d_sgeo,\n                           d_vgeoL, elems, d_vmapM, d_vmapP, d_elemtobndy) where {dim, N}\n    nelem = length(elems)\n    @cuda(threads=(ntuple(j->N+1, dim-1)..., 1), blocks=nelem,\n          knl_fluxrhs!(Val(dim), Val(N), d_rhsL, d_QL, d_sgeo, d_vgeoL, nelem, d_vmapM,\n                       d_vmapP, d_elemtobndy))\nend\n\n@hascuda function updatesolution!(::Val{dim}, ::Val{N}, d_rhsL::CuArray, d_QL,\n                                  d_vgeoL, elems, rka, rkb, dt) where {dim, N}\n    nelem = length(elems)\n    @cuda(threads=ntuple(j->N+1, dim), blocks=nelem,\n          knl_updatesolution!(Val(dim), Val(N), d_rhsL, d_QL, d_vgeoL, nelem, rka,\n                              rkb, dt))\nend}}}{{{ L2 Error (for all dimensions)function L2errorsquared(::Val{dim}, ::Val{N}, Q, vgeo, elems, Qex, t) where {dim, N}\n    DFloat = eltype(Q)\n    Np = (N+1)^dim\n    err = zero(DFloat)\n\n    @inbounds for e = elems, s=1:_nstate, i = 1:Np\n        diff = Q[i, s, e] - Qex[i, s, e]\n        err += vgeo[i, _MJ, e] * diff^2\n    end\n\n    err\nend}}}{{{ L2 Energy (for all dimensions)function L2energysquared(::Val{dim}, ::Val{N}, Q, vgeo, elems) where {dim, N}\n    DFloat = eltype(Q)\n    Np = (N+1)^dim\n    (~, nstate, nelem) = size(Q)\n    energy = zero(DFloat)\n\n    @inbounds for e = elems, s=1:_nstate, i = 1:Np\n        energy += vgeo[i, _MJ, e] * Q[i, s, e]^2\n    end\n\n    energy\nend}}}{{{ Send Data Qfunction senddata_Q(::Val{dim}, ::Val{N}, mesh, sendreq, recvreq, sendQ,\n                  recvQ, d_sendelems, d_sendQ, d_recvQ, d_QL, mpicomm;\n                  ArrType=ArrType) where {dim, N}\n    mpirank = MPI.Comm_rank(mpicomm)Create send and recv request array    nnabr = length(mesh.nabrtorank)\n    d_sendelems = ArrType(mesh.sendelems)\n    nrealelem = length(mesh.realelems)post MPI receives    for n = 1:nnabr\n        recvreq[n] = MPI.Irecv!((@view recvQ[:, :, mesh.nabrtorecv[n]]),\n                                mesh.nabrtorank[n], 777, mpicomm)\n    endwait on (prior) MPI sends    MPI.Waitall!(sendreq)pack data from dQL into send buffer    fillsendQ!(Val(dim), Val(N), sendQ, dsendQ, dQL, dsendelems)    sendQ[:, :, :] .= d_QL[:, :, d_sendelems]post MPI sends    for n = 1:nnabr\n        sendreq[n] = MPI.Isend((@view sendQ[:, :, mesh.nabrtosend[n]]),\n                               mesh.nabrtorank[n], 777, mpicomm)\n    end\nend}}}{{{ Send Data Grad(Q)function senddata_gradQ(::Val{dim}, ::Val{N}, mesh, sendreq, recvreq, sendQ,\n                  recvQ, d_sendelems, d_sendQ, d_recvQ, d_QL, mpicomm;\n                  ArrType=ArrType) where {dim, N}\n    mpirank = MPI.Comm_rank(mpicomm)Create send and recv request array    nnabr = length(mesh.nabrtorank)\n    d_sendelems = ArrType(mesh.sendelems)\n    nrealelem = length(mesh.realelems)post MPI receives    for n = 1:nnabr\n        recvreq[n] = MPI.Irecv!((@view recvQ[:, :, :, mesh.nabrtorecv[n]]),\n                                mesh.nabrtorank[n], 777, mpicomm)\n    endwait on (prior) MPI sends    MPI.Waitall!(sendreq)pack data from dQL into send buffer    fillsendQ!(Val(dim), Val(N), sendQ, dsendQ, dQL, dsendelems)    sendQ[:, :, :, :] .= d_QL[:, :, :, d_sendelems]post MPI sends    for n = 1:nnabr\n        sendreq[n] = MPI.Isend((@view sendQ[:, :, :, mesh.nabrtosend[n]]),\n                               mesh.nabrtorank[n], 777, mpicomm)\n    end\nend}}}{{{ Receive Data Qfunction receivedata_Q!(::Val{dim}, ::Val{N}, mesh, recvreq, recvQ,\n                        d_recvQ, d_QL) where {dim, N}\n    nrealelem = length(mesh.realelems)wait on MPI receives    MPI.Waitall!(recvreq)copy data to state vector d_QL    #transferrecvQ!(Val(dim), Val(N), d_recvQ, recvQ, d_QL, nrealelem)\n    d_QL[:, :, nrealelem+1:end] .= recvQ[:, :, :]\n\nend}}}{{{ Receive Data Grad(Q)function receivedata_gradQ!(::Val{dim}, ::Val{N}, mesh, recvreq, recvQ,\n                            d_recvQ, d_QL) where {dim, N}\n    nrealelem = length(mesh.realelems)wait on MPI receives    MPI.Waitall!(recvreq)copy data to state vector d_QL    #transferrecvQ!(Val(dim), Val(N), d_recvQ, recvQ, d_QL, nrealelem)\n    d_QL[:, :, :, nrealelem+1:end] .= recvQ[:, :, :, :]\nend}}}{{{ RK loopfunction lowstorageRK(::Val{dim}, ::Val{N}, mesh, vgeo, sgeo, Q, rhs, D,\n                      dt, nsteps, tout, vmapM, vmapP, mpicomm, iplot, visc;\n                      ArrType=ArrType, plotstep=0) where {dim, N}\n    DFloat = eltype(Q)\n    mpirank = MPI.Comm_rank(mpicomm)Fourth-order, low-storage, Runge–Kutta scheme of Carpenter and Kennedy (1994) ((5,4) 2N-Storage RK scheme.Ref: @TECHREPORT{CarpenterKennedy1994,   author = {M.~H. Carpenter and C.~A. Kennedy},   title = {Fourth-order {2N-storage} {Runge-Kutta} schemes},   institution = {National Aeronautics and Space Administration},   year = {1994},   number = {NASA TM-109112},   address = {Langley Research Center, Hampton, VA}, }    RKA = (DFloat(0),\n           DFloat(-567301805773)  / DFloat(1357537059087),\n           DFloat(-2404267990393) / DFloat(2016746695238),\n           DFloat(-3550918686646) / DFloat(2091501179385),\n           DFloat(-1275806237668) / DFloat(842570457699 ))\n\n    RKB = (DFloat(1432997174477) / DFloat(9575080441755 ),\n           DFloat(5161836677717) / DFloat(13612068292357),\n           DFloat(1720146321549) / DFloat(2090206949498 ),\n           DFloat(3134564353537) / DFloat(4481467310338 ),\n           DFloat(2277821191437) / DFloat(14882151754819))\n\n    RKC = (DFloat(0),\n           DFloat(1432997174477) / DFloat(9575080441755),\n           DFloat(2526269341429) / DFloat(6820363962896),\n           DFloat(2006345519317) / DFloat(3224310063776),\n           DFloat(2802321613138) / DFloat(2924317926251))Create send and recv request array    nnabr = length(mesh.nabrtorank)\n    sendreq = fill(MPI.REQUEST_NULL, nnabr)\n    recvreq = fill(MPI.REQUEST_NULL, nnabr)Create send and recv buffer    sendQ = zeros(DFloat, (N+1)^dim, size(Q,2), length(mesh.sendelems))\n    recvQ = zeros(DFloat, (N+1)^dim, size(Q,2), length(mesh.ghostelems))Create send and recv LDG buffer    sendgradQ = zeros(DFloat, (N+1)^dim, size(Q,2), dim, length(mesh.sendelems))\n    recvgradQ = zeros(DFloat, (N+1)^dim, size(Q,2), dim, length(mesh.ghostelems))Store Constants    nrealelem = length(mesh.realelems)\n    nsendelem = length(mesh.sendelems)\n    nrecvelem = length(mesh.ghostelems)\n    nelem = length(mesh.elems)\n\n    #Create Device Arrays\n    d_QL, d_rhsL = ArrType(Q), ArrType(rhs)\n    d_vgeoL, d_sgeo = ArrType(vgeo), ArrType(sgeo)\n    d_vmapM, d_vmapP = ArrType(vmapM), ArrType(vmapP)\n    d_sendelems, d_elemtobndy = ArrType(mesh.sendelems), ArrType(mesh.elemtobndy)\n    d_sendQ, d_recvQ = ArrType(sendQ), ArrType(recvQ)\n    d_D = ArrType(D)\n    #Create Device LDG Arrays\n    d_gradQL = zeros(DFloat, (N+1)^dim, _nstate, dim, nelem)\n    d_rhs_gradQL = zeros(DFloat, (N+1)^dim, _nstate, dim, nelem)\n    d_sendgradQ, d_recvgradQ = ArrType(sendgradQ), ArrType(recvgradQ)\n\n    #Template Reshape Arrays\n    Qshape    = (fill(N+1, dim)..., size(Q, 2), size(Q, 3))\n    vgeoshape = (fill(N+1, dim)..., _nvgeo, size(Q, 3))\n    gradQshape = (fill(N+1, dim)..., size(d_gradQL,2), size(d_gradQL,3), size(d_gradQL,4))\n\n    #Reshape Device Arrays\n    d_QC = reshape(d_QL, Qshape)\n    d_rhsC = reshape(d_rhsL, Qshape...)\n    d_vgeoC = reshape(d_vgeoL, vgeoshape)\n    #Reshape Device LDG Arrays\n    d_gradQC = reshape(d_gradQL, gradQshape)\n    d_rhs_gradQC = reshape(d_rhs_gradQL, gradQshape...)Send Data Q    senddata_Q(Val(dim), Val(N), mesh, sendreq, recvreq, sendQ,\n               recvQ, d_sendelems, d_sendQ, d_recvQ, d_QL, mpicomm;\n               ArrType=ArrType)Receive Data Q    receivedata_Q!(Val(dim), Val(N), mesh, recvreq, recvQ, d_recvQ, d_QL)volume grad Q computation    volume_grad!(Val(dim), Val(N), d_rhs_gradQC, d_QC, d_vgeoC, d_D, mesh.realelems)flux grad Q computation    flux_grad!(Val(dim), Val(N), d_rhs_gradQL, d_QL, d_sgeo, mesh.realelems, d_vmapM, d_vmapP, d_elemtobndy)Construct grad Q    update_gradQ!(Val(dim), Val(N), d_gradQL, d_rhs_gradQL, d_vgeoL, mesh.realelems)Send Data grad(Q)    senddata_gradQ(Val(dim), Val(N), mesh, sendreq, recvreq, sendgradQ,\n                   recvgradQ, d_sendelems, d_sendgradQ, d_recvgradQ,\n                   d_gradQL, mpicomm;ArrType=ArrType)volume div(grad Q) computation    volume_div!(Val(dim), Val(N), d_rhs_gradQC, d_gradQC, d_vgeoC, d_D, mesh.realelems)Receive Data grad(Q)    receivedata_gradQ!(Val(dim), Val(N), mesh, recvreq, recvgradQ, d_recvgradQ, d_gradQL)flux div(grad Q) computation    flux_div!(Val(dim), Val(N), d_rhs_gradQL, d_gradQL, d_sgeo, mesh.realelems, d_vmapM, d_vmapP, d_elemtobndy)Construct grad Q    update_divgradQ!(Val(dim), Val(N), d_QL, d_rhs_gradQL, d_vgeoL, mesh.realelems)\n\n    #Q[:,:,:]= d_gradQL[:,:,3,:] #1st derivative\n    Q .= d_QL #2nd derivative\nend}}}{{{ convert_variablesfunction convert_set2nc_to_set2c(::Val{dim}, ::Val{N}, vgeo, Q) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1)^dim\n    (~, ~, nelem) = size(Q)\n\n    println(\"[CPU] converting variables (CPU)...\")\n    @inbounds for e = 1:nelem, n = 1:Np\n        ρ, u, v, w, E = Q[n, _ρ, e], Q[n, _U, e], Q[n, _V, e], Q[n, _W, e], Q[n, _E, e]\n        Q[n, _U, e] = ρ*u\n        Q[n, _V, e] = ρ*v\n        Q[n, _W, e] = ρ*w\n        Q[n, _E, e] = ρ*E\n    end\nend}}}function convert_set2c_to_set2nc(::Val{dim}, ::Val{N}, vgeo, Q) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1)^dim\n    (~, ~, nelem) = size(Q)\n\n    @inbounds for e = 1:nelem, n = 1:Np\n        ρ, U, V, W, E = Q[n, _ρ, e], Q[n, _U, e], Q[n, _V, e], Q[n, _W, e], Q[n, _E, e]\n        u=U/ρ\n        v=V/ρ\n        w=W/ρ\n        E=E/ρ\n        Q[n, _U, e] = u\n        Q[n, _V, e] = v\n        Q[n, _W, e] = w\n        Q[n, _E, e] = E\n    end\nend\n\nfunction convert_set2nc_to_set3c(::Val{dim}, ::Val{N}, vgeo, Q) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1)^dim\n    (~, ~, nelem) = size(Q)\n\n    @inbounds for e = 1:nelem, n = 1:Np\n        ρ, u, v, w, E = Q[n, _ρ, e], Q[n, _U, e], Q[n, _V, e], Q[n, _W, e], Q[n, _E, e]\n        z = vgeo[n, _z, e]\n        P = p0 * (ρ * R_gas * E / p0)^(c_p / c_v)\n        T = P/(ρ*R_gas)\n        E = c_v*T + 0.5*(u^2 + v^2 + w^2) + gravity*z\n        Q[n, _U, e] = ρ*u\n        Q[n, _V, e] = ρ*v\n        Q[n, _W, e] = ρ*w\n        Q[n, _E, e] = ρ*E\n    end\nend\n\nfunction convert_set3c_to_set2nc(::Val{dim}, ::Val{N}, vgeo, Q) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1)^dim\n    (~, ~, nelem) = size(Q)\n\n    @inbounds for e = 1:nelem, n = 1:Np\n        ρ, U, V, W, E = Q[n, _ρ, e], Q[n, _U, e], Q[n, _V, e], Q[n, _W, e], Q[n, _E, e]\n        z = vgeo[n, _z, e]\n        u=U/ρ\n        v=V/ρ\n        w=W/ρ\n        E=E/ρ\n        P = (R_gas/c_v)*ρ*(E - 0.5*(u^2 + v^2 + w^2) - gravity*z)\n        E=p0/(ρ * R_gas)*( P/p0 )^(c_v/c_p)\n        Q[n, _U, e] = u\n        Q[n, _V, e] = v\n        Q[n, _W, e] = w\n        Q[n, _E, e] = E\n    end\nend\n\nfunction convert_set2nc_to_set4c(::Val{dim}, ::Val{N}, vgeo, Q) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1)^dim\n    (~, ~, nelem) = size(Q)\n\n    @inbounds for e = 1:nelem, n = 1:Np\n        ρ, u, v, w, E = Q[n, _ρ, e], Q[n, _U, e], Q[n, _V, e], Q[n, _W, e], Q[n, _E, e]\n        P = p0 * (ρ * R_gas * E / p0)^(c_p / c_v)\n        T = P/(ρ*R_gas)\n        E = c_v*T\n        Q[n, _U, e] = ρ*u\n        Q[n, _V, e] = ρ*v\n        Q[n, _W, e] = ρ*w\n        Q[n, _E, e] = ρ*E\n    end\nend\n\nfunction convert_set4c_to_set2nc(::Val{dim}, ::Val{N}, vgeo, Q) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1)^dim\n    (~, ~, nelem) = size(Q)\n\n    @inbounds for e = 1:nelem, n = 1:Np\n        ρ, U, V, W, E = Q[n, _ρ, e], Q[n, _U, e], Q[n, _V, e], Q[n, _W, e], Q[n, _E, e]\n        u=U/ρ\n        v=V/ρ\n        w=W/ρ\n        T=E/(ρ*c_v)\n        P=ρ*R_gas*T\n        E=p0/(ρ* R_gas)*(P/p0)^(c_v/c_p)\n        Q[n, _U, e] = u\n        Q[n, _V, e] = v\n        Q[n, _W, e] = w\n        Q[n, _E, e] = E\n    end\nend}}}{{{ LDG driverfunction LDG(::Val{dim}, ::Val{N}, mpicomm, ic, mesh, tend, iplot, visc;\n               meshwarp=(x...)->identity(x),\n               tout = 1, ArrType=Array, plotstep=0) where {dim, N}\n    DFloat = typeof(tend)\n\n    mpirank = MPI.Comm_rank(mpicomm)\n    mpisize = MPI.Comm_size(mpicomm)Partion the mesh using a Hilbert curve based partitioning    mpirank == 0 && println(\"[CPU] partitioning mesh...\")\n    mesh = partition(mpicomm, mesh...)Connect the mesh in parallel    mpirank == 0 && println(\"[CPU] connecting mesh...\")\n    mesh = connectmesh(mpicomm, mesh...)Get the vmaps    mpirank == 0 && println(\"[CPU] computing mappings...\")\n    (vmapM, vmapP) = mappings(N, mesh.elemtoelem, mesh.elemtoface,\n                              mesh.elemtoordr)Create 1-D operators    (ξ, ω) = lglpoints(DFloat, N)\n    D = spectralderivative(ξ)Compute the geometry    mpirank == 0 && println(\"[CPU] computing metrics...\")\n    (vgeo, sgeo) = computegeometry(Val(dim), mesh, D, ξ, ω, meshwarp, vmapM)\n    (nface, nelem) = size(mesh.elemtoelem)Storage for the solution, rhs, and error    mpirank == 0 && println(\"[CPU] creating fields (CPU)...\")\n    Q = zeros(DFloat, (N+1)^dim, _nstate, nelem)\n    Qexact = zeros(DFloat, (N+1)^dim, _nstate, nelem)\n    rhs = zeros(DFloat, (N+1)^dim, _nstate, nelem)setup the initial condition    mpirank == 0 && println(\"[CPU] computing initial conditions (CPU)...\")\n    @inbounds for e = 1:nelem, i = 1:(N+1)^dim\n        x, y, z = vgeo[i, _x, e], vgeo[i, _y, e], vgeo[i, _z, e]\n        h, hexact = ic(x, y, z)\n        Q[i, _ρ, e] = h\n        Q[i, _U, e] = h\n        Q[i, _V, e] = h\n        Q[i, _W, e] = h\n        Q[i, _E, e] = h\n        Qexact[i, _ρ, e] = hexact\n        Qexact[i, _U, e] = hexact\n        Qexact[i, _V, e] = hexact\n        Qexact[i, _W, e] = hexact\n        Qexact[i, _E, e] = hexact\n    endConvert to proper variables    mpirank == 0 && println(\"[CPU] converting variables (CPU)...\")Compute time step    mpirank == 0 && println(\"[CPU] computing dt (CPU)...\")\n    base_dt=0.02\n    Courant= 1.0\n    mpirank == 0 && @show (base_dt, Courant)\n\n    nsteps = ceil(Int64, tend / base_dt)\n    dt = tend / nsteps\n    mpirank == 0 && @show (dt, nsteps, dt * nsteps, tend)Do time stepping    stats = zeros(DFloat, 3)\n    mpirank == 0 && println(\"[CPU] computing initial energy...\")\n    Q_temp=copy(Q)\n    stats[1] = L2energysquared(Val(dim), Val(N), Q_temp, vgeo, mesh.realelems)\n    @show (sqrt.(stats[1]))Write VTK file: plot the initial condition    mkpath(\"viz\")\n    X = ntuple(j->reshape((@view vgeo[:, _x+j-1, :]), ntuple(j->N+1,dim)...,\n                          nelem), dim)\n    ρ = reshape((@view Q_temp[:, _ρ, :]), ntuple(j->(N+1),dim)..., nelem)\n    U = reshape((@view Q_temp[:, _U, :]), ntuple(j->(N+1),dim)..., nelem)\n    V = reshape((@view Q_temp[:, _V, :]), ntuple(j->(N+1),dim)..., nelem)\n    W = reshape((@view Q_temp[:, _W, :]), ntuple(j->(N+1),dim)..., nelem)\n    E = reshape((@view Q_temp[:, _E, :]), ntuple(j->(N+1),dim)..., nelem)\n    writemesh(@sprintf(\"viz/LDG%dD_set3c_%s_rank_%04d_step_%05d\",\n                       dim, ArrType, mpirank, 0), X...;\n              fields=((\"ρ\", ρ), (\"U\", U), (\"V\", V), (\"W\", W), (\"E\", E)),\n              realelems=mesh.realelems)\n\n    mpirank == 0 && println(\"[DEV] starting time stepper...\")\n    lowstorageRK(Val(dim), Val(N), mesh, vgeo, sgeo, Q, rhs, D, dt, nsteps, tout,\n                 vmapM, vmapP, mpicomm, iplot, visc; ArrType=ArrType, plotstep=plotstep)Write VTK: final solution    Q_temp=copy(Q)\n    X = ntuple(j->reshape((@view vgeo[:, _x+j-1, :]), ntuple(j->N+1,dim)...,\n                          nelem), dim)\n    ρ = reshape((@view Q_temp[:, _ρ, :]), ntuple(j->(N+1),dim)..., nelem)\n    U = reshape((@view Q_temp[:, _U, :]), ntuple(j->(N+1),dim)..., nelem)\n    V = reshape((@view Q_temp[:, _V, :]), ntuple(j->(N+1),dim)..., nelem)\n    W = reshape((@view Q_temp[:, _W, :]), ntuple(j->(N+1),dim)..., nelem)\n    E = reshape((@view Q_temp[:, _E, :]), ntuple(j->(N+1),dim)..., nelem)\n    writemesh(@sprintf(\"viz/LDG%dD_set3c_%s_rank_%04d_step_%05d\",\n                       dim, ArrType, mpirank, nsteps), X...;\n              fields=((\"ρ\", ρ), (\"U\", U), (\"V\", V), (\"W\", W), (\"E\", E)),\n              realelems=mesh.realelems)\n\n    mpirank == 0 && println(\"[CPU] computing final energy...\")\n    stats[2] = L2energysquared(Val(dim), Val(N), Q_temp, vgeo, mesh.realelems)\n    stats[3] = L2errorsquared(Val(dim), Val(N), Q_temp, vgeo, mesh.realelems, Qexact,tend)\n\n    stats = sqrt.(MPI.allreduce(stats, MPI.SUM, mpicomm))\n\n    if  mpirank == 0\n        @show eng0 = stats[1]\n        @show engf = stats[2]\n        @show Δeng = engf - eng0\n    end\nend}}}{{{ mainfunction main()\n    DFloat = Float64MPI.Init()    MPI.Initialized() || MPI.Init()\n    MPI.finalize_atexit()\n\n    mpicomm = MPI.COMM_WORLD\n    mpirank = MPI.Comm_rank(mpicomm)\n    mpisize = MPI.Comm_size(mpicomm)FIXME: query via hostname    @hascuda device!(mpirank % length(devices()))\n\n    #Initial Conditions\n    function ic(dim, x...)FIXME: Type generic?        DFloat = eltype(x)\n        γ::DFloat       = _γ\n        p0::DFloat      = _p0\n        R_gas::DFloat   = _R_gas\n        c_p::DFloat     = _c_p\n        c_v::DFloat     = _c_v\n        gravity::DFloat = _gravity\n\n        #=\n        u0 = 0\n        r = sqrt((x[1]-500)^2 + (x[dim]-350)^2 )\n        rc = 250.0\n        θ_ref=300.0\n        θ_c=0.5\n        Δθ=0.0\n        if r <= rc\n            Δθ = 0.5 * θ_c * (1.0 + cos(π * r/rc))\n        end\n        θ_k=θ_ref + Δθ\n        π_k=1.0 - gravity/(c_p*θ_k)*x[dim]\n        c=c_v/R_gas\n        ρ_k=p0/(R_gas*θ_k)*(π_k)^c\n        ρ = ρ_k\n        U = u0\n        V = 0.0\n        W = 0.0\n        E = θ_k\n        ρ, U, V, W, E\n        =#\n\n        #xy-plane\n        h = sin( π*x[1] )*sin( π*x[2] )\n        h_x = cos( π*x[1] )*sin( π*x[2] )\n        h_y = sin( π*x[1] )*cos( π*x[2] )\n        h_xx = -sin( π*x[1] )*sin( π*x[2] )\n        h_yy = -sin( π*x[1] )*sin( π*x[2] )\n\n        #xz-plane\n        h = sin( π*x[1] )*sin( π*x[3] )\n        h_x = cos( π*x[1] )*sin( π*x[3] )\n        h_z = sin( π*x[1] )*cos( π*x[3] )\n        h_xx = -sin( π*x[1] )*sin( π*x[3] )\n        h_zz = -sin( π*x[1] )*sin( π*x[3] )\n\n        #xyz-space\n        h = sin( π*x[1] )*sin( π*x[2] )*sin( π*x[3] )\n        h_x = cos( π*x[1] )*sin( π*x[2] )*sin( π*x[3] )\n        h_y = sin( π*x[1] )*cos( π*x[2] )*sin( π*x[3] )\n        h_z = sin( π*x[1] )*sin( π*x[2] )*cos( π*x[3] )\n        h_xx = -sin( π*x[1] )*sin( π*x[2] )*sin( π*x[3] )\n        h_yy = -sin( π*x[1] )*sin( π*x[2] )*sin( π*x[3] )\n        h_zz = -sin( π*x[1] )*sin( π*x[2] )*sin( π*x[3] )\n\n        hexact=(h_xx + h_yy + h_zz)/3.0\n        h, hexact\n    end\n\n    time_final = DFloat(10.0)\n    iplot=100\n    Ne = 10\n    N  = 4\n    visc = 0.0\n    dim = 3\n    hardware=\"cpu\"\n    @show (N,Ne,visc,iplot,time_final,hardware,mpisize)\n\n    mesh3D = brickmesh((range(DFloat(0); length=Ne+1, stop=2),\n                        range(DFloat(0); length=Ne+1, stop=2),\n                        range(DFloat(0); length=Ne+1, stop=2)),\n                       (true, true, true),\n                       part=mpirank+1, numparts=mpisize)\n\n    if hardware == \"cpu\"\n        mpirank == 0 && println(\"Running 3d (CPU)...\")\n        LDG(Val(dim), Val(N), mpicomm, (x...)->ic(dim, x...), mesh3D, time_final, iplot, visc;\n              ArrType=Array, tout = 10)\n        mpirank == 0 && println()\n    elseif hardware == \"gpu\"\n        @hascuda begin\n            mpirank == 0 && println(\"Running 3d (GPU)...\")\n            LDG(Val(dim), Val(N), mpicomm, (x...)->ic(dim, x...), mesh3D, time_final, iplot, visc;\n                  ArrType=CuArray, tout = 10)\n            mpirank == 0 && println()\n        end\n    end\n    nothing\nend}}}main()This page was generated using Literate.jl."
},

{
    "location": "examples/generated/3d_kernels/nse3d/#",
    "page": "3D Compressible Navier-Stokes Equations",
    "title": "3D Compressible Navier-Stokes Equations",
    "category": "page",
    "text": "EditURL = \"https://github.com/climate-machine/Canary.jl/blob/master/examples/3d_kernels/nse3d.jl\""
},

{
    "location": "examples/generated/3d_kernels/nse3d/#D-Compressible-Navier-Stokes-Equations-1",
    "page": "3D Compressible Navier-Stokes Equations",
    "title": "3D Compressible Navier-Stokes Equations",
    "category": "section",
    "text": ""
},

{
    "location": "examples/generated/3d_kernels/nse3d/#Introduction-1",
    "page": "3D Compressible Navier-Stokes Equations",
    "title": "Introduction",
    "category": "section",
    "text": "This example shows how to solve the 3D compressible Navier-Stokes equations using vanilla DG."
},

{
    "location": "examples/generated/3d_kernels/nse3d/#Continuous-Governing-Equations-1",
    "page": "3D Compressible Navier-Stokes Equations",
    "title": "Continuous Governing Equations",
    "category": "section",
    "text": "We solve the following equation:fracpartial rhopartial t + nabla cdot mathbfU = 0   (11)fracpartial mathbfUpartial t + nabla cdot left( fracmathbfU otimes mathbfUrho + P mathbfI_2 right) + rho g hatmathbfk= nabla cdot mathbfF_U^visc   (12)fracpartial Epartial t + nabla cdot left( fracmathbfU left(E+P right)rho right) = nabla cdot mathbfF_E^visc   (13)where mathbfu=(uvw) is the velocity, mathbfU=rho mathbfu, is the momentum, with rho the total density and E=(gamma-1) rho left( c_v T + frac12 mathbfu cdot mathbfu + g z right) the total energy (internal + kinetic + potential). The viscous fluxes are defined as followsmathbfF_U^visc = mu left( nabla mathbfu +  left( nabla mathbfu right)^T + lambda left( nabla cdot mathbfu right)  mathbfI_2 right)andmathbfF_E^visc =  mathbfu cdot mathbfF_U^visc + fracc_pPr nabla Twhere mu is the kinematic (or artificial) viscosity, lambda=-frac23 is the Stokes hypothesis, Pr approx 071 is the Prandtl number for air and T is the temperature. We employ periodic boundary conditions in the horizontaland no-flux boundary conditions in the vertical.  At the bottom and top of the domain, we need to impose no-flux boundary conditions in nabla T to avoid a (artificial) thermal boundary layer."
},

{
    "location": "examples/generated/3d_kernels/nse3d/#Discontinous-Galerkin-Method-1",
    "page": "3D Compressible Navier-Stokes Equations",
    "title": "Discontinous Galerkin Method",
    "category": "section",
    "text": "To solve Eq. (1) we use the discontinuous Galerkin method with basis functions comprised of Lagrange polynomials based on Lobatto points. Multiplying Eq. (1) by a test function psi and integrating within each element Omega_e such that Omega = bigcup_e=1^N_e Omega_e we getint_Omega_e psi fracpartial mathbfq^(e)_Npartial t dOmega_e + int_Omega_e psi nabla cdot mathbfF^(e)_N dOmega_e =  int_Omega_e psi Sleft( q^(e)_N right) dOmega_e   (2)where mathbfq^(e)_N=sum_i=1^(N+1)^dim psi_i(mathbfx) mathbfq_i(t) is the finite dimensional expansion with basis functions psi(mathbfx), where mathbfq=left( rho mathbfU^T E right)^T,mathbfF=left( mathbfU fracmathbfU otimes mathbfUrho + P mathbfI_2   fracmathbfU left(E+P right)rho right)and Sleft( q^(e)_N right)  = nu left( 0 nabla cdot mathbfF_U^visc nabla cdot mathbfF_E^visc right)Integrating Eq. (2) by parts yieldsint_Omega_e psi fracpartial mathbfq^(e)_Npartial t dOmega_e + int_Gamma_e psi mathbfn cdot mathbfF^(*e)_N dGamma_e - int_Omega_e nabla psi cdot mathbfF^(e)_N dOmega_e = int_Omega_e psi Sleft( q^(e)_N right) dOmega_e   (3)where the second term on the left denotes the flux integral term (computed in \"function flux_rhs\") and the third term denotes the volume integral term (computed in \"function volume_rhs\").  The superscript (*e) in the flux integral term denotes the numerical flux. Here we use the Rusanov flux."
},

{
    "location": "examples/generated/3d_kernels/nse3d/#Local-Discontinous-Galerkin-Method-1",
    "page": "3D Compressible Navier-Stokes Equations",
    "title": "Local Discontinous Galerkin Method",
    "category": "section",
    "text": "To approximate the second order terms on the right hand side of Eq. (1) we use the local discontinuous Galerkin (LDG) method, which we described in LDG3d.jl. We will highlight the main steps below for completeness. We employ the following two-step process: first we approximate the gradient of q as followsmathbfQ(mathbfx) = nabla mathbfq(mathbfx)   (4)where mathbfQ is an auxiliary vector function, followed bynabla cdot mathbfF^viscleft( mathbfQ right)   (5)which completes the approximation of the second order derivatives."
},

{
    "location": "examples/generated/3d_kernels/nse3d/#Commented-Program-1",
    "page": "3D Compressible Navier-Stokes Equations",
    "title": "Commented Program",
    "category": "section",
    "text": "include(joinpath(@__DIR__,\"vtk.jl\"))\nusing MPI\nusing Canary\nusing Printf: @sprintf\nconst HAVE_CUDA = try\n    using CUDAnative\n    using CUDAdrv\n    true\ncatch\n    false\nend\nif HAVE_CUDA\n    macro hascuda(ex)\n        return :($(esc(ex)))\n    end\nelse\n    macro hascuda(ex)\n        return :()\n    end\nend{{{ reshape for CuArray@hascuda function Base.reshape(A::CuArray, dims::NTuple{N, Int}) where {N}\n    @assert prod(dims) == prod(size(A))\n    CuArray{eltype(A), length(dims)}(dims, A.buf)\nend}}}{{{ constants note the order of the fields below is also assumed in the code.const _nstate = 5\nconst _U, _V, _W, _ρ, _E = 1:_nstate\nconst stateid = (U = _U, V = _V, W = _W, ρ = _ρ, E = _E)\n\nconst _nvgeo = 14\nconst _ξx, _ηx, _ζx, _ξy, _ηy, _ζy, _ξz, _ηz, _ζz, _MJ, _MJI,\n_x, _y, _z = 1:_nvgeo\nconst vgeoid = (ξx = _ξx, ηx = _ηx, ζx = _ζx,\n                ξy = _ξy, ηy = _ηy, ζy = _ζy,\n                ξz = _ξz, ηz = _ηz, ζz = _ζz,\n                MJ = _MJ, MJI = _MJI,\n                x = _x,   y = _y,   z = _z)\n\nconst _nsgeo = 5\nconst _nx, _ny, _nz, _sMJ, _vMJI = 1:_nsgeo\nconst sgeoid = (nx = _nx, ny = _ny, nz = _nz, sMJ = _sMJ, vMJI = _vMJI)\n\nconst _γ = 14  // 10\nconst _p0 = 100000\nconst _R_gas = 28717 // 100\nconst _c_p = 100467 // 100\nconst _c_v = 7175 // 10\nconst _gravity = 10\nconst _Prandtl = 71 // 10\nconst _Stokes = -2 // 3}}}{{{ courantfunction courantnumber(::Val{dim}, ::Val{N}, vgeo, Q, mpicomm) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1)^dim\n    (~, ~, nelem) = size(Q)\n\n    dt = [floatmax(DFloat)]\n    Courant = - [floatmax(DFloat)]\n\n    #Compute DT\n    @inbounds for e = 1:nelem, n = 1:Np\n        ρ, U, V, W, E = Q[n, _ρ, e], Q[n, _U, e], Q[n, _V, e], Q[n, _W, e], Q[n, _E, e]\n        ξx, ξy, ξz = vgeo[n, _ξx, e], vgeo[n, _ξy, e], vgeo[n, _ξz, e]\n        ηx, ηy, ηz = vgeo[n, _ηx, e], vgeo[n, _ηy, e], vgeo[n, _ηz, e]\n        ζx, ζy, ζz = vgeo[n, _ζx, e], vgeo[n, _ζy, e], vgeo[n, _ζz, e]\n        z = vgeo[n, _z, e]\n        P = (R_gas/c_v)*(E - (U^2 + V^2 + W^2)/(2*ρ) - ρ*gravity*z)\n        u, v, w = U/ρ, V/ρ, W/ρ\n        dx=sqrt( (1.0/(2*ξx))^2 + 0*(1.0/(2*ηy))^2  + (1.0/(2*ζz))^2 )\n        vel=sqrt( u^2 + v^2 + w^2)\n        wave_speed = (vel + sqrt(γ * P / ρ))\n        loc_dt = 1.0*dx/wave_speed/N\n        dt[1] = min(dt[1], loc_dt)\n    end\n    dt_min=MPI.Allreduce(dt[1], MPI.MIN, mpicomm)\n\n    #Compute Courant\n    @inbounds for e = 1:nelem, n = 1:Np\n        ρ, U, V, W, E = Q[n, _ρ, e], Q[n, _U, e], Q[n, _V, e], Q[n, _W, e], Q[n, _E, e]\n        ξx, ξy, ξz = vgeo[n, _ξx, e], vgeo[n, _ξy, e], vgeo[n, _ξz, e]\n        ηx, ηy, ηz = vgeo[n, _ηx, e], vgeo[n, _ηy, e], vgeo[n, _ηz, e]\n        ζx, ζy, ζz = vgeo[n, _ζx, e], vgeo[n, _ζy, e], vgeo[n, _ζz, e]\n        z = vgeo[n, _z, e]\n        P = (R_gas/c_v)*(E - (U^2 + V^2 + W^2)/(2*ρ) - ρ*gravity*z)\n        u, v, w = U/ρ, V/ρ, W/ρ\n        dx=sqrt( (1.0/(2*ξx))^2 + 0*(1.0/(2*ηy))^2  + (1.0/(2*ζz))^2 )\n        vel=sqrt( u^2 + v^2 + w^2)\n        wave_speed = (vel + sqrt(γ * P / ρ))\n        loc_Courant = wave_speed*dt_min*N/dx\n        Courant[1] = max(Courant[1], loc_Courant)\n    end\n    Courant_max=MPI.Allreduce(Courant[1], MPI.MAX, mpicomm)\n\n    (dt_min, Courant_max)\nend}}}{{{ cflfunction cfl(::Val{dim}, ::Val{N}, vgeo, Q, mpicomm) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1)^dim\n    (~, ~, nelem) = size(Q)\n\n    dt = [floatmax(DFloat)]\n\n    @inbounds for e = 1:nelem, n = 1:Np\n        ρ, U, V, W = Q[n, _ρ, e], Q[n, _U, e], Q[n, _V, e], Q[n, _W, e]\n        E = Q[n, _E, e]\n        P = p0 * (R_gas * E / p0)^(c_p / c_v)\n\n        ξx, ξy, ξz = vgeo[n, _ξx, e], vgeo[n, _ξy, e], vgeo[n, _ξz, e]\n        ηx, ηy, ηz = vgeo[n, _ηx, e], vgeo[n, _ηy, e], vgeo[n, _ηz, e]\n        ζx, ζy, ζz = vgeo[n, _ζx, e], vgeo[n, _ζy, e], vgeo[n, _ζz, e]\n\n        loc_dt = 2ρ / max(abs(U * ξx + V * ξy + W * ξz) + ρ * sqrt(γ * P / ρ),\n                          abs(U * ηx + V * ηy + W * ηz) + ρ * sqrt(γ * P / ρ),\n                          abs(U * ζx + V * ζy + W * ζz) + ρ * sqrt(γ * P / ρ))\n        dt[1] = min(dt[1], loc_dt)\n    end\n\n    MPI.Allreduce(dt[1], MPI.MIN, mpicomm)\nend}}}{{{ compute geometryfunction computegeometry(::Val{dim}, mesh, D, ξ, ω, meshwarp, vmapM) where dimCompute metric terms    Nq = size(D, 1)\n    DFloat = eltype(D)\n\n    (nface, nelem) = size(mesh.elemtoelem)\n\n    crd = creategrid(Val(dim), mesh.elemtocoord, ξ)\n\n    vgeo = zeros(DFloat, Nq^dim, _nvgeo, nelem)\n    sgeo = zeros(DFloat, _nsgeo, Nq^(dim-1), nface, nelem)\n\n    (ξx, ηx, ζx, ξy, ηy, ζy, ξz, ηz, ζz, MJ, MJI, x, y, z) =\n        ntuple(j->(@view vgeo[:, j, :]), _nvgeo)\n    J = similar(x)\n    (nx, ny, nz, sMJ, vMJI) = ntuple(j->(@view sgeo[ j, :, :, :]), _nsgeo)\n    sJ = similar(sMJ)\n\n    X = ntuple(j->(@view vgeo[:, _x+j-1, :]), dim)\n    creategrid!(X..., mesh.elemtocoord, ξ)\n\n    @inbounds for j = 1:length(x)\n        (x[j], y[j], z[j]) = meshwarp(x[j], y[j], z[j])\n    endCompute the metric terms    computemetric!(x, y, z, J, ξx, ηx, ζx, ξy, ηy, ζy, ξz, ηz, ζz, sJ,\n                   nx, ny, nz, D)\n\n    M = kron(1, ntuple(j->ω, dim)...)\n    MJ .= M .* J\n    MJI .= 1 ./ MJ\n    vMJI .= MJI[vmapM]\n\n    sM = dim > 1 ? kron(1, ntuple(j->ω, dim-1)...) : one(DFloat)\n    sMJ .= sM .* sJ\n\n    (vgeo, sgeo)\nend}}}{{{ CPU Kernels Volume RHSfunction volume_rhs!(::Val{dim}, ::Val{N}, rhs::Array, Q, vgeo, D, elems) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Nq = N + 1\n    nelem = size(Q)[end]\n\n    Q = reshape(Q, Nq, Nq, Nq, _nstate, nelem)\n    rhs = reshape(rhs, Nq, Nq, Nq, _nstate, nelem)\n    vgeo = reshape(vgeo, Nq, Nq, Nq, _nvgeo, nelem)\n\n    s_F = Array{DFloat}(undef, Nq, Nq, Nq, _nstate)\n    s_G = Array{DFloat}(undef, Nq, Nq, Nq, _nstate)\n    s_H = Array{DFloat}(undef, Nq, Nq, Nq, _nstate)\n\n    @inbounds for e in elems\n        for k = 1:Nq, j = 1:Nq, i = 1:Nq\n            MJ = vgeo[i, j, k, _MJ, e]\n            ξx, ξy, ξz = vgeo[i,j,k,_ξx,e], vgeo[i,j,k,_ξy,e], vgeo[i,j,k,_ξz,e]\n            ηx, ηy, ηz = vgeo[i,j,k,_ηx,e], vgeo[i,j,k,_ηy,e], vgeo[i,j,k,_ηz,e]\n            ζx, ζy, ζz = vgeo[i,j,k,_ζx,e], vgeo[i,j,k,_ζy,e], vgeo[i,j,k,_ζz,e]\n            z = vgeo[i,j,k,_z,e]\n\n            U, V, W = Q[i, j, k, _U, e], Q[i, j, k, _V, e], Q[i, j, k, _W, e]\n            ρ, E = Q[i, j, k, _ρ, e], Q[i, j, k, _E, e]\n            P = (R_gas/c_v)*(E - (U^2 + V^2 + W^2)/(2*ρ) - ρ*gravity*z)\n\n            ρinv = 1 / ρ\n            fluxρ_x = U\n            fluxU_x = ρinv * U * U + P\n            fluxV_x = ρinv * V * U\n            fluxW_x = ρinv * W * U\n            fluxE_x = ρinv * U * (E+P)\n\n            fluxρ_y = V\n            fluxU_y = ρinv * U * V\n            fluxV_y = ρinv * V * V + P\n            fluxW_y = ρinv * W * V\n            fluxE_y = ρinv * V * (E+P)\n\n            fluxρ_z = W\n            fluxU_z = ρinv * U * W\n            fluxV_z = ρinv * V * W\n            fluxW_z = ρinv * W * W + P\n            fluxE_z = ρinv * W * (E+P)\n\n            s_F[i, j, k, _ρ] = MJ * (ξx * fluxρ_x + ξy * fluxρ_y + ξz * fluxρ_z)\n            s_F[i, j, k, _U] = MJ * (ξx * fluxU_x + ξy * fluxU_y + ξz * fluxU_z)\n            s_F[i, j, k, _V] = MJ * (ξx * fluxV_x + ξy * fluxV_y + ξz * fluxV_z)\n            s_F[i, j, k, _W] = MJ * (ξx * fluxW_x + ξy * fluxW_y + ξz * fluxW_z)\n            s_F[i, j, k, _E] = MJ * (ξx * fluxE_x + ξy * fluxE_y + ξz * fluxE_z)\n\n            s_G[i, j, k, _ρ] = MJ * (ηx * fluxρ_x + ηy * fluxρ_y + ηz * fluxρ_z)\n            s_G[i, j, k, _U] = MJ * (ηx * fluxU_x + ηy * fluxU_y + ηz * fluxU_z)\n            s_G[i, j, k, _V] = MJ * (ηx * fluxV_x + ηy * fluxV_y + ηz * fluxV_z)\n            s_G[i, j, k, _W] = MJ * (ηx * fluxW_x + ηy * fluxW_y + ηz * fluxW_z)\n            s_G[i, j, k, _E] = MJ * (ηx * fluxE_x + ηy * fluxE_y + ηz * fluxE_z)\n\n            s_H[i, j, k, _ρ] = MJ * (ζx * fluxρ_x + ζy * fluxρ_y + ζz * fluxρ_z)\n            s_H[i, j, k, _U] = MJ * (ζx * fluxU_x + ζy * fluxU_y + ζz * fluxU_z)\n            s_H[i, j, k, _V] = MJ * (ζx * fluxV_x + ζy * fluxV_y + ζz * fluxV_z)\n            s_H[i, j, k, _W] = MJ * (ζx * fluxW_x + ζy * fluxW_y + ζz * fluxW_z)\n            s_H[i, j, k, _E] = MJ * (ζx * fluxE_x + ζy * fluxE_y + ζz * fluxE_z)buoyancy term            rhs[i, j, k, _W, e] -= MJ * ρ * gravity\n        endloop of ξ-grid lines        for s = 1:_nstate, k = 1:Nq, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, k, s, e] += D[n, i] * s_F[n, j, k, s]\n        endloop of η-grid lines        for s = 1:_nstate, k = 1:Nq, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, k, s, e] += D[n, j] * s_G[i, n, k, s]\n        endloop of ζ-grid lines        for s = 1:_nstate, k = 1:Nq, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, k, s, e] += D[n, k] * s_H[i, j, n, s]\n        end\n    end\nendflux RHSfunction flux_rhs!(::Val{dim}, ::Val{N}, rhs::Array, Q, sgeo, vgeo, elems, vmapM,\n                  vmapP, elemtobndy) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1)^dim\n    Nfp = (N+1)^(dim-1)\n    nface = 2*dim\n\n    @inbounds for e in elems\n        for f = 1:nface\n            for n = 1:Nfp\n                (nxM, nyM, nzM, sMJ, ~) = sgeo[:, n, f, e]\n                idM, idP = vmapM[n, f, e], vmapP[n, f, e]\n\n                eM, eP = e, ((idP - 1) ÷ Np) + 1\n                vidM, vidP = ((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1\n\n                #Left conservation variables\n                ρM = Q[vidM, _ρ, eM]\n                UM = Q[vidM, _U, eM]\n                VM = Q[vidM, _V, eM]\n                WM = Q[vidM, _W, eM]\n                EM = Q[vidM, _E, eM]\n                zM = vgeo[vidM, _z, eM]\n                PM = (R_gas/c_v)*(EM - (UM^2 + VM^2 + WM^2)/(2*ρM) - ρM*gravity*zM)\n                #Right conservation variables\n                bc = elemtobndy[f, e]\n                if bc == 0\n                    ρP = Q[vidP, _ρ, eP]\n                    UP = Q[vidP, _U, eP]\n                    VP = Q[vidP, _V, eP]\n                    WP = Q[vidP, _W, eP]\n                    EP = Q[vidP, _E, eP]\n                    zP = vgeo[vidP, _z, eP]\n                    PP = (R_gas/c_v)*(EP - (UP^2 + VP^2 + WP^2)/(2*ρP) - ρP*gravity*zP)\n                elseif bc == 1\n                    UnM = nxM * UM + nyM * VM + nzM * WM\n                    UP = UM - 2 * UnM * nxM\n                    VP = VM - 2 * UnM * nyM\n                    WP = WM - 2 * UnM * nzM\n                    ρP = ρM\n                    EP = EM\n                    PP = PM\n                else\n                    error(\"Invalid boundary conditions $bc on face $f of element $e\")\n                end\n\n                ρMinv = 1 / ρM\n                fluxρM_x = UM\n                fluxUM_x = ρMinv * UM * UM + PM\n                fluxVM_x = ρMinv * VM * UM\n                fluxWM_x = ρMinv * WM * UM\n                fluxEM_x = ρMinv * UM * (EM+PM)\n\n                fluxρM_y = VM\n                fluxUM_y = ρMinv * UM * VM\n                fluxVM_y = ρMinv * VM * VM + PM\n                fluxWM_y = ρMinv * WM * VM\n                fluxEM_y = ρMinv * VM * (EM+PM)\n\n                fluxρM_z = WM\n                fluxUM_z = ρMinv * UM * WM\n                fluxVM_z = ρMinv * VM * WM\n                fluxWM_z = ρMinv * WM * WM + PM\n                fluxEM_z = ρMinv * WM * (EM+PM)\n\n                ρPinv = 1 / ρP\n                fluxρP_x = UP\n                fluxUP_x = ρPinv * UP * UP + PP\n                fluxVP_x = ρPinv * VP * UP\n                fluxWP_x = ρPinv * WP * UP\n                fluxEP_x = ρPinv * UP * (EP+PP)\n\n                fluxρP_y = VP\n                fluxUP_y = ρPinv * UP * VP\n                fluxVP_y = ρPinv * VP * VP + PP\n                fluxWP_y = ρPinv * WP * VP\n                fluxEP_y = ρPinv * VP * (EP+PP)\n\n                fluxρP_z = WP\n                fluxUP_z = ρPinv * UP * WP\n                fluxVP_z = ρPinv * VP * WP\n                fluxWP_z = ρPinv * WP * WP + PP\n                fluxEP_z = ρPinv * WP * (EP+PP)\n\n                λM = ρMinv * abs(nxM * UM + nyM * VM + nzM * WM) + sqrt(ρMinv * γ * PM)\n                λP = ρPinv * abs(nxM * UP + nyM * VP + nzM * WP) + sqrt(ρPinv * γ * PP)\n                λ  =  max(λM, λP)\n\n                #Compute Numerical Flux and Update\n                fluxρS = (nxM * (fluxρM_x + fluxρP_x) + nyM * (fluxρM_y + fluxρP_y) +\n                          nzM * (fluxρM_z + fluxρP_z) - λ * (ρP - ρM)) / 2\n                fluxUS = (nxM * (fluxUM_x + fluxUP_x) + nyM * (fluxUM_y + fluxUP_y) +\n                          nzM * (fluxUM_z + fluxUP_z) - λ * (UP - UM)) / 2\n                fluxVS = (nxM * (fluxVM_x + fluxVP_x) + nyM * (fluxVM_y + fluxVP_y) +\n                          nzM * (fluxVM_z + fluxVP_z) - λ * (VP - VM)) / 2\n                fluxWS = (nxM * (fluxWM_x + fluxWP_x) + nyM * (fluxWM_y + fluxWP_y) +\n                          nzM * (fluxWM_z + fluxWP_z) - λ * (WP - WM)) / 2\n                fluxES = (nxM * (fluxEM_x + fluxEP_x) + nyM * (fluxEM_y + fluxEP_y) +\n                          nzM * (fluxEM_z + fluxEP_z) - λ * (EP - EM)) / 2\n\n\n                #Update RHS\n                rhs[vidM, _ρ, eM] -= sMJ * fluxρS\n                rhs[vidM, _U, eM] -= sMJ * fluxUS\n                rhs[vidM, _V, eM] -= sMJ * fluxVS\n                rhs[vidM, _W, eM] -= sMJ * fluxWS\n                rhs[vidM, _E, eM] -= sMJ * fluxES\n            end\n        end\n    end\nend}}}{{{ Volume grad(Q)function volume_grad!(::Val{dim}, ::Val{N}, rhs::Array, Q, vgeo, D, elems) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Nq = N + 1\n    nelem = size(Q)[end]\n\n    Q = reshape(Q, Nq, Nq, Nq, _nstate, nelem)\n    rhs = reshape(rhs, Nq, Nq, Nq, _nstate, dim, nelem)\n    vgeo = reshape(vgeo, Nq, Nq, Nq, _nvgeo, nelem)\n\n    #Initialize RHS vector\n    fill!( rhs, zero(rhs[1]))\n\n    #Allocate Arrays\n    s_F = Array{DFloat}(undef, Nq, Nq, Nq, _nstate, dim)\n    s_G = Array{DFloat}(undef, Nq, Nq, Nq, _nstate, dim)\n    s_H = Array{DFloat}(undef, Nq, Nq, Nq, _nstate, dim)\n\n    @inbounds for e in elems\n        for k = 1:Nq, j = 1:Nq, i = 1:Nq\n            MJ = vgeo[i, j, k, _MJ, e]\n            ξx, ξy, ξz = vgeo[i,j,k,_ξx,e], vgeo[i,j,k,_ξy,e], vgeo[i,j,k,_ξz,e]\n            ηx, ηy, ηz = vgeo[i,j,k,_ηx,e], vgeo[i,j,k,_ηy,e], vgeo[i,j,k,_ηz,e]\n            ζx, ζy, ζz = vgeo[i,j,k,_ζx,e], vgeo[i,j,k,_ζy,e], vgeo[i,j,k,_ζz,e]\n            z = vgeo[i,j,k,_z,e]\n\n            U, V, W = Q[i, j, k, _U, e], Q[i, j, k, _V, e], Q[i, j, k, _W, e]\n            ρ, E = Q[i, j, k, _ρ, e], Q[i, j, k, _E, e]\n            P = (R_gas/c_v)*(E - (U^2 + V^2 + W^2)/(2*ρ) - ρ*gravity*z)\n\n            #Primitive variables\n            u=U/ρ\n            v=V/ρ\n            w=W/ρ\n            T=P/(R_gas*ρ)\n\n            #Compute fluxes\n            fluxρ = ρ\n            fluxU = u\n            fluxV = v\n            fluxW = w\n            fluxE = T\n\n            s_F[i, j, k, _ρ, 1], s_F[i, j, k, _ρ, 2], s_F[i, j, k, _ρ, 3] = MJ * (ξx * fluxρ), MJ * (ξy * fluxρ), MJ * (ξz * fluxρ)\n            s_F[i, j, k, _U, 1], s_F[i, j, k, _U, 2], s_F[i, j, k, _U, 3] = MJ * (ξx * fluxU), MJ * (ξy * fluxU), MJ * (ξz * fluxU)\n            s_F[i, j, k, _V, 1], s_F[i, j, k, _V, 2], s_F[i, j, k, _V, 3] = MJ * (ξx * fluxV), MJ * (ξy * fluxV), MJ * (ξz * fluxV)\n            s_F[i, j, k, _W, 1], s_F[i, j, k, _W, 2], s_F[i, j, k, _W, 3] = MJ * (ξx * fluxW), MJ * (ξy * fluxW), MJ * (ξz * fluxW)\n            s_F[i, j, k, _E, 1], s_F[i, j, k, _E, 2], s_F[i, j, k, _E, 3] = MJ * (ξx * fluxE), MJ * (ξy * fluxE), MJ * (ξz * fluxE)\n\n            s_G[i, j, k, _ρ, 1], s_G[i, j, k, _ρ, 2], s_G[i, j, k, _ρ, 3] = MJ * (ηx * fluxρ), MJ * (ηy * fluxρ), MJ * (ηz * fluxρ)\n            s_G[i, j, k, _U, 1], s_G[i, j, k, _U, 2], s_G[i, j, k, _U, 3] = MJ * (ηx * fluxU), MJ * (ηy * fluxU), MJ * (ηz * fluxU)\n            s_G[i, j, k, _V, 1], s_G[i, j, k, _V, 2], s_G[i, j, k, _V, 3] = MJ * (ηx * fluxV), MJ * (ηy * fluxV), MJ * (ηz * fluxV)\n            s_G[i, j, k, _W, 1], s_G[i, j, k, _W, 2], s_G[i, j, k, _W, 3] = MJ * (ηx * fluxW), MJ * (ηy * fluxW), MJ * (ηz * fluxW)\n            s_G[i, j, k, _E, 1], s_G[i, j, k, _E, 2], s_G[i, j, k, _E, 3] = MJ * (ηx * fluxE), MJ * (ηy * fluxE), MJ * (ηz * fluxE)\n\n            s_H[i, j, k, _ρ, 1], s_H[i, j, k, _ρ, 2], s_H[i, j, k, _ρ, 3] = MJ * (ζx * fluxρ), MJ * (ζy * fluxρ), MJ * (ζz * fluxρ)\n            s_H[i, j, k, _U, 1], s_H[i, j, k, _U, 2], s_H[i, j, k, _U, 3] = MJ * (ζx * fluxU), MJ * (ζy * fluxU), MJ * (ζz * fluxU)\n            s_H[i, j, k, _V, 1], s_H[i, j, k, _V, 2], s_H[i, j, k, _V, 3] = MJ * (ζx * fluxV), MJ * (ζy * fluxV), MJ * (ζz * fluxV)\n            s_H[i, j, k, _W, 1], s_H[i, j, k, _W, 2], s_H[i, j, k, _W, 3] = MJ * (ζx * fluxW), MJ * (ζy * fluxW), MJ * (ζz * fluxW)\n            s_H[i, j, k, _E, 1], s_H[i, j, k, _E, 2], s_H[i, j, k, _E, 3] = MJ * (ζx * fluxE), MJ * (ζy * fluxE), MJ * (ζz * fluxE)\n        endloop of ξ-grid lines        for s = 1:_nstate, k = 1:Nq, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, k, s, 1, e] -= D[n, i] * s_F[n, j, k, s, 1]\n            rhs[i, j, k, s, 2, e] -= D[n, i] * s_F[n, j, k, s, 2]\n            rhs[i, j, k, s, 3, e] -= D[n, i] * s_F[n, j, k, s, 3]\n        endloop of η-grid lines        for s = 1:_nstate, k = 1:Nq, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, k, s, 1, e] -= D[n, j] * s_G[i, n, k, s, 1]\n            rhs[i, j, k, s, 2, e] -= D[n, j] * s_G[i, n, k, s, 2]\n            rhs[i, j, k, s, 3, e] -= D[n, j] * s_G[i, n, k, s, 3]\n        endloop of ζ-grid lines        for s = 1:_nstate, k = 1:Nq, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, k, s, 1, e] -= D[n, k] * s_H[i, j, n, s, 1]\n            rhs[i, j, k, s, 2, e] -= D[n, k] * s_H[i, j, n, s, 2]\n            rhs[i, j, k, s, 3, e] -= D[n, k] * s_H[i, j, n, s, 3]\n        end\n    end\nend}}}Flux grad(Q)function flux_grad!(::Val{dim}, ::Val{N}, rhs::Array,  Q, sgeo, vgeo, elems, vmapM, vmapP, elemtobndy) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1)^dim\n    Nfp = (N+1)^(dim-1)\n    nface = 2*dim\n\n    @inbounds for e in elems\n        for f = 1:nface\n            for n = 1:Nfp\n                (nxM, nyM, nzM, sMJ, ~) = sgeo[:, n, f, e]\n                idM, idP = vmapM[n, f, e], vmapP[n, f, e]\n\n                eM, eP = e, ((idP - 1) ÷ Np) + 1\n                vidM, vidP = ((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1\n\n                #Left variables\n                ρM = Q[vidM, _ρ, eM]\n                UM = Q[vidM, _U, eM]\n                VM = Q[vidM, _V, eM]\n                WM = Q[vidM, _W, eM]\n                EM = Q[vidM, _E, eM]\n                zM = vgeo[vidM, _z, eM]\n                PM = (R_gas/c_v)*(EM - (UM^2 + VM^2 + WM^2)/(2*ρM) - ρM*gravity*zM)\n                uM=UM/ρM\n                vM=VM/ρM\n                wM=WM/ρM\n                TM=PM/(R_gas*ρM)\n\n                #Right variables\n                bc = elemtobndy[f, e]\n                ρP = UP = VP = WP = EP = PP = zero(eltype(Q))\n                if bc == 0\n                    ρP = Q[vidP, _ρ, eP]\n                    UP = Q[vidP, _U, eP]\n                    VP = Q[vidP, _V, eP]\n                    WP = Q[vidP, _W, eP]\n                    EP = Q[vidP, _E, eP]\n                    zP = vgeo[vidP, _z, eP]\n                    PP = (R_gas/c_v)*(EP - (UP^2 + VP^2 + WP^2)/(2*ρP) - ρP*gravity*zP)\n                    uP=UP/ρP\n                    vP=VP/ρP\n                    wP=WP/ρP\n                    TP=PP/(R_gas*ρP)\n                elseif bc == 1\n                    UnM = nxM * UM + nyM * VM + nzM * WM\n                    UP = UM - 2 * UnM * nxM\n                    VP = VM - 2 * UnM * nyM\n                    WP = WM - 2 * UnM * nzM\n                    ρP = ρM\n                    EP = EM\n                    PP = PM\n                    uP=UP/ρP\n                    vP=VP/ρP\n                    wP=WP/ρP\n                    TP=TM\n                else\n                    error(\"Invalid boundary conditions $bc on face $f of element $e\")\n                end\n\n                #Left Fluxes\n                fluxρM = ρM\n                fluxUM = uM\n                fluxVM = vM\n                fluxWM = wM\n                fluxEM = TM\n\n                #Right Fluxes\n                fluxρP = ρP\n                fluxUP = uP\n                fluxVP = vP\n                fluxWP = wP\n                fluxEP = TP\n\n                #Compute Numerical/Rusanov Flux\n                fluxρS = 0.5*(fluxρM + fluxρP)\n                fluxUS = 0.5*(fluxUM + fluxUP)\n                fluxVS = 0.5*(fluxVM + fluxVP)\n                fluxWS = 0.5*(fluxWM + fluxWP)\n                fluxES = 0.5*(fluxEM + fluxEP)\n\n                #Update RHS\n                rhs[vidM, _ρ, 1, eM] += sMJ * nxM*fluxρS\n                rhs[vidM, _ρ, 2, eM] += sMJ * nyM*fluxρS\n                rhs[vidM, _ρ, 3, eM] += sMJ * nzM*fluxρS\n                rhs[vidM, _U, 1, eM] += sMJ * nxM*fluxUS\n                rhs[vidM, _U, 2, eM] += sMJ * nyM*fluxUS\n                rhs[vidM, _U, 3, eM] += sMJ * nzM*fluxUS\n                rhs[vidM, _V, 1, eM] += sMJ * nxM*fluxVS\n                rhs[vidM, _V, 2, eM] += sMJ * nyM*fluxVS\n                rhs[vidM, _V, 3, eM] += sMJ * nzM*fluxVS\n                rhs[vidM, _W, 1, eM] += sMJ * nxM*fluxWS\n                rhs[vidM, _W, 2, eM] += sMJ * nyM*fluxWS\n                rhs[vidM, _W, 3, eM] += sMJ * nzM*fluxWS\n                rhs[vidM, _E, 1, eM] += sMJ * nxM*fluxES\n                rhs[vidM, _E, 2, eM] += sMJ * nyM*fluxES\n                rhs[vidM, _E, 3, eM] += sMJ * nzM*fluxES\n            end\n        end\n    end\nend}}}{{{ Volume div(grad(Q))function volume_div!(::Val{dim}, ::Val{N}, rhs::Array, gradQ, Q, vgeo, D, elems) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n    Pr::DFloat = _Prandtl\n    lambda::DFloat = _Stokes\n\n    Nq = N + 1\n    nelem = size(Q)[end]\n\n    Q = reshape(Q, Nq, Nq, Nq, _nstate, nelem)\n    gradQ = reshape(gradQ, Nq, Nq, Nq, _nstate, dim, nelem)\n    rhs = reshape(rhs, Nq, Nq, Nq, _nstate, dim, nelem)\n    vgeo = reshape(vgeo, Nq, Nq, Nq, _nvgeo, nelem)\n\n    #Initialize RHS vector\n    fill!( rhs, zero(rhs[1]))\n\n    #Allocate Arrays\n    s_F = Array{DFloat}(undef, Nq, Nq, Nq, _nstate)\n    s_G = Array{DFloat}(undef, Nq, Nq, Nq, _nstate)\n    s_H = Array{DFloat}(undef, Nq, Nq, Nq, _nstate)\n\n    @inbounds for e in elems\n        for k = 1:Nq, j = 1:Nq, i = 1:Nq\n            MJ = vgeo[i, j, k, _MJ, e]\n            ξx, ξy, ξz = vgeo[i,j,k,_ξx,e], vgeo[i,j,k,_ξy,e], vgeo[i,j,k,_ξz,e]\n            ηx, ηy, ηz = vgeo[i,j,k,_ηx,e], vgeo[i,j,k,_ηy,e], vgeo[i,j,k,_ηz,e]\n            ζx, ζy, ζz = vgeo[i,j,k,_ζx,e], vgeo[i,j,k,_ζy,e], vgeo[i,j,k,_ζz,e]\n\n            ρx, ρy, ρz = gradQ[i,j,k,_ρ,1,e], gradQ[i,j,k,_ρ,2,e], gradQ[i,j,k,_ρ,3,e]\n            Ux, Uy, Uz = gradQ[i,j,k,_U,1,e], gradQ[i,j,k,_U,2,e], gradQ[i,j,k,_U,3,e]\n            Vx, Vy, Vz = gradQ[i,j,k,_V,1,e], gradQ[i,j,k,_V,2,e], gradQ[i,j,k,_V,3,e]\n            Wx, Wy, Wz = gradQ[i,j,k,_W,1,e], gradQ[i,j,k,_W,2,e], gradQ[i,j,k,_W,3,e]\n            Ex, Ey, Ez = gradQ[i,j,k,_E,1,e], gradQ[i,j,k,_E,2,e], gradQ[i,j,k,_E,3,e]\n            ρ, U, V, W= Q[i,j,k,_ρ,e], Q[i,j,k,_U,e], Q[i,j,k,_V,e], Q[i,j,k,_W,e]\n\n            #Compute primitive variables\n            ux, uy, uz = Ux, Uy, Uz\n            vx, vy, vz = Vx, Vy, Vz\n            wx, wy, wz = Wx, Wy, Wz\n            Tx, Ty, Tz = Ex, Ey, Ez\n            div_u=ux + vy + wz\n            u=U/ρ\n            v=V/ρ\n            w=W/ρ\n\n            #Compute fluxes\n            fluxρ_x = 0*ρx\n            fluxρ_y = 0*ρy\n            fluxρ_z = 0*ρz\n            fluxU_x = 2*ux + lambda*div_u\n            fluxU_y = uy + vx\n            fluxU_z = uz + wx\n            fluxV_x = vx + uy\n            fluxV_y = 2*vy + lambda*div_u\n            fluxV_z = vz + wy\n            fluxW_x = wx + uz\n            fluxW_y = wy + vz\n            fluxW_z = 2*wz + lambda*div_u\n            fluxE_x = u*(2*ux + lambda*div_u) + v*(uy + vx) + w*(uz + wx) + c_p/Pr*Tx\n            fluxE_y = u*(vx + uy) + v*(2*vy + lambda*div_u) + w*(vz + wy) + c_p/Pr*Ty\n            fluxE_z = u*(wx + uz) + v*(wy + vz) + w*(2*wz + lambda*div_u) + c_p/Pr*Tz\n\n            s_F[i, j, k, _ρ] = MJ * (ξx * fluxρ_x + ξy * fluxρ_y + ξz * fluxρ_z)\n            s_F[i, j, k, _U] = MJ * (ξx * fluxU_x + ξy * fluxU_y + ξz * fluxU_z)\n            s_F[i, j, k, _V] = MJ * (ξx * fluxV_x + ξy * fluxV_y + ξz * fluxV_z)\n            s_F[i, j, k, _W] = MJ * (ξx * fluxW_x + ξy * fluxW_y + ξz * fluxW_z)\n            s_F[i, j, k, _E] = MJ * (ξx * fluxE_x + ξy * fluxE_y + ξz * fluxE_z)\n\n            s_G[i, j, k, _ρ] = MJ * (ηx * fluxρ_x + ηy * fluxρ_y + ηz * fluxρ_z)\n            s_G[i, j, k, _U] = MJ * (ηx * fluxU_x + ηy * fluxU_y + ηz * fluxU_z)\n            s_G[i, j, k, _V] = MJ * (ηx * fluxV_x + ηy * fluxV_y + ηz * fluxV_z)\n            s_G[i, j, k, _W] = MJ * (ηx * fluxW_x + ηy * fluxW_y + ηz * fluxW_z)\n            s_G[i, j, k, _E] = MJ * (ηx * fluxE_x + ηy * fluxE_y + ηz * fluxE_z)\n\n            s_H[i, j, k, _ρ] = MJ * (ζx * fluxρ_x + ζy * fluxρ_y + ζz * fluxρ_z)\n            s_H[i, j, k, _U] = MJ * (ζx * fluxU_x + ζy * fluxU_y + ζz * fluxU_z)\n            s_H[i, j, k, _V] = MJ * (ζx * fluxV_x + ζy * fluxV_y + ζz * fluxV_z)\n            s_H[i, j, k, _W] = MJ * (ζx * fluxW_x + ζy * fluxW_y + ζz * fluxW_z)\n            s_H[i, j, k, _E] = MJ * (ζx * fluxE_x + ζy * fluxE_y + ζz * fluxE_z)\n        endloop of ξ-grid lines        for s = 1:_nstate, k = 1:Nq, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, k, s, 1, e] -= D[n, i] * s_F[n, j, k, s]\n        endloop of η-grid lines        for s = 1:_nstate, k = 1:Nq, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, k, s, 1, e] -= D[n, j] * s_G[i, n, k, s]\n        endloop of ζ-grid lines        for s = 1:_nstate, k = 1:Nq, j = 1:Nq, i = 1:Nq, n = 1:Nq\n            rhs[i, j, k, s, 1, e] -= D[n, k] * s_H[i, j, n, s]\n        end\n    end\nend}}}Flux div(grad(Q))function flux_div!(::Val{dim}, ::Val{N}, rhs::Array,  gradQ, Q, sgeo, elems, vmapM, vmapP, elemtobndy) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n    Pr::DFloat = _Prandtl\n    lambda::DFloat = _Stokes\n\n    Np = (N+1)^dim\n    Nfp = (N+1)^(dim-1)\n    nface = 2*dim\n\n    @inbounds for e in elems\n        for f = 1:nface\n            for n = 1:Nfp\n                (nxM, nyM, nzM, sMJ, ~) = sgeo[:, n, f, e]\n                idM, idP = vmapM[n, f, e], vmapP[n, f, e]\n\n                eM, eP = e, ((idP - 1) ÷ Np) + 1\n                vidM, vidP = ((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1\n\n                #Left variables\n                ρxM = gradQ[vidM, _ρ, 1, eM]\n                ρyM = gradQ[vidM, _ρ, 2, eM]\n                ρzM = gradQ[vidM, _ρ, 3, eM]\n                UxM = gradQ[vidM, _U, 1, eM]\n                UyM = gradQ[vidM, _U, 2, eM]\n                UzM = gradQ[vidM, _U, 3, eM]\n                VxM = gradQ[vidM, _V, 1, eM]\n                VyM = gradQ[vidM, _V, 2, eM]\n                VzM = gradQ[vidM, _V, 3, eM]\n                WxM = gradQ[vidM, _W, 1, eM]\n                WyM = gradQ[vidM, _W, 2, eM]\n                WzM = gradQ[vidM, _W, 3, eM]\n                ExM = gradQ[vidM, _E, 1, eM]\n                EyM = gradQ[vidM, _E, 2, eM]\n                EzM = gradQ[vidM, _E, 3, eM]\n                ρM = Q[vidM, _ρ, eM]\n                UM = Q[vidM, _U, eM]\n                VM = Q[vidM, _V, eM]\n                WM = Q[vidM, _W, eM]\n\n                uM=UM/ρM\n                vM=VM/ρM\n                wM=WM/ρM\n                uxM, uyM, uzM = UxM, UyM, UzM\n                vxM, vyM, vzM = VxM, VyM, VzM\n                wxM, wyM, wzM = WxM, WyM, WzM\n                TxM, TyM, TzM = ExM, EyM, EzM\n\n                #Right variables\n                bc = elemtobndy[f, e]\n                ρxP = ρyP = ρzP = zero(eltype(Q))\n                UxP = UyP = UzP = zero(eltype(Q))\n                VxP = VyP = VzP = zero(eltype(Q))\n                WxP = WyP = WzP = zero(eltype(Q))\n                ExP = EyP = EzP = zero(eltype(Q))\n                if bc == 0\n                    ρxP = gradQ[vidP, _ρ, 1, eP]\n                    ρyP = gradQ[vidP, _ρ, 2, eP]\n                    ρzP = gradQ[vidP, _ρ, 3, eP]\n                    UxP = gradQ[vidP, _U, 1, eP]\n                    UyP = gradQ[vidP, _U, 2, eP]\n                    UzP = gradQ[vidP, _U, 3, eP]\n                    VxP = gradQ[vidP, _V, 1, eP]\n                    VyP = gradQ[vidP, _V, 2, eP]\n                    VzP = gradQ[vidP, _V, 3, eP]\n                    WxP = gradQ[vidP, _W, 1, eP]\n                    WyP = gradQ[vidP, _W, 2, eP]\n                    WzP = gradQ[vidP, _W, 3, eP]\n                    ExP = gradQ[vidP, _E, 1, eP]\n                    EyP = gradQ[vidP, _E, 2, eP]\n                    EzP = gradQ[vidP, _E, 3, eP]\n                    ρP = Q[vidP, _ρ, eP]\n                    UP = Q[vidP, _U, eP]\n                    VP = Q[vidP, _V, eP]\n                    WP = Q[vidP, _W, eP]\n\n                    uP=UP/ρP\n                    vP=VP/ρP\n                    wP=WP/ρP\n                    uxP, uyP, uzP = UxP, UyP, UzP\n                    vxP, vyP, vzP = VxP, VyP, VzP\n                    wxP, wyP, wzP = WxP, WyP, WzP\n                    TxP, TyP, TzP = ExP, EyP, EzP\n                elseif bc == 1\n                    ρnM = nxM * ρxM + nyM * ρyM + nzM * ρzM\n                    ρxP = ρxM - 2 * ρnM * nxM\n                    ρyP = ρyM - 2 * ρnM * nyM\n                    ρzP = ρzM - 2 * ρnM * nzM\n                    UnM = nxM * UxM + nyM * UyM + nzM * UzM\n                    UxP = UxM - 2 * UnM * nxM\n                    UyP = UyM - 2 * UnM * nyM\n                    UzP = UzM - 2 * UnM * nzM\n                    VnM = nxM * VxM + nyM * VyM + nzM * VzM\n                    VxP = VxM - 2 * VnM * nxM\n                    VyP = VyM - 2 * VnM * nyM\n                    VzP = VzM - 2 * VnM * nzM\n                    WnM = nxM * WxM + nyM * WyM + nzM * WzM\n                    WxP = WxM - 2 * WnM * nxM\n                    WyP = WyM - 2 * WnM * nyM\n                    WzP = WzM - 2 * WnM * nzM\n                    EnM = nxM * ExM + nyM * EyM + nzM * EzM\n                    ExP = ExM - 2 * EnM * nxM\n                    EyP = EyM - 2 * EnM * nyM\n                    EzP = EzM - 2 * EnM * nzM\n\n                    unM = nxM * uM + nyM * vM + nzM * wM\n                    uP = uM - 2 * unM * nxM\n                    vP = vM - 2 * unM * nyM\n                    wP = wM - 2 * unM * nzM\n                    uxP, uyP, uzP = UxP, UyP, UzP #FXG: Not sure about this BC\n                    vxP, vyP, vzP = VxP, VyP, VzP #FXG: Not sure about this BC\n                    wxP, wyP, wzP = WxP, WyP, WzP #FXG: Not sure about this BC\n                    #TxP, TyP, TzP = ExP, EyP, EzP #Produces thermal boundary layer\n                    TxP, TyP, TzP = TxM, TyM, TzM\n                else\n                    error(\"Invalid boundary conditions $bc on face $f of element $e\")\n                end\n\n                #Left Fluxes\n                div_uM=uxM + vyM + wzM\n                fluxρM_x = 0*ρxM\n                fluxρM_y = 0*ρyM\n                fluxρM_z = 0*ρzM\n                fluxUM_x = 2*uxM + lambda*div_uM\n                fluxUM_y = uyM + vxM\n                fluxUM_z = uzM + wxM\n                fluxVM_x = vxM + uyM\n                fluxVM_y = 2*vyM + lambda*div_uM\n                fluxVM_z = vzM + wyM\n                fluxWM_x = wxM + uzM\n                fluxWM_y = wyM + vzM\n                fluxWM_z = 2*wzM + lambda*div_uM\n                fluxEM_x = uM*(2*uxM + lambda*div_uM) + vM*(uyM + vxM) + wM*(uzM + wxM) + c_p/Pr*TxM\n                fluxEM_y = uM*(vxM + uyM) + vM*(2*vyM + lambda*div_uM) + wM*(vzM + wyM) + c_p/Pr*TyM\n                fluxEM_z = uM*(wxM + uzM) + vM*(wyM + vzM) + wM*(2*wzM + lambda*div_uM) + c_p/Pr*TzM\n\n                #Right Fluxes\n                div_uP=uxP + vyP + wzP\n                fluxρP_x = 0*ρxP\n                fluxρP_y = 0*ρyP\n                fluxρP_z = 0*ρzP\n                fluxUP_x = 2*uxP + lambda*div_uP\n                fluxUP_y = uyP + vxP\n                fluxUP_z = uzP + wxP\n                fluxVP_x = vxP + uyP\n                fluxVP_y = 2*vyP + lambda*div_uP\n                fluxVP_z = vzP + wyP\n                fluxWP_x = wxP + uzP\n                fluxWP_y = wyP + vzP\n                fluxWP_z = 2*wzP + lambda*div_uP\n                fluxEP_x = uP*(2*uxP + lambda*div_uP) + vP*(uyP + vxP) + wP*(uzP + wxP) + c_p/Pr*TxP\n                fluxEP_y = uP*(vxP + uyP) + vP*(2*vyP + lambda*div_uP) + wP*(vzP + wyP) + c_p/Pr*TyP\n                fluxEP_z = uP*(wxP + uzP) + vP*(wyP + vzP) + wP*(2*wzP + lambda*div_uP) + c_p/Pr*TzP\n\n                #Compute Numerical Flux\n                fluxρS = 0.5*(nxM * (fluxρM_x + fluxρP_x) + nyM * (fluxρM_y + fluxρP_y) + nzM * (fluxρM_z + fluxρP_z))\n                fluxUS = 0.5*(nxM * (fluxUM_x + fluxUP_x) + nyM * (fluxUM_y + fluxUP_y) + nzM * (fluxUM_z + fluxUP_z))\n                fluxVS = 0.5*(nxM * (fluxVM_x + fluxVP_x) + nyM * (fluxVM_y + fluxVP_y) + nzM * (fluxVM_z + fluxVP_z))\n                fluxWS = 0.5*(nxM * (fluxWM_x + fluxWP_x) + nyM * (fluxWM_y + fluxWP_y) + nzM * (fluxWM_z + fluxWP_z))\n                fluxES = 0.5*(nxM * (fluxEM_x + fluxEP_x) + nyM * (fluxEM_y + fluxEP_y) + nzM * (fluxEM_z + fluxEP_z))\n\n                #Update RHS\n                rhs[vidM, _ρ, 1, eM] += sMJ * fluxρS\n                rhs[vidM, _U, 1, eM] += sMJ * fluxUS\n                rhs[vidM, _V, 1, eM] += sMJ * fluxVS\n                rhs[vidM, _W, 1, eM] += sMJ * fluxWS\n                rhs[vidM, _E, 1, eM] += sMJ * fluxES\n            end\n        end\n    end\nend}}}{{{ Update grad Q solutionfunction update_gradQ!(::Val{dim}, ::Val{N}, Q, rhs, vgeo, elems) where {dim, N}\n\n    Nq=(N+1)^dim\n\n    @inbounds for e = elems, s = 1:_nstate, i = 1:Nq\n        Q[i, s, 1, e] = rhs[i, s, 1, e] * vgeo[i, _MJI, e]\n        Q[i, s, 2, e] = rhs[i, s, 2, e] * vgeo[i, _MJI, e]\n        Q[i, s, 3, e] = rhs[i, s, 3, e] * vgeo[i, _MJI, e]\n    end\n\nend}}}{{{ Update grad Q solutionfunction update_divgradQ!(::Val{dim}, ::Val{N}, Q, rhs, vgeo, elems) where {dim, N}\n\n    Nq=(N+1)^dim\n\n    @inbounds for e = elems, s = 1:_nstate, i = 1:Nq\n        Q[i, s, e] = rhs[i, s, 1, e] * vgeo[i, _MJI, e]\n    end\n\nend}}}{{{ Update solution (for all dimensions)function updatesolution!(::Val{dim}, ::Val{N}, rhs::Array, rhs_gradQ, Q, vgeo, elems, rka,\n                         rkb, dt, visc) where {dim, N}\n\n    Nq=(N+1)^dim\n\n    @inbounds for e = elems, s = 1:_nstate, i = 1:Nq\n        rhs[i, s, e] += visc*rhs_gradQ[i,s,1,e]\n        Q[i, s, e] += rkb * dt * rhs[i, s, e] * vgeo[i, _MJI, e]\n        rhs[i, s, e] *= rka\n    end\nend}}} }}}{{{ improved GPU kernles{{{ Volume RHS for 3D@hascuda function knl_volume_rhs!(::Val{3}, ::Val{N}, rhs, Q, vgeo, D, nelem) where N\n    DFloat = eltype(D)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Nq = N + 1\n\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    s_D = @cuStaticSharedMem(eltype(D), (Nq, Nq))\n    s_F = @cuStaticSharedMem(eltype(Q), (Nq, Nq, Nq, _nstate))\n    s_G = @cuStaticSharedMem(eltype(Q), (Nq, Nq, Nq, _nstate))\n    s_H = @cuStaticSharedMem(eltype(Q), (Nq, Nq, Nq, _nstate))\n\n    rhsU = rhsV = rhsW = rhsρ = rhsE = zero(eltype(rhs))\n    @inbounds if i <= Nq && j <= Nq && k <= Nq && e <= nelemLoad derivative into shared memory        if k == 1\n            s_D[i, j] = D[i, j]\n        endLoad values will need into registers        MJ = vgeo[i, j, k, _MJ, e]\n        ξx, ξy, ξz = vgeo[i,j,k,_ξx,e], vgeo[i,j,k,_ξy,e], vgeo[i,j,k,_ξz,e]\n        ηx, ηy, ηz = vgeo[i,j,k,_ηx,e], vgeo[i,j,k,_ηy,e], vgeo[i,j,k,_ηz,e]\n        ζx, ζy, ζz = vgeo[i,j,k,_ζx,e], vgeo[i,j,k,_ζy,e], vgeo[i,j,k,_ζz,e]\n\n        U, V, W = Q[i, j, k, _U, e], Q[i, j, k, _V, e], Q[i, j, k, _W, e]\n        ρ, E = Q[i, j, k, _ρ, e], Q[i, j, k, _E, e]\n\n        P = p0 * CUDAnative.pow(R_gas * E / p0, c_p / c_v)\n\n        ρinv = 1 / ρ\n        fluxρ_x = U\n        fluxU_x = ρinv * U * U + P\n        fluxV_x = ρinv * V * U\n        fluxW_x = ρinv * W * U\n        fluxE_x = E * ρinv * U\n\n        fluxρ_y = V\n        fluxU_y = ρinv * U * V\n        fluxV_y = ρinv * V * V + P\n        fluxW_y = ρinv * W * V\n        fluxE_y = E * ρinv * V\n\n        fluxρ_z = W\n        fluxU_z = ρinv * U * W\n        fluxV_z = ρinv * V * W\n        fluxW_z = ρinv * W * W + P\n        fluxE_z = E * ρinv * W\n\n        s_F[i, j, k, _ρ] = MJ * (ξx * fluxρ_x + ξy * fluxρ_y + ξz * fluxρ_z)\n        s_F[i, j, k, _U] = MJ * (ξx * fluxU_x + ξy * fluxU_y + ξz * fluxU_z)\n        s_F[i, j, k, _V] = MJ * (ξx * fluxV_x + ξy * fluxV_y + ξz * fluxV_z)\n        s_F[i, j, k, _W] = MJ * (ξx * fluxW_x + ξy * fluxW_y + ξz * fluxW_z)\n        s_F[i, j, k, _E] = MJ * (ξx * fluxE_x + ξy * fluxE_y + ξz * fluxE_z)\n\n        s_G[i, j, k, _ρ] = MJ * (ηx * fluxρ_x + ηy * fluxρ_y + ηz * fluxρ_z)\n        s_G[i, j, k, _U] = MJ * (ηx * fluxU_x + ηy * fluxU_y + ηz * fluxU_z)\n        s_G[i, j, k, _V] = MJ * (ηx * fluxV_x + ηy * fluxV_y + ηz * fluxV_z)\n        s_G[i, j, k, _W] = MJ * (ηx * fluxW_x + ηy * fluxW_y + ηz * fluxW_z)\n        s_G[i, j, k, _E] = MJ * (ηx * fluxE_x + ηy * fluxE_y + ηz * fluxE_z)\n\n        s_H[i, j, k, _ρ] = MJ * (ζx * fluxρ_x + ζy * fluxρ_y + ζz * fluxρ_z)\n        s_H[i, j, k, _U] = MJ * (ζx * fluxU_x + ζy * fluxU_y + ζz * fluxU_z)\n        s_H[i, j, k, _V] = MJ * (ζx * fluxV_x + ζy * fluxV_y + ζz * fluxV_z)\n        s_H[i, j, k, _W] = MJ * (ζx * fluxW_x + ζy * fluxW_y + ζz * fluxW_z)\n        s_H[i, j, k, _E] = MJ * (ζx * fluxE_x + ζy * fluxE_y + ζz * fluxE_z)\n\n        rhsU, rhsV, rhsW = (rhs[i, j, k, _U, e],\n                            rhs[i, j, k, _V, e],\n                            rhs[i, j, k, _W, e])\n        rhsρ, rhsE = rhs[i, j, k, _ρ, e], rhs[i, j, k, _E, e]buoyancy term        rhsW -= MJ * ρ * gravity\n    end\n\n    sync_threads()\n\n    @inbounds if i <= Nq && j <= Nq && k <= Nq && e <= nelemloop of ξ-grid lines        for n = 1:Nq\n            Dni = s_D[n, i]\n            Dnj = s_D[n, j]\n            Dnk = s_D[n, k]\n\n            rhsρ += Dni * s_F[n, j, k, _ρ]\n            rhsρ += Dnj * s_G[i, n, k, _ρ]\n            rhsρ += Dnk * s_H[i, j, n, _ρ]\n\n            rhsU += Dni * s_F[n, j, k, _U]\n            rhsU += Dnj * s_G[i, n, k, _U]\n            rhsU += Dnk * s_H[i, j, n, _U]\n\n            rhsV += Dni * s_F[n, j, k, _V]\n            rhsV += Dnj * s_G[i, n, k, _V]\n            rhsV += Dnk * s_H[i, j, n, _V]\n\n            rhsW += Dni * s_F[n, j, k, _W]\n            rhsW += Dnj * s_G[i, n, k, _W]\n            rhsW += Dnk * s_H[i, j, n, _W]\n\n            rhsE += Dni * s_F[n, j, k, _E]\n            rhsE += Dnj * s_G[i, n, k, _E]\n            rhsE += Dnk * s_H[i, j, n, _E]\n        end\n\n        rhs[i, j, k, _U, e] = rhsU\n        rhs[i, j, k, _V, e] = rhsV\n        rhs[i, j, k, _W, e] = rhsW\n        rhs[i, j, k, _ρ, e] = rhsρ\n        rhs[i, j, k, _E, e] = rhsE\n    end\n    nothing\nend}}}{{{ Face RHS (all dimensions)@hascuda function knl_flux_rhs!(::Val{dim}, ::Val{N}, rhs, Q, sgeo, vgeo, nelem, vmapM,\n                               vmapP, elemtobndy) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1) * (N+1) * (N+1)\n    nface = 6\n\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    Nq = N+1\n    half = convert(eltype(Q), 0.5)\n\n    @inbounds if i <= Nq && j <= Nq && k == 1 && e <= nelem\n        n = i + (j-1) * Nq\n        for lf = 1:2:nface\n            for f = lf:lf+1\n                (nxM, nyM) = (sgeo[_nx, n, f, e], sgeo[_ny, n, f, e])\n                (nzM, sMJ) = (sgeo[_nz, n, f, e], sgeo[_sMJ, n, f, e])\n\n                (idM, idP) = (vmapM[n, f, e], vmapP[n, f, e])\n\n                (eM, eP) = (e, ((idP - 1) ÷ Np) + 1)\n                (vidM, vidP) = (((idM - 1) % Np) + 1,  ((idP - 1) % Np) + 1)\n\n                ρM = Q[vidM, _ρ, eM]\n                UM = Q[vidM, _U, eM]\n                VM = Q[vidM, _V, eM]\n                WM = Q[vidM, _W, eM]\n                EM = Q[vidM, _E, eM]\n\n                bc = elemtobndy[f, e]\n                PM = p0 * CUDAnative.pow(R_gas * EM / p0, c_p / c_v)\n                ρP = UP = VP = WP = EP = PP = zero(eltype(Q))\n                if bc == 0\n                    ρP = Q[vidP, _ρ, eP]\n                    UP = Q[vidP, _U, eP]\n                    VP = Q[vidP, _V, eP]\n                    WP = Q[vidP, _W, eP]\n                    EP = Q[vidP, _E, eP]\n                    PP = p0 * CUDAnative.pow(R_gas * EP / p0, c_p / c_v)\n                elseif bc == 1\n                    UnM = nxM * UM + nyM * VM + nzM * WM\n                    UP = UM - 2 * UnM * nxM\n                    VP = VM - 2 * UnM * nyM\n                    WP = WM - 2 * UnM * nzM\n                    ρP = ρM\n                    EP = EM\n                    PP = PM\n                end\n\n                ρMinv = 1 / ρM\n                fluxρM_x = UM\n                fluxUM_x = ρMinv * UM * UM + PM\n                fluxVM_x = ρMinv * VM * UM\n                fluxWM_x = ρMinv * WM * UM\n                fluxEM_x = ρMinv * UM * EM\n\n                fluxρM_y = VM\n                fluxUM_y = ρMinv * UM * VM\n                fluxVM_y = ρMinv * VM * VM + PM\n                fluxWM_y = ρMinv * WM * VM\n                fluxEM_y = ρMinv * VM * EM\n\n                fluxρM_z = WM\n                fluxUM_z = ρMinv * UM * WM\n                fluxVM_z = ρMinv * VM * WM\n                fluxWM_z = ρMinv * WM * WM + PM\n                fluxEM_z = ρMinv * WM * EM\n\n                ρPinv = 1 / ρP\n                fluxρP_x = UP\n                fluxUP_x = ρPinv * UP * UP + PP\n                fluxVP_x = ρPinv * VP * UP\n                fluxWP_x = ρPinv * WP * UP\n                fluxEP_x = ρPinv * UP * EP\n\n                fluxρP_y = VP\n                fluxUP_y = ρPinv * UP * VP\n                fluxVP_y = ρPinv * VP * VP + PP\n                fluxWP_y = ρPinv * WP * VP\n                fluxEP_y = ρPinv * VP * EP\n\n                fluxρP_z = WP\n                fluxUP_z = ρPinv * UP * WP\n                fluxVP_z = ρPinv * VP * WP\n                fluxWP_z = ρPinv * WP * WP + PP\n                fluxEP_z = ρPinv * WP * EP\n\n                λM = ρMinv * abs(nxM * UM + nyM * VM + nzM * WM) + CUDAnative.sqrt(ρMinv * γ * PM)\n                λP = ρPinv * abs(nxM * UP + nyM * VP + nzM * WP) + CUDAnative.sqrt(ρPinv * γ * PP)\n                λ  =  max(λM, λP)\n\n                #Compute Numerical Flux and Update\n                fluxρS = (nxM * (fluxρM_x + fluxρP_x) + nyM * (fluxρM_y + fluxρP_y) +\n                          nzM * (fluxρM_z + fluxρP_z) - λ * (ρP - ρM)) / 2\n                fluxUS = (nxM * (fluxUM_x + fluxUP_x) + nyM * (fluxUM_y + fluxUP_y) +\n                          nzM * (fluxUM_z + fluxUP_z) - λ * (UP - UM)) / 2\n                fluxVS = (nxM * (fluxVM_x + fluxVP_x) + nyM * (fluxVM_y + fluxVP_y) +\n                          nzM * (fluxVM_z + fluxVP_z) - λ * (VP - VM)) / 2\n                fluxWS = (nxM * (fluxWM_x + fluxWP_x) + nyM * (fluxWM_y + fluxWP_y) +\n                          nzM * (fluxWM_z + fluxWP_z) - λ * (WP - WM)) / 2\n                fluxES = (nxM * (fluxEM_x + fluxEP_x) + nyM * (fluxEM_y + fluxEP_y) +\n                          nzM * (fluxEM_z + fluxEP_z) - λ * (EP - EM)) / 2\n\n                #Update RHS\n                rhs[vidM, _ρ, eM] -= sMJ * fluxρS\n                rhs[vidM, _U, eM] -= sMJ * fluxUS\n                rhs[vidM, _V, eM] -= sMJ * fluxVS\n                rhs[vidM, _W, eM] -= sMJ * fluxWS\n                rhs[vidM, _E, eM] -= sMJ * fluxES\n            end\n            sync_threads()\n        end\n    end\nnothing\nend}}}{{{ Update solution (for all dimensions)@hascuda function knl_updatesolution!(::Val{dim}, ::Val{N}, rhs, Q, vgeo, nelem, rka,\n                                      rkb, dt) where {dim, N}\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    Nq = N+1\n    @inbounds if i <= Nq && j <= Nq && k <= Nq && e <= nelem\n        n = i + (j-1) * Nq + (k-1) * Nq * Nq\n        MJI = vgeo[n, _MJI, e]\n        for s = 1:_nstate\n            Q[n, s, e] += rkb * dt * rhs[n, s, e] * MJI\n            rhs[n, s, e] *= rka\n        end\n    end\n    nothing\nend}}}}}}{{{ Fill sendQ on device with Q (for all dimensions)@hascuda function knl_fillsendQ!(::Val{dim}, ::Val{N}, sendQ, Q,\n                                 sendelems) where {N, dim}\n    Nq = N + 1\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    @inbounds if i <= Nq && j <= Nq && k <= Nq && e <= length(sendelems)\n        n = i + (j-1) * Nq + (k-1) * Nq * Nq\n        re = sendelems[e]\n        for s = 1:_nstate\n            sendQ[n, s, e] = Q[n, s, re]\n        end\n    end\n    nothing\nend}}}{{{ Fill Q on device with recvQ (for all dimensions)@hascuda function knl_transferrecvQ!(::Val{dim}, ::Val{N}, Q, recvQ, nelem,\n                                     nrealelem) where {N, dim}\n    Nq = N + 1\n    (i, j, k) = threadIdx()\n    e = blockIdx().x\n\n    @inbounds if i <= Nq && j <= Nq && k <= Nq && e <= nelem\n        n = i + (j-1) * Nq + (k-1) * Nq * Nq\n        for s = 1:_nstate\n            Q[n, s, nrealelem + e] = recvQ[n, s, e]\n        end\n    end\n    nothing\nend}}}{{{ MPI Buffer handlingfunction fillsendQ!(::Val{dim}, ::Val{N}, sendQ, d_sendQ::Array, Q,\n                    sendelems) where {dim, N}\n    sendQ[:, :, :] .= Q[:, :, sendelems]\nend\n\n@hascuda function fillsendQ!(::Val{dim}, ::Val{N}, sendQ, d_sendQ::CuArray,\n                             d_QL, d_sendelems) where {dim, N}\n    nsendelem = length(d_sendelems)\n    if nsendelem > 0\n        @cuda(threads=ntuple(j->N+1, dim), blocks=nsendelem,\n              knl_fillsendQ!(Val(dim), Val(N), d_sendQ, d_QL, d_sendelems))\n        sendQ .= d_sendQ\n    end\nend\n\n@hascuda function transferrecvQ!(::Val{dim}, ::Val{N}, d_recvQ::CuArray, recvQ,\n                                 d_QL, nrealelem) where {dim, N}\n    nrecvelem = size(recvQ)[end]\n    if nrecvelem > 0\n        d_recvQ .= recvQ\n        @cuda(threads=ntuple(j->N+1, dim), blocks=nrecvelem,\n              knl_transferrecvQ!(Val(dim), Val(N), d_QL, d_recvQ, nrecvelem,\n                                 nrealelem))\n    end\nend\n\nfunction transferrecvQ!(::Val{dim}, ::Val{N}, d_recvQ::Array, recvQ, Q,\n                        nrealelem) where {dim, N}\n    Q[:, :, nrealelem+1:end] .= recvQ[:, :, :]\nend}}}{{{ GPU kernel wrappers@hascuda function volume_rhs!(::Val{dim}, ::Val{N}, d_rhsC::CuArray, d_QC,\n                             d_vgeoC, d_D, elems) where {dim, N}\n    nelem = length(elems)\n    @cuda(threads=ntuple(j->N+1, dim), blocks=nelem,\n          knl_volume_rhs!(Val(dim), Val(N), d_rhsC, d_QC, d_vgeoC, d_D, nelem))\nend\n\n@hascuda function flux_rhs!(::Val{dim}, ::Val{N}, d_rhsL::CuArray, d_QL, d_sgeo,\n                           d_vgeoL, elems, d_vmapM, d_vmapP, d_elemtobndy) where {dim, N}\n    nelem = length(elems)\n    @cuda(threads=(ntuple(j->N+1, dim-1)..., 1), blocks=nelem,\n          knl_flux_rhs!(Val(dim), Val(N), d_rhsL, d_QL, d_sgeo, d_vgeoL, nelem, d_vmapM,\n                       d_vmapP, d_elemtobndy))\nend\n\n@hascuda function updatesolution!(::Val{dim}, ::Val{N}, d_rhsL::CuArray, d_QL,\n                                  d_vgeoL, elems, rka, rkb, dt) where {dim, N}\n    nelem = length(elems)\n    @cuda(threads=ntuple(j->N+1, dim), blocks=nelem,\n          knl_updatesolution!(Val(dim), Val(N), d_rhsL, d_QL, d_vgeoL, nelem, rka,\n                              rkb, dt))\nend}}}{{{ L2 Energy (for all dimensions)function L2energysquared(::Val{dim}, ::Val{N}, Q, vgeo, elems) where {dim, N}\n    DFloat = eltype(Q)\n    Np = (N+1)^dim\n    (~, nstate, nelem) = size(Q)\n\n    energy = zero(DFloat)\n\n    @inbounds for e = elems, q = 1:nstate, i = 1:Np\n        energy += vgeo[i, _MJ, e] * Q[i, q, e]^2\n    end\n\n    energy\nend}}}{{{ Send Data Qfunction senddata_Q(::Val{dim}, ::Val{N}, mesh, sendreq, recvreq, sendQ,\n                  recvQ, d_sendelems, d_sendQ, d_recvQ, d_QL, mpicomm;\n                  ArrType=ArrType) where {dim, N}\n    mpirank = MPI.Comm_rank(mpicomm)Create send and recv request array    nnabr = length(mesh.nabrtorank)\n    d_sendelems = ArrType(mesh.sendelems)\n    nrealelem = length(mesh.realelems)post MPI receives    for n = 1:nnabr\n        recvreq[n] = MPI.Irecv!((@view recvQ[:, :, mesh.nabrtorecv[n]]),\n                                mesh.nabrtorank[n], 777, mpicomm)\n    endwait on (prior) MPI sends    MPI.Waitall!(sendreq)pack data from dQL into send buffer    fillsendQ!(Val(dim), Val(N), sendQ, dsendQ, dQL, dsendelems)    sendQ[:, :, :] .= d_QL[:, :, d_sendelems]post MPI sends    for n = 1:nnabr\n        sendreq[n] = MPI.Isend((@view sendQ[:, :, mesh.nabrtosend[n]]),\n                               mesh.nabrtorank[n], 777, mpicomm)\n    end\nend}}}{{{ Send Data Grad(Q)function senddata_gradQ(::Val{dim}, ::Val{N}, mesh, sendreq, recvreq, sendQ,\n                  recvQ, d_sendelems, d_sendQ, d_recvQ, d_QL, mpicomm;\n                  ArrType=ArrType) where {dim, N}\n    mpirank = MPI.Comm_rank(mpicomm)Create send and recv request array    nnabr = length(mesh.nabrtorank)\n    d_sendelems = ArrType(mesh.sendelems)\n    nrealelem = length(mesh.realelems)post MPI receives    for n = 1:nnabr\n        recvreq[n] = MPI.Irecv!((@view recvQ[:, :, :, mesh.nabrtorecv[n]]),\n                                mesh.nabrtorank[n], 777, mpicomm)\n    endwait on (prior) MPI sends    MPI.Waitall!(sendreq)pack data from dQL into send buffer    fillsendQ!(Val(dim), Val(N), sendQ, dsendQ, dQL, dsendelems)    sendQ[:, :, :, :] .= d_QL[:, :, :, d_sendelems]post MPI sends    for n = 1:nnabr\n        sendreq[n] = MPI.Isend((@view sendQ[:, :, :, mesh.nabrtosend[n]]),\n                               mesh.nabrtorank[n], 777, mpicomm)\n    end\nend}}}{{{ Receive Data Qfunction receivedata_Q!(::Val{dim}, ::Val{N}, mesh, recvreq, recvQ,\n                        d_recvQ, d_QL) where {dim, N}\n    nrealelem = length(mesh.realelems)wait on MPI receives    MPI.Waitall!(recvreq)copy data to state vector d_QL    #transferrecvQ!(Val(dim), Val(N), d_recvQ, recvQ, d_QL, nrealelem)\n    d_QL[:, :, nrealelem+1:end] .= recvQ[:, :, :]\n\nend}}}{{{ Receive Data Grad(Q)function receivedata_gradQ!(::Val{dim}, ::Val{N}, mesh, recvreq, recvQ,\n                            d_recvQ, d_QL) where {dim, N}\n    nrealelem = length(mesh.realelems)wait on MPI receives    MPI.Waitall!(recvreq)copy data to state vector d_QL    #transferrecvQ!(Val(dim), Val(N), d_recvQ, recvQ, d_QL, nrealelem)\n    d_QL[:, :, :, nrealelem+1:end] .= recvQ[:, :, :, :]\nend}}}{{{ RK loopfunction lowstorageRK(::Val{dim}, ::Val{N}, mesh, vgeo, sgeo, Q, rhs, D,\n                      dt, nsteps, tout, vmapM, vmapP, mpicomm, iplot, visc;\n                      ArrType=ArrType, plotstep=0) where {dim, N}\n    DFloat = eltype(Q)\n    mpirank = MPI.Comm_rank(mpicomm)Fourth-order, low-storage, Runge–Kutta scheme of Carpenter and Kennedy (1994) ((5,4) 2N-Storage RK scheme.Ref: @TECHREPORT{CarpenterKennedy1994,   author = {M.~H. Carpenter and C.~A. Kennedy},   title = {Fourth-order {2N-storage} {Runge-Kutta} schemes},   institution = {National Aeronautics and Space Administration},   year = {1994},   number = {NASA TM-109112},   address = {Langley Research Center, Hampton, VA}, }    RKA = (DFloat(0),\n           DFloat(-567301805773)  / DFloat(1357537059087),\n           DFloat(-2404267990393) / DFloat(2016746695238),\n           DFloat(-3550918686646) / DFloat(2091501179385),\n           DFloat(-1275806237668) / DFloat(842570457699 ))\n\n    RKB = (DFloat(1432997174477) / DFloat(9575080441755 ),\n           DFloat(5161836677717) / DFloat(13612068292357),\n           DFloat(1720146321549) / DFloat(2090206949498 ),\n           DFloat(3134564353537) / DFloat(4481467310338 ),\n           DFloat(2277821191437) / DFloat(14882151754819))\n\n    RKC = (DFloat(0),\n           DFloat(1432997174477) / DFloat(9575080441755),\n           DFloat(2526269341429) / DFloat(6820363962896),\n           DFloat(2006345519317) / DFloat(3224310063776),\n           DFloat(2802321613138) / DFloat(2924317926251))Create send and recv request array    nnabr = length(mesh.nabrtorank)\n    sendreq = fill(MPI.REQUEST_NULL, nnabr)\n    recvreq = fill(MPI.REQUEST_NULL, nnabr)Create send and recv buffer    sendQ = zeros(DFloat, (N+1)^dim, size(Q,2), length(mesh.sendelems))\n    recvQ = zeros(DFloat, (N+1)^dim, size(Q,2), length(mesh.ghostelems))Create send and recv LDG buffer    sendgradQ = zeros(DFloat, (N+1)^dim, size(Q,2), dim, length(mesh.sendelems))\n    recvgradQ = zeros(DFloat, (N+1)^dim, size(Q,2), dim, length(mesh.ghostelems))Store Constants    nrealelem = length(mesh.realelems)\n    nsendelem = length(mesh.sendelems)\n    nrecvelem = length(mesh.ghostelems)\n    nelem = length(mesh.elems)\n\n    #Create Device Arrays\n    d_QL, d_rhsL = ArrType(Q), ArrType(rhs)\n    d_vgeoL, d_sgeo = ArrType(vgeo), ArrType(sgeo)\n    d_vmapM, d_vmapP = ArrType(vmapM), ArrType(vmapP)\n    d_sendelems, d_elemtobndy = ArrType(mesh.sendelems), ArrType(mesh.elemtobndy)\n    d_sendQ, d_recvQ = ArrType(sendQ), ArrType(recvQ)\n    d_D = ArrType(D)\n    #Create Device LDG Arrays\n    d_gradQL = zeros(DFloat, (N+1)^dim, _nstate, dim, nelem)\n    d_rhs_gradQL = zeros(DFloat, (N+1)^dim, _nstate, dim, nelem)\n    d_sendgradQ, d_recvgradQ = ArrType(sendgradQ), ArrType(recvgradQ)\n\n    #Template Reshape Arrays\n    Qshape    = (fill(N+1, dim)..., size(Q, 2), size(Q, 3))\n    vgeoshape = (fill(N+1, dim)..., _nvgeo, size(Q, 3))\n    gradQshape = (fill(N+1, dim)..., size(d_gradQL,2), size(d_gradQL,3), size(d_gradQL,4))\n\n    #Reshape Device Arrays\n    d_QC = reshape(d_QL, Qshape)\n    d_rhsC = reshape(d_rhsL, Qshape...)\n    d_vgeoC = reshape(d_vgeoL, vgeoshape)\n    #Reshape Device LDG Arrays\n    d_gradQC = reshape(d_gradQL, gradQshape)\n    d_rhs_gradQC = reshape(d_rhs_gradQL, gradQshape...)\n\n    start_time = t1 = time_ns()\n    for step = 1:nsteps\n        for s = 1:length(RKA)Send Data Q            senddata_Q(Val(dim), Val(N), mesh, sendreq, recvreq, sendQ,\n                       recvQ, d_sendelems, d_sendQ, d_recvQ, d_QL, mpicomm;\n                       ArrType=ArrType)volume RHS computation            volume_rhs!(Val(dim), Val(N), d_rhsC, d_QC, d_vgeoC, d_D, mesh.realelems)Receive Data Q            receivedata_Q!(Val(dim), Val(N), mesh, recvreq, recvQ, d_recvQ, d_QL)face RHS computation            flux_rhs!(Val(dim), Val(N), d_rhsL, d_QL, d_sgeo, d_vgeoL, mesh.realelems, d_vmapM,\n                     d_vmapP, d_elemtobndy)            if (visc > 0)volume grad Q computation                volume_grad!(Val(dim), Val(N), d_rhs_gradQC, d_QC, d_vgeoC, d_D, mesh.realelems)flux grad Q computation                flux_grad!(Val(dim), Val(N), d_rhs_gradQL, d_QL, d_sgeo, d_vgeoL, mesh.realelems, d_vmapM, d_vmapP, d_elemtobndy)Construct grad Q                update_gradQ!(Val(dim), Val(N), d_gradQL, d_rhs_gradQL, d_vgeoL, mesh.realelems)Send Data grad(Q)                senddata_gradQ(Val(dim), Val(N), mesh, sendreq, recvreq, sendgradQ,\n                               recvgradQ, d_sendelems, d_sendgradQ, d_recvgradQ,\n                               d_gradQL, mpicomm;ArrType=ArrType)volume div(grad Q) computation                volume_div!(Val(dim), Val(N), d_rhs_gradQC, d_gradQC, d_QC, d_vgeoC, d_D, mesh.realelems)Receive Data grad(Q)                receivedata_gradQ!(Val(dim), Val(N), mesh, recvreq, recvgradQ, d_recvgradQ, d_gradQL)flux div(grad Q) computation                flux_div!(Val(dim), Val(N), d_rhs_gradQL, d_gradQL, d_QL, d_sgeo, mesh.realelems, d_vmapM, d_vmapP, d_elemtobndy)\n            endupdate solution and scale RHS            updatesolution!(Val(dim), Val(N), d_rhsL, d_rhs_gradQL, d_QL, d_vgeoL, mesh.realelems,\n                            RKA[s%length(RKA)+1], RKB[s], dt, visc)\n        end\n        if step == 1\n            @hascuda synchronize()\n            start_time = time_ns()\n        end\n        if mpirank == 0 && (time_ns() - t1)*1e-9 > tout\n            @hascuda synchronize()\n            t1 = time_ns()\n            avg_stage_time = (time_ns() - start_time) * 1e-9 / ((step-1) * length(RKA))\n            @show (step, nsteps, avg_stage_time)\n        endWrite VTK file        if mod(step,iplot) == 0\n            Q .= d_QL\n            convert_set3c_to_set2nc(Val(dim), Val(N), vgeo, Q)\n            X = ntuple(j->reshape((@view vgeo[:, _x+j-1, :]), ntuple(j->N+1,dim)..., nelem), dim)\n            ρ = reshape((@view Q[:, _ρ, :]), ntuple(j->(N+1),dim)..., nelem)\n            U = reshape((@view Q[:, _U, :]), ntuple(j->(N+1),dim)..., nelem)\n            V = reshape((@view Q[:, _V, :]), ntuple(j->(N+1),dim)..., nelem)\n            W = reshape((@view Q[:, _W, :]), ntuple(j->(N+1),dim)..., nelem)\n            E = reshape((@view Q[:, _E, :]), ntuple(j->(N+1),dim)..., nelem)\n            E = E .- 300.0\n            writemesh(@sprintf(\"viz/nse%dD_set3c_%s_rank_%04d_step_%05d\",dim, ArrType, mpirank, step), X...;\n                      fields=((\"ρ\", ρ), (\"U\", U), (\"V\", V), (\"W\", W), (\"E\", E)), realelems=mesh.realelems)\n        end\n    end\nif mpirank == 0\n    avg_stage_time = (time_ns() - start_time) * 1e-9 / ((nsteps-1) * length(RKA))\n    @show (nsteps, avg_stage_time)\nend\nQ .= d_QL\nrhs .= d_rhsL\nend}}}{{{ convert_variablesfunction convert_set2nc_to_set2c(::Val{dim}, ::Val{N}, vgeo, Q) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1)^dim\n    (~, ~, nelem) = size(Q)\n\n    println(\"[CPU] converting variables (CPU)...\")\n    @inbounds for e = 1:nelem, n = 1:Np\n        ρ, u, v, w, E = Q[n, _ρ, e], Q[n, _U, e], Q[n, _V, e], Q[n, _W, e], Q[n, _E, e]\n        Q[n, _U, e] = ρ*u\n        Q[n, _V, e] = ρ*v\n        Q[n, _W, e] = ρ*w\n        Q[n, _E, e] = ρ*E\n    end\nend}}}function convert_set2c_to_set2nc(::Val{dim}, ::Val{N}, vgeo, Q) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1)^dim\n    (~, ~, nelem) = size(Q)\n\n    @inbounds for e = 1:nelem, n = 1:Np\n        ρ, U, V, W, E = Q[n, _ρ, e], Q[n, _U, e], Q[n, _V, e], Q[n, _W, e], Q[n, _E, e]\n        u=U/ρ\n        v=V/ρ\n        w=W/ρ\n        E=E/ρ\n        Q[n, _U, e] = u\n        Q[n, _V, e] = v\n        Q[n, _W, e] = w\n        Q[n, _E, e] = E\n    end\nend\n\nfunction convert_set2nc_to_set3c(::Val{dim}, ::Val{N}, vgeo, Q) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1)^dim\n    (~, ~, nelem) = size(Q)\n\n    @inbounds for e = 1:nelem, n = 1:Np\n        ρ, u, v, w, E = Q[n, _ρ, e], Q[n, _U, e], Q[n, _V, e], Q[n, _W, e], Q[n, _E, e]\n        z = vgeo[n, _z, e]\n        P = p0 * (ρ * R_gas * E / p0)^(c_p / c_v)\n        T = P/(ρ*R_gas)\n        E = c_v*T + 0.5*(u^2 + v^2 + w^2) + gravity*z\n        Q[n, _U, e] = ρ*u\n        Q[n, _V, e] = ρ*v\n        Q[n, _W, e] = ρ*w\n        Q[n, _E, e] = ρ*E\n    end\nend\n\nfunction convert_set3c_to_set2nc(::Val{dim}, ::Val{N}, vgeo, Q) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1)^dim\n    (~, ~, nelem) = size(Q)\n\n    @inbounds for e = 1:nelem, n = 1:Np\n        ρ, U, V, W, E = Q[n, _ρ, e], Q[n, _U, e], Q[n, _V, e], Q[n, _W, e], Q[n, _E, e]\n        z = vgeo[n, _z, e]\n        u=U/ρ\n        v=V/ρ\n        w=W/ρ\n        E=E/ρ\n        P = (R_gas/c_v)*ρ*(E - 0.5*(u^2 + v^2 + w^2) - gravity*z)\n        E=p0/(ρ * R_gas)*( P/p0 )^(c_v/c_p)\n        Q[n, _U, e] = u\n        Q[n, _V, e] = v\n        Q[n, _W, e] = w\n        Q[n, _E, e] = E\n    end\nend\n\nfunction convert_set2nc_to_set4c(::Val{dim}, ::Val{N}, vgeo, Q) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1)^dim\n    (~, ~, nelem) = size(Q)\n\n    @inbounds for e = 1:nelem, n = 1:Np\n        ρ, u, v, w, E = Q[n, _ρ, e], Q[n, _U, e], Q[n, _V, e], Q[n, _W, e], Q[n, _E, e]\n        P = p0 * (ρ * R_gas * E / p0)^(c_p / c_v)\n        T = P/(ρ*R_gas)\n        E = c_v*T\n        Q[n, _U, e] = ρ*u\n        Q[n, _V, e] = ρ*v\n        Q[n, _W, e] = ρ*w\n        Q[n, _E, e] = ρ*E\n    end\nend\n\nfunction convert_set4c_to_set2nc(::Val{dim}, ::Val{N}, vgeo, Q) where {dim, N}\n    DFloat = eltype(Q)\n    γ::DFloat       = _γ\n    p0::DFloat      = _p0\n    R_gas::DFloat   = _R_gas\n    c_p::DFloat     = _c_p\n    c_v::DFloat     = _c_v\n    gravity::DFloat = _gravity\n\n    Np = (N+1)^dim\n    (~, ~, nelem) = size(Q)\n\n    @inbounds for e = 1:nelem, n = 1:Np\n        ρ, U, V, W, E = Q[n, _ρ, e], Q[n, _U, e], Q[n, _V, e], Q[n, _W, e], Q[n, _E, e]\n        u=U/ρ\n        v=V/ρ\n        w=W/ρ\n        T=E/(ρ*c_v)\n        P=ρ*R_gas*T\n        E=p0/(ρ* R_gas)*(P/p0)^(c_v/c_p)\n        Q[n, _U, e] = u\n        Q[n, _V, e] = v\n        Q[n, _W, e] = w\n        Q[n, _E, e] = E\n    end\nend}}}{{{ nse driverfunction nse(::Val{dim}, ::Val{N}, mpicomm, ic, mesh, tend, iplot, visc;\n               meshwarp=(x...)->identity(x),\n               tout = 1, ArrType=Array, plotstep=0) where {dim, N}\n    DFloat = typeof(tend)\n\n    mpirank = MPI.Comm_rank(mpicomm)\n    mpisize = MPI.Comm_size(mpicomm)Partion the mesh using a Hilbert curve based partitioning    mpirank == 0 && println(\"[CPU] partitioning mesh...\")\n    mesh = partition(mpicomm, mesh...)Connect the mesh in parallel    mpirank == 0 && println(\"[CPU] connecting mesh...\")\n    mesh = connectmesh(mpicomm, mesh...)Get the vmaps    mpirank == 0 && println(\"[CPU] computing mappings...\")\n    (vmapM, vmapP) = mappings(N, mesh.elemtoelem, mesh.elemtoface,\n                              mesh.elemtoordr)Create 1-D operators    (ξ, ω) = lglpoints(DFloat, N)\n    D = spectralderivative(ξ)Compute the geometry    mpirank == 0 && println(\"[CPU] computing metrics...\")\n    (vgeo, sgeo) = computegeometry(Val(dim), mesh, D, ξ, ω, meshwarp, vmapM)\n    (nface, nelem) = size(mesh.elemtoelem)Storage for the solution, rhs, and error    mpirank == 0 && println(\"[CPU] creating fields (CPU)...\")\n    Q = zeros(DFloat, (N+1)^dim, _nstate, nelem)\n    rhs = zeros(DFloat, (N+1)^dim, _nstate, nelem)setup the initial condition    mpirank == 0 && println(\"[CPU] computing initial conditions (CPU)...\")\n    @inbounds for e = 1:nelem, i = 1:(N+1)^dim\n        x, y, z = vgeo[i, _x, e], vgeo[i, _y, e], vgeo[i, _z, e]\n        ρ, U, V, W, E = ic(x, y, z)\n        Q[i, _ρ, e] = ρ\n        Q[i, _U, e] = U\n        Q[i, _V, e] = V\n        Q[i, _W, e] = W\n        Q[i, _E, e] = E\n    endConvert to proper variables    mpirank == 0 && println(\"[CPU] converting variables (CPU)...\")\n    convert_set2nc_to_set3c(Val(dim), Val(N), vgeo, Q)Compute time step    mpirank == 0 && println(\"[CPU] computing dt (CPU)...\")\n    (base_dt, Courant) = courantnumber(Val(dim), Val(N), vgeo, Q, mpicomm)\n    #base_dt=0.02\n    mpirank == 0 && @show (base_dt, Courant)\n\n    nsteps = ceil(Int64, tend / base_dt)\n    dt = tend / nsteps\n    mpirank == 0 && @show (dt, nsteps, dt * nsteps, tend)Do time stepping    stats = zeros(DFloat, 2)\n    mpirank == 0 && println(\"[CPU] computing initial energy...\")\n    Q_temp=copy(Q)\n    convert_set3c_to_set2nc(Val(dim), Val(N), vgeo, Q_temp)\n    stats[1] = L2energysquared(Val(dim), Val(N), Q_temp, vgeo, mesh.realelems)\n    @show (sqrt.(stats[1]))Write VTK file: plot the initial condition    mkpath(\"viz\")\n    X = ntuple(j->reshape((@view vgeo[:, _x+j-1, :]), ntuple(j->N+1,dim)...,\n                          nelem), dim)\n    ρ = reshape((@view Q_temp[:, _ρ, :]), ntuple(j->(N+1),dim)..., nelem)\n    U = reshape((@view Q_temp[:, _U, :]), ntuple(j->(N+1),dim)..., nelem)\n    V = reshape((@view Q_temp[:, _V, :]), ntuple(j->(N+1),dim)..., nelem)\n    W = reshape((@view Q_temp[:, _W, :]), ntuple(j->(N+1),dim)..., nelem)\n    E = reshape((@view Q_temp[:, _E, :]), ntuple(j->(N+1),dim)..., nelem)\n    E = E .- 300.0\n    writemesh(@sprintf(\"viz/nse%dD_set3c_%s_rank_%04d_step_%05d\",\n                       dim, ArrType, mpirank, 0), X...;\n              fields=((\"ρ\", ρ), (\"U\", U), (\"V\", V), (\"W\", W), (\"E\", E)),\n              realelems=mesh.realelems)\n\n    mpirank == 0 && println(\"[DEV] starting time stepper...\")\n    lowstorageRK(Val(dim), Val(N), mesh, vgeo, sgeo, Q, rhs, D, dt, nsteps, tout,\n                 vmapM, vmapP, mpicomm, iplot, visc; ArrType=ArrType, plotstep=plotstep)Write VTK: final solution    Q_temp=copy(Q)\n    convert_set3c_to_set2nc(Val(dim), Val(N), vgeo, Q_temp)\n    X = ntuple(j->reshape((@view vgeo[:, _x+j-1, :]), ntuple(j->N+1,dim)...,\n                          nelem), dim)\n    ρ = reshape((@view Q_temp[:, _ρ, :]), ntuple(j->(N+1),dim)..., nelem)\n    U = reshape((@view Q_temp[:, _U, :]), ntuple(j->(N+1),dim)..., nelem)\n    V = reshape((@view Q_temp[:, _V, :]), ntuple(j->(N+1),dim)..., nelem)\n    W = reshape((@view Q_temp[:, _W, :]), ntuple(j->(N+1),dim)..., nelem)\n    E = reshape((@view Q_temp[:, _E, :]), ntuple(j->(N+1),dim)..., nelem)\n    E = E .- 300.0\n    writemesh(@sprintf(\"viz/nse%dD_set3c_%s_rank_%04d_step_%05d\",\n                       dim, ArrType, mpirank, nsteps), X...;\n              fields=((\"ρ\", ρ), (\"U\", U), (\"V\", V), (\"W\", W), (\"E\", E)),\n              realelems=mesh.realelems)\n\n    mpirank == 0 && println(\"[CPU] computing final energy...\")\n    stats[2] = L2energysquared(Val(dim), Val(N), Q_temp, vgeo, mesh.realelems)\n\n    stats = sqrt.(MPI.allreduce(stats, MPI.SUM, mpicomm))\n\n    if  mpirank == 0\n        @show eng0 = stats[1]\n        @show engf = stats[2]\n        @show Δeng = engf - eng0\n    end\nend}}}{{{ mainfunction main()\n    DFloat = Float64MPI.Init()    MPI.Initialized() || MPI.Init()\n    MPI.finalize_atexit()\n\n    mpicomm = MPI.COMM_WORLD\n    mpirank = MPI.Comm_rank(mpicomm)\n    mpisize = MPI.Comm_size(mpicomm)FIXME: query via hostname    @hascuda device!(mpirank % length(devices()))\n\n    #Initial Conditions\n    function ic(dim, x...)FIXME: Type generic?        DFloat = eltype(x)\n        γ::DFloat       = _γ\n        p0::DFloat      = _p0\n        R_gas::DFloat   = _R_gas\n        c_p::DFloat     = _c_p\n        c_v::DFloat     = _c_v\n        gravity::DFloat = _gravity\n\n        u0 = 0\n        r = sqrt((x[1]-500)^2 + (x[dim]-350)^2 )\n        rc = 250.0\n        θ_ref=300.0\n        θ_c=0.5\n        Δθ=0.0\n        if r <= rc\n            Δθ = 0.5 * θ_c * (1.0 + cos(π * r/rc))\n        end\n        θ_k=θ_ref + Δθ\n        π_k=1.0 - gravity/(c_p*θ_k)*x[dim]\n        c=c_v/R_gas\n        ρ_k=p0/(R_gas*θ_k)*(π_k)^c\n        ρ = ρ_k\n        U = u0\n        V = 0.0\n        W = 0.0\n        E = θ_k\n        ρ, U, V, W, E\n    end\n\n    time_final = DFloat(10.0)\n    iplot=100\n    Ne = 10\n    N  = 4\n    visc = 1.0\n    dim = 3\n    hardware=\"cpu\"\n    @show (N,Ne,visc,iplot,time_final,hardware,mpisize)\n\n    mesh3D = brickmesh((range(DFloat(0); length=Ne+1, stop=1000),\n                        range(DFloat(0); length=2, stop=1000),\n                        range(DFloat(0); length=Ne+1, stop=1000)),\n                       (true, true, false),\n                       part=mpirank+1, numparts=mpisize)\n\n    if hardware == \"cpu\"\n        mpirank == 0 && println(\"Running 3d (CPU)...\")\n        nse(Val(dim), Val(N), mpicomm, (x...)->ic(dim, x...), mesh3D, time_final, iplot, visc;\n              ArrType=Array, tout = 10)\n        mpirank == 0 && println()\n    elseif hardware == \"gpu\"\n        @hascuda begin\n            mpirank == 0 && println(\"Running 3d (GPU)...\")\n            nse(Val(dim), Val(N), mpicomm, (x...)->ic(dim, x...), mesh3D, time_final, iplot, visc;\n                  ArrType=CuArray, tout = 10)\n            mpirank == 0 && println()\n        end\n    end\n    nothing\nend}}}main()This page was generated using Literate.jl."
},

{
    "location": "reference/mesh/#",
    "page": "Mesh",
    "title": "Mesh",
    "category": "page",
    "text": "DocTestSetup = :(using Canary)"
},

{
    "location": "reference/mesh/#Canary.brickmesh",
    "page": "Mesh",
    "title": "Canary.brickmesh",
    "category": "function",
    "text": "brickmesh(x, periodic; part=1, numparts=1; boundary)\n\nGenerate a brick mesh with coordinates given by the tuple x and the periodic dimensions given by the periodic tuple.\n\nThe brick can optionally be partitioned into numparts and this returns partition part.  This is a simple Cartesian partition and further partitioning (e.g, based on a space-filling curve) should be done before the mesh is used for computation.\n\nBy default boundary faces will be marked with a one and other faces with a zero.  Specific boundary numbers can also be passed for each face of the brick in boundary.  This will mark the nonperiodic brick faces with the given boundary number.\n\nExamples\n\nWe can build a 3 by 2 element two-dimensional mesh that is periodic in the x_2-direction with\n\njulia> using Canary\njulia> (elemtovert, elemtocoord, elemtobndy, faceconnections) =\n        brickmesh((2:5,4:6), (false,true); boundary=[1 3; 2 4]);\n\nThis returns the mesh structure for\n\n         x_2\n\n          ^\n          |\n         6-  9----10----11----12\n          |  |     |     |     |\n          |  |  4  |  5  |  6  |\n          |  |     |     |     |\n         5-  5-----6-----7-----8\n          |  |     |     |     |\n          |  |  1  |  2  |  3  |\n          |  |     |     |     |\n         4-  1-----2-----3-----4\n          |\n          +--|-----|-----|-----|--> x_1\n             2     3     4     5\n\nThe (number of corners by number of elements) array elemtovert gives the global vertex number for the corners of each element.\n\njulia> elemtovert\n4×6 Array{Int64,2}:\n 1  2  3   5   6   7\n 2  3  4   6   7   8\n 5  6  7   9  10  11\n 6  7  8  10  11  12\n\nNote that the vertices are listed in Cartesian order.\n\nThe (dimension by number of corners by number of elements) array elemtocoord gives the coordinates of the corners of each element.\n\njulia> elemtocoord\n2×4×6 Array{Int64,3}:\n[:, :, 1] =\n 2  3  2  3\n 4  4  5  5\n\n[:, :, 2] =\n 3  4  3  4\n 4  4  5  5\n\n[:, :, 3] =\n 4  5  4  5\n 4  4  5  5\n\n[:, :, 4] =\n 2  3  2  3\n 5  5  6  6\n\n[:, :, 5] =\n 3  4  3  4\n 5  5  6  6\n\n[:, :, 6] =\n 4  5  4  5\n 5  5  6  6\n\nThe (number of faces by number of elements) array elemtobndy gives the boundary number for each face of each element.  A zero will be given for connected faces.\n\njulia> elemtobndy\n4×6 Array{Int64,2}:\n 1  0  0  1  0  0\n 0  0  2  0  0  2\n 0  0  0  0  0  0\n 0  0  0  0  0  0\n\nNote that the faces are listed in Cartesian order.\n\nFinally, the periodic face connections are given in faceconnections which is a list of arrays, one for each connection. Each array in the list is given in the format [e, f, vs...] where\n\ne  is the element number;\nf  is the face number; and\nvs is the global vertices that face associated with.\n\nI the example\n\njulia> faceconnections\n3-element Array{Array{Int64,1},1}:\n [4, 4, 1, 2]\n [5, 4, 2, 3]\n [6, 4, 3, 4]\n\nwe see that face 4 of element 5 is associated with vertices [2 3] (the vertices for face 1 of element 2).\n\n\n\n\n\n"
},

{
    "location": "reference/mesh/#Canary.connectmesh",
    "page": "Mesh",
    "title": "Canary.connectmesh",
    "category": "function",
    "text": "connectmesh(comm::MPI.Comm, elemtovert, elemtocoord, elemtobndy,\n            faceconnections)\n\nThis function takes in a mesh (as returned for example by brickmesh) and returns a connected mesh.  This returns a NamedTuple of:\n\nelems the range of element indices\nrealelems the range of real (aka nonghost) element indices\nghostelems the range of ghost element indices\nsendelems an array of send element indices sorted so that\nelemtocoord element to vertex coordinates; elemtocoord[d,i,e] is the  dth coordinate of corner i of element e\nelemtoelem element to neighboring element; elemtoelem[f,e] is the number of the element neighboring element e across face f.  If there is no neighboring element then elemtoelem[f,e] == e.\nelemtoface element to neighboring element face; elemtoface[f,e] is the face number of the element neighboring element e across face f.  If there is no neighboring element then elemtoface[f,e] == f.\nelemtoordr element to neighboring element order; elemtoordr[f,e] is the ordering number of the element neighboring element e across face f.  If there is no neighboring element then elemtoordr[f,e] == 1.\nelemtobndy element to bounday number; elemtobndy[f,e] is the boundary number of face f of element e.  If there is a neighboring element then elemtobndy[f,e] == 0.\nnabrtorank a list of the MPI ranks for the neighboring processes\nnabrtorecv a range in ghost elements to receive for each neighbor\nnabrtosend a range in sendelems to send for each neighbor\n\n\n\n\n\n"
},

{
    "location": "reference/mesh/#Canary.mappings",
    "page": "Mesh",
    "title": "Canary.mappings",
    "category": "function",
    "text": "mappings(N, elemtoelem, elemtoface, elemtoordr)\n\nThis function takes in a polynomial order N and parts of a mesh (as returned from connectmesh) and returns index mappings for the element surface flux computation.  The returned Tuple contains:\n\nvmapM an array of linear indices into the volume degrees of freedom where vmapM[:,f,e] are the degrees of freedom indices for face f of element  e.\nvmapP an array of linear indices into the volume degrees of freedom where vmapP[:,f,e] are the degrees of freedom indices for the face neighboring face f of element e.\n\n\n\n\n\n"
},

{
    "location": "reference/mesh/#Canary.partition",
    "page": "Mesh",
    "title": "Canary.partition",
    "category": "function",
    "text": "partition(comm::MPI.Comm, elemtovert, elemtocoord, elemtobndy,\n          faceconnections)\n\nThis function takes in a mesh (as returned for example by brickmesh) and returns a Hilbert curve based partitioned mesh.\n\n\n\n\n\n"
},

{
    "location": "reference/mesh/#Mesh-1",
    "page": "Mesh",
    "title": "Mesh",
    "category": "section",
    "text": "brickmesh\nconnectmesh\nmappings\npartition"
},

{
    "location": "reference/metric/#",
    "page": "Metric Terms",
    "title": "Metric Terms",
    "category": "page",
    "text": "DocTestSetup = :(using Canary)"
},

{
    "location": "reference/metric/#Canary.creategrid",
    "page": "Metric Terms",
    "title": "Canary.creategrid",
    "category": "function",
    "text": "creategrid(::Val{1}, elemtocoord::AbstractArray{S, 3},\n           r::AbstractVector{T}) where {S, T}\n\nCreate a grid using elemtocoord (see brickmesh) using the 1-D (-1, 1) reference coordinates r. The element grids are filled using bilinear interpolation of the element coordinates.\n\nThe grid is returned as a tuple of with x array\n\n\n\n\n\ncreategrid(::Val{2}, elemtocoord::AbstractArray{S, 3},\n           r::AbstractVector{T}) where {S, T}\n\nCreate a 2-D tensor product grid using elemtocoord (see brickmesh) using the 1-D (-1, 1) reference coordinates r. The element grids are filled using bilinear interpolation of the element coordinates.\n\nThe grid is returned as a tuple of the x and y arrays\n\n\n\n\n\ncreategrid(::Val{3}, elemtocoord::AbstractArray{S, 3},\n           r::AbstractVector{T}) where {S, T}\n\nCreate a 3-D tensor product grid using elemtocoord (see brickmesh) using the 1-D (-1, 1) reference coordinates r. The element grids are filled using bilinear interpolation of the element coordinates.\n\nThe grid is returned as a tuple of the x, y, z arrays\n\n\n\n\n\n"
},

{
    "location": "reference/metric/#Canary.creategrid!",
    "page": "Metric Terms",
    "title": "Canary.creategrid!",
    "category": "function",
    "text": "creategrid!(x, elemtocoord, r)\n\nCreate a 1-D grid using elemtocoord (see brickmesh) using the 1-D (-1, 1) reference coordinates r. The element grids are filled using linear interpolation of the element coordinates.\n\nIf Nq = length(r) and nelem = size(elemtocoord, 3) then the preallocated array x should be Nq * nelem == length(x).\n\n\n\n\n\ncreategrid!(x, y, elemtocoord, r)\n\nCreate a 2-D tensor product grid using elemtocoord (see brickmesh) using the 1-D (-1, 1) reference coordinates r. The element grids are filled using bilinear interpolation of the element coordinates.\n\nIf Nq = length(r) and nelem = size(elemtocoord, 3) then the preallocated arrays x and y should be Nq^2 * nelem == size(x) == size(y).\n\n\n\n\n\ncreategrid!(x, y, z, elemtocoord, r)\n\nCreate a 3-D tensor product grid using elemtocoord (see brickmesh) using the 1-D (-1, 1) reference coordinates r. The element grids are filled using trilinear interpolation of the element coordinates.\n\nIf Nq = length(r) and nelem = size(elemtocoord, 3) then the preallocated arrays x, y, and z should be Nq^3 * nelem == size(x) == size(y) == size(z).\n\n\n\n\n\n"
},

{
    "location": "reference/metric/#Canary.computemetric",
    "page": "Metric Terms",
    "title": "Canary.computemetric",
    "category": "function",
    "text": "computemetric(x::AbstractArray{T, 2}, D::AbstractMatrix{T}) where T\n\nCompute the 1-D metric terms from the element grid array x using the derivative matrix D. The derivative matrix D should be consistent with the reference grid r used in creategrid!.\n\nThe metric terms are returned as a \'NamedTuple` of the following arrays:\n\nJ the Jacobian determinant\nξx derivative ∂r / ∂x\'\nsJ the surface Jacobian\n\'nx` outward pointing unit normal in x-direction\n\n\n\n\n\ncomputemetric(x::AbstractArray{T, 3}, y::AbstractArray{T, 3},\n              D::AbstractMatrix{T}) where T\n\nCompute the 2-D metric terms from the element grid arrays x and y using the derivative matrix D. The derivative matrix D should be consistent with the reference grid r used in creategrid!.\n\nThe metric terms are returned as a \'NamedTuple` of the following arrays:\n\nJ the Jacobian determinant\nξx derivative ∂r / ∂x\'\nηx derivative ∂s / ∂x\'\nξy derivative ∂r / ∂y\'\nηy derivative ∂s / ∂y\'\nsJ the surface Jacobian\n\'nx` outward pointing unit normal in x-direction\n\'ny` outward pointing unit normal in y-direction\n\n\n\n\n\ncomputemetric(x::AbstractArray{T, 3}, y::AbstractArray{T, 3},\n              D::AbstractMatrix{T}) where T\n\nCompute the 3-D metric terms from the element grid arrays x, y, and z using the derivative matrix D. The derivative matrix D should be consistent with the reference grid r used in creategrid!.\n\nThe metric terms are returned as a \'NamedTuple` of the following arrays:\n\nJ the Jacobian determinant\nξx derivative ∂r / ∂x\'\nηx derivative ∂s / ∂x\'\nζx derivative ∂t / ∂x\'\nξy derivative ∂r / ∂y\'\nηy derivative ∂s / ∂y\'\nζy derivative ∂t / ∂y\'\nξz derivative ∂r / ∂z\'\nηz derivative ∂s / ∂z\'\nζz derivative ∂t / ∂z\'\nsJ the surface Jacobian\n\'nx` outward pointing unit normal in x-direction\n\'ny` outward pointing unit normal in y-direction\n\'nz` outward pointing unit normal in z-direction\n\nnote: Note\n\n\nThe storage of the volume terms and surface terms from this function are    slightly different. The volume terms used Cartesian indexing whereas the    surface terms use linear indexing.\n\n\n\n\n\n"
},

{
    "location": "reference/metric/#Canary.computemetric!",
    "page": "Metric Terms",
    "title": "Canary.computemetric!",
    "category": "function",
    "text": "computemetric!(x, J, ξx, sJ, nx, D)\n\nCompute the 1-D metric terms from the element grid arrays x. All the arrays are preallocated by the user and the (square) derivative matrix D should be consistent with the reference grid r used in creategrid!.\n\nIf Nq = size(D, 1) and nelem = div(length(x), Nq) then the volume arrays x, J, and ξx should all have length Nq * nelem.  Similarly, the face arrays sJ and nx should be of length nface * nelem with nface = 2.\n\n\n\n\n\ncomputemetric!(x, y, J, ξx, ηx, ξy, ηy, sJ, nx, ny, D)\n\nCompute the 2-D metric terms from the element grid arrays x and y. All the arrays are preallocated by the user and the (square) derivative matrix D should be consistent with the reference grid r used in creategrid!.\n\nIf Nq = size(D, 1) and nelem = div(length(x), Nq^2) then the volume arrays x, y, J, ξx, ηx, ξy, and ηy should all be of size (Nq, Nq, nelem).  Similarly, the face arrays sJ, nx, and ny should be of size (Nq, nface, nelem) with nface = 4.\n\n\n\n\n\ncomputemetric!(x, y, z, J, ξx, ηx, ζx, ξy, ηy, ζy, ξz, ηz, ζz, sJ, nx,\n               ny, nz, D)\n\nCompute the 3-D metric terms from the element grid arrays x, y, and z. All the arrays are preallocated by the user and the (square) derivative matrix D should be consistent with the reference grid r used in creategrid!.\n\nIf Nq = size(D, 1) and nelem = div(length(x), Nq^3) then the volume arrays x, y, z, J, ξx, ηx, ζx, ξy, ηy, ζy, ξz, ηz, and ζz should all be of length Nq^3 * nelem.  Similarly, the face arrays sJ, nx, ny, and nz should be of size Nq^2 * nface * nelem with nface = 6.\n\nThe curl invariant formulation of Kopriva (2006), equation 37, is used.\n\nReference:   Kopriva, David A. \"Metric identities and the discontinuous spectral element   method on curvilinear meshes.\" Journal of Scientific Computing 26.3 (2006):   301-327. https://doi.org/10.1007/s10915-005-9070-8\n\n\n\n\n\n"
},

{
    "location": "reference/metric/#Metric-Terms-1",
    "page": "Metric Terms",
    "title": "Metric Terms",
    "category": "section",
    "text": "creategrid\ncreategrid!\ncomputemetric\ncomputemetric!"
},

{
    "location": "reference/operators/#",
    "page": "Element Operators",
    "title": "Element Operators",
    "category": "page",
    "text": "DocTestSetup = :(using Canary)"
},

{
    "location": "reference/operators/#Canary.baryweights",
    "page": "Element Operators",
    "title": "Canary.baryweights",
    "category": "function",
    "text": "baryweights(r)\n\nreturns the barycentric weights associated with the array of points r\n\nReference:   Jean-Paul Berrut & Lloyd N. Trefethen, \"Barycentric Lagrange Interpolation\",   SIAM Review 46 (2004), pp. 501-517.   https://doi.org/10.1137/S0036144502417715\n\n\n\n\n\n"
},

{
    "location": "reference/operators/#Canary.interpolationmatrix",
    "page": "Element Operators",
    "title": "Canary.interpolationmatrix",
    "category": "function",
    "text": "interpolationmatrix(rsrc::AbstractVector{T}, rdst::AbstractVector{T},\n                    wbsrc=baryweights(rsrc)::AbstractVector{T}) where T\n\nreturns the polynomial interpolation matrix for interpolating between the points rsrc (with associated barycentric weights wbsrc) and rdst\n\nReference:   Jean-Paul Berrut & Lloyd N. Trefethen, \"Barycentric Lagrange Interpolation\",   SIAM Review 46 (2004), pp. 501-517.   https://doi.org/10.1137/S0036144502417715\n\n\n\n\n\n"
},

{
    "location": "reference/operators/#Canary.lglpoints",
    "page": "Element Operators",
    "title": "Canary.lglpoints",
    "category": "function",
    "text": "lglpoints(::Type{T}, N::Integer) where T <: AbstractFloat\n\nreturns the points r and weights w associated with the N+1-point Gauss-Legendre-Lobatto quadrature rule of type T\n\n\n\n\n\n"
},

{
    "location": "reference/operators/#Canary.lgpoints",
    "page": "Element Operators",
    "title": "Canary.lgpoints",
    "category": "function",
    "text": "lgpoints(::Type{T}, N::Integer) where T <: AbstractFloat\n\nreturns the points r and weights w associated with the N+1-point Gauss-Legendre quadrature rule of type T\n\n\n\n\n\n"
},

{
    "location": "reference/operators/#Canary.spectralderivative",
    "page": "Element Operators",
    "title": "Canary.spectralderivative",
    "category": "function",
    "text": "spectralderivative(r::AbstractVector{T},\n                   wb=baryweights(r)::AbstractVector{T}) where T\n\nreturns the spectral differentiation matrix for a polynomial defined on the points r with associated barycentric weights wb\n\nReference:   Jean-Paul Berrut & Lloyd N. Trefethen, \"Barycentric Lagrange Interpolation\",   SIAM Review 46 (2004), pp. 501-517.   https://doi.org/10.1137/S0036144502417715\n\n\n\n\n\n"
},

{
    "location": "reference/operators/#Element-Operators-1",
    "page": "Element Operators",
    "title": "Element Operators",
    "category": "section",
    "text": "baryweights\ninterpolationmatrix\nlglpoints\nlgpoints\nspectralderivative"
},

]}
